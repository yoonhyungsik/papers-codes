# ğŸ“˜ [Whatâ€™s in the Image? A Deepâ€‘Dive into the Vision of Vision Language Models]

## 1. ê°œìš” (Overview)

- **ì œëª©**: Whatâ€™s in the Image? A Deepâ€‘Dive into the Vision of Vision Language Models  
- **ì €ì**: Yaniv Kaduri, Hila Chefer, Idan Schwartz, Shai Bagon, Tali Dekel  
- **ì†Œì†**: Weizmann Institute of Science, NVIDIA Research, Bar-Ilan University  
- **í•™íšŒ**: arXiv preprint, submitted November 2024  
- **ë§í¬**: [arXiv](https://arxiv.org/abs/2411.17491) / [GitHub](https://github.com/yanivkaz/vlm_vision_analysis) / [Papers with Code](https://paperswithcode.com/paper/what-s-in-the-image-a-deep-dive-into-the)

> **ë…¼ë¬¸ ì„ ì • ì´ìœ  ë° ê°„ë‹¨í•œ ë„ì…ë¶€**  
> ìµœê·¼ Vision-Language Model(VLM)ì˜ í™œìš©ì´ ê¸‰ê²©íˆ í™•ì‚°ë˜ë©´ì„œ, ì´ëŸ¬í•œ ëª¨ë¸ë“¤ì´ **ì´ë¯¸ì§€ë¥¼ ì–´ë–»ê²Œ í•´ì„í•˜ê³  ì–¸ì–´ë¡œ ì „í™˜í•˜ëŠ”ì§€**ì— ëŒ€í•œ ê·¼ë³¸ì ì¸ ì§ˆë¬¸ì´ ì¤‘ìš”í•´ì¡Œë‹¤.  
> ì´ ë…¼ë¬¸ì€ VLM ë‚´ë¶€ì˜ **ì–´í…ì…˜ êµ¬ì¡°ì™€ ì •ë³´ íë¦„ì„ ì •ëŸ‰ì ìœ¼ë¡œ ë¶„ì„**í•˜ì—¬, ì´ë¯¸ì§€ ì •ë³´ê°€ í…ìŠ¤íŠ¸ ì‘ë‹µì— ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ ì˜í–¥ì„ ì£¼ëŠ”ì§€ë¥¼ ì‹¬ì¸µì ìœ¼ë¡œ íŒŒí—¤ì¹œë‹¤.  
> ì‹œê°ì  ì •ë³´ë¥¼ ëª¨ë¸ì´ ì–´ë””ì„œ, ì–´ë–»ê²Œ ì²˜ë¦¬í•˜ëŠ”ì§€ ëª…í™•íˆ ì´í•´í•˜ê³ ì ì´ ë…¼ë¬¸ì„ ì„ ì •í–ˆë‹¤.


## 2. ë¬¸ì œ ì •ì˜ (Problem Formulation)

**ë¬¸ì œ ë° ê¸°ì¡´ í•œê³„**:

- ê¸°ì¡´ Vision-Language Models (VLMs)ëŠ” ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì´ê³  ìˆìœ¼ë‚˜, **ì‹œê° ì •ë³´ê°€ ë‚´ë¶€ì ìœ¼ë¡œ ì–´ë–»ê²Œ ì–¸ì–´ ì •ë³´ë¡œ ë³€í™˜ë˜ëŠ”ì§€**ëŠ” ëª…í™•í•˜ê²Œ ë¶„ì„ë˜ì§€ ì•Šì•˜ë‹¤.
- ëŒ€ë¶€ë¶„ì˜ ì—°êµ¬ëŠ” VLMì˜ ê²°ê³¼ë¬¼(í…ìŠ¤íŠ¸ ì‘ë‹µ)ì— ì´ˆì ì„ ë§ì¶”ë©°, **ë‚´ë¶€ ë©”ì»¤ë‹ˆì¦˜â€”íŠ¹íˆ ì–´ë–¤ ì¸µì—ì„œ ì‹œê° ì •ë³´ê°€ í†µí•©ë˜ëŠ”ì§€**ì— ëŒ€í•œ ì‹¤ì¦ì ì¸ ë¶„ì„ì€ ë¶€ì¡±í–ˆë‹¤.
- ë˜í•œ, ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ ê°„ì˜ ì •ë³´ íë¦„ì´ **ì–´ëŠ ì¸µì„ í†µí•´ ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ ì´ë™í•˜ëŠ”ì§€**, **ì–´ë–¤ í† í°ì´ ì‹œê° ì •ë³´ë¥¼ ì£¼ë¡œ í™œìš©í•˜ëŠ”ì§€** ë“±ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ì„¤ëª…ì´ ì—†ì—ˆë‹¤.

**ì œì•ˆ ë°©ì‹**:

- ë³¸ ë…¼ë¬¸ì€ **Transformer ë‚´ë¶€ì˜ attention íë¦„ì„ ì •ëŸ‰ì ìœ¼ë¡œ ë¶„ì„**í•˜ì—¬ ì‹œê° ì •ë³´ê°€ ì–´ë–»ê²Œ ì–¸ì–´ ì‘ë‹µì— ë°˜ì˜ë˜ëŠ”ì§€ë¥¼ ì¡°ì‚¬í•œë‹¤.
- ì´ë¥¼ ìœ„í•´ **Layer-wise Attention Blocking** ê¸°ë²•ì„ ë„ì…í•˜ì—¬, íŠ¹ì • ì¸µì—ì„œ vision tokenì„ ì°¨ë‹¨í•œ ë’¤ ì–¸ì–´ ìƒì„± ê²°ê³¼ë¥¼ ë¹„êµ ë¶„ì„í•œë‹¤.
- ë‹¤ì–‘í•œ í”„ë¡¬í”„íŠ¸ ìœ í˜•(ì˜ˆ: open-ended description, object-attribute ì¶”ë¡ )ì— ëŒ€í•´ **ì–´ë–¤ í† í°ì´ ì–´ëŠ ìœ„ì¹˜ì˜ ì´ë¯¸ì§€ ì •ë³´ë¥¼ ì£¼ë¡œ í™œìš©í•˜ëŠ”ì§€**ë¥¼ í‰ê°€í•œë‹¤.

> â€» **í•µì‹¬ ê°œë… ì •ì˜**:
> - **Vision Token**: ì´ë¯¸ì§€ ì…ë ¥ì´ patch ë‹¨ìœ„ë¡œ ë¶„í• ë˜ì–´ transformerì— ì…ë ¥ë˜ëŠ” ì‹œê° ì •ë³´ ë‹¨ìœ„.
> - **Query Token**: "ì´ë¯¸ì§€ë¥¼ ì„¤ëª…í•´ì¤˜", "ì´ ì¥ë©´ì€ ì–´ë””ëƒ?" ë“±ì˜ í”„ë¡¬í”„íŠ¸ì— í¬í•¨ëœ ì–¸ì–´ í† í°ìœ¼ë¡œ, ì‹œê° ì •ë³´ ìš”ì•½ê³¼ ì „ë‹¬ì˜ í•µì‹¬ ì—­í• ì„ í•œë‹¤.
> - **Attention Blocking**: íŠ¹ì • ì¸µì—ì„œ ì´ë¯¸ì§€ í† í°ì— ëŒ€í•œ ì–´í…ì…˜ ì ‘ê·¼ì„ ê°•ì œë¡œ ë§‰ì•„, ì •ë³´ íë¦„ì˜ ìœ„ì¹˜ì™€ ì—­í• ì„ í‰ê°€í•˜ëŠ” ê¸°ë²•.

---

## 3. ëª¨ë¸ êµ¬ì¡° (Architecture)

### ì „ì²´ êµ¬ì¡°

![ëª¨ë¸ êµ¬ì¡°](ê²½ë¡œ)

- ë³¸ ë…¼ë¬¸ì€ ìƒˆë¡œìš´ ëª¨ë¸ì„ ì œì•ˆí•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ê¸°ì¡´ VLM ì•„í‚¤í…ì²˜(ex: BLIP-2, LLaVA ë“±)ì˜ ë‚´ë¶€ ë™ì‘ì„ **ë¶„ì„ ë° ê³„ì¸µë³„ ì‹œê° ì •ë³´ íë¦„ ì¸¡ë©´ì—ì„œ í•´ì„**í•˜ëŠ” ì‹¤í—˜ ì¤‘ì‹¬ êµ¬ì¡°ë¥¼ ê°–ëŠ”ë‹¤.
- ì‹¤í—˜ ëŒ€ìƒì´ ëœ VLMì€ ì¼ë°˜ì ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ì€ êµ¬ì¡°ë¥¼ ë”°ë¥¸ë‹¤:

```text
[Image Encoder (e.g. ViT)] â†’ [Query/Prompt Token Embedding] â†’ [Transformer Decoder (Language Model)] â†’ [Text Output]
```

- **ì…ë ¥ íë¦„**:
  - ì´ë¯¸ì§€ â†’ Patch â†’ Vision Encoder â†’ Vision Token (V)
  - í”„ë¡¬í”„íŠ¸ â†’ Tokenizer â†’ Text Token (T)
- **ì¶œë ¥ íë¦„**:
  - Language Transformer â†’ Predict next token â†’ Generate caption or response

- ì´ êµ¬ì¡° ìœ„ì— ì €ìë“¤ì€ íŠ¹ì • ì‹¤í—˜ ê¸°ë²•ì„ ì‚½ì…í•˜ì—¬ ë‚´ë¶€ ì •ë³´ íë¦„ì„ ì¶”ì í•œë‹¤.

---

### ğŸ’  í•µì‹¬ ëª¨ë“ˆ ë˜ëŠ” êµ¬ì„± ìš”ì†Œ

#### ğŸ“Œ Attention Blocking Layer (ABL)

- **ì‘ë™ ë°©ì‹**:  
  íŠ¹ì • transformer ì¸µ `L_i`ì—ì„œ **image token â†’ text token ê°„ attentionì„ 0ìœ¼ë¡œ ë§ˆìŠ¤í‚¹**.  
  ì¦‰, í•´ë‹¹ ì¸µì—ì„œ ì´ë¯¸ì§€ ì •ë³´ê°€ í…ìŠ¤íŠ¸ í† í°ìœ¼ë¡œ ì „ë‹¬ë˜ì§€ ëª»í•˜ë„ë¡ ì°¨ë‹¨í•¨.
  
- **ìˆ˜ì‹ì ìœ¼ë¡œ í‘œí˜„í•˜ë©´**:  
  Attention matrix `A`ì—ì„œ vision token ê´€ë ¨ attention weight `A[v, t]`ë¥¼ `0`ìœ¼ë¡œ ì„¤ì •  
$A[v, t] = 0 \quad (v: \text{vision token}, \ t: \text{text token})$

- **ì‚¬ìš© ëª©ì **:
- ì–´ë–¤ ì¸µì´ ì´ë¯¸ì§€ ì •ë³´ë¥¼ ì „ë‹¬í•˜ëŠ”ì§€ ë¶„ì„
- íŠ¹ì • ì¸µë§Œ ì‹œê° ì •ë³´ ì ‘ê·¼ ê°€ëŠ¥í•˜ê²Œ ë§Œë“¤ê³  ë‚˜ë¨¸ì§€ëŠ” ì°¨ë‹¨

---

#### ğŸ“Œ Vision-Query Token Dependency Analyzer

- **ì—­í• **:  
ìƒì„±ëœ í…ìŠ¤íŠ¸ í† í°(ì˜ˆ: "red bike")ì´ **ì–´ë–¤ vision tokenì— attentionì„ ì§‘ì¤‘í•˜ëŠ”ì§€**ë¥¼ ë¶„ì„  
â†’ ì´ë¥¼ í†µí•´ ì–´ë–¤ í…ìŠ¤íŠ¸ê°€ ì–´ë–¤ ì´ë¯¸ì§€ ìœ„ì¹˜ì— ëŒ€ì‘ë˜ëŠ”ì§€ íŒŒì•… ê°€ëŠ¥

- **ê¸°ì¡´ ë°©ì‹ê³¼ì˜ ì°¨ë³„ì **:
- ë‹¨ìˆœ attention ì‹œê°í™”ê°€ ì•„ë‹ˆë¼, **ë‹¨ì–´ ë‹¨ìœ„ë¡œ image-patch ì—°ê²°ì„ ì •ëŸ‰ ë¶„ì„**í•¨
- "object-attribute grounding"ì´ë¼ëŠ” ì¸¡ë©´ì—ì„œ fine-grainedí•˜ê²Œ í‰ê°€

---

#### ğŸ“Œ Prompt Ablation Experiment

- **ì‘ë™ ë°©ì‹**:  
í”„ë¡¬í”„íŠ¸ì˜ ì¼ë¶€(ì˜ˆ: â€œin this image,â€ â€œdescribeâ€)ë¥¼ ì œê±°í•˜ê³  ì„±ëŠ¥ì„ ë¹„êµ  
â†’ **query token ìì²´ê°€ ì‹œê° ì •ë³´ë¥¼ ë‚´í¬í•˜ëŠ”ì§€** ê²€ì¦

- **ì˜ì˜**:  
- VLMì´ í”„ë¡¬í”„íŠ¸ êµ¬ì¡° ìì²´ì— í•™ìŠµë˜ì–´, ì‹œê°ì  ì •ë³´ë¥¼ í•´ë‹¹ í† í°ì— ì•”ë¬µì ìœ¼ë¡œ ì¸ì½”ë”©í•˜ê³  ìˆìŒ  
- ì¦‰, **í…ìŠ¤íŠ¸ í† í°ì´ ì´ë¯¸ì§€ ìš”ì•½ ì •ë³´ë¥¼ ë³´ìœ **í•œë‹¤ëŠ” ê²ƒì„ ì‹¤í—˜ì ìœ¼ë¡œ ì…ì¦

---

### ğŸ” ì–´í…ì…˜ êµ¬ì¡° ë¶„ì„ (Detailed Attention Flow Analysis)

#### ğŸ“Œ 1. Cross-Modal Attention ì •ì˜

VLMì˜ í•µì‹¬ì€ Vision Token ($V$)ê³¼ Text Token ($T$) ê°„ì˜ **Cross-Attention**ì…ë‹ˆë‹¤.  
Transformerì˜ self-attentionì€ ì›ë˜ ë™ì¢… í† í°ë¼ë¦¬ ì •ë³´ë¥¼ ì£¼ê³ ë°›ì§€ë§Œ, VLMì—ì„œëŠ” ì„œë¡œ ë‹¤ë¥¸ modality ê°„ attentionì´ ì¡´ì¬í•©ë‹ˆë‹¤.

- ì¼ë°˜ self-attention: $A[t_i, t_j]$
- cross-modal attention: $A[t_i, v_j]$ ë˜ëŠ” $A[v_i, t_j]$

#### ğŸ“Œ 2. ì‹¤í—˜ ì„¤ê³„: Layer-wise Attention Blocking (ABL)

ì €ìë“¤ì€ ê° Transformer Layer $L_i$ì—ì„œ ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒì ìœ¼ë¡œ ì°¨ë‹¨í•©ë‹ˆë‹¤:

1. **Text-to-Vision ì°¨ë‹¨**:\
   $A[t_i, v_j] = 0 \quad \text{for all } i, j$
   â†’ í…ìŠ¤íŠ¸ í† í°ì´ ë¹„ì „ í† í°ì—ì„œ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í•˜ê²Œ í•¨

3. **Vision-to-Text ì°¨ë‹¨**:  
   $A[v_i, t_j] = 0 \quad \text{for all } i, j$
   â†’ ë¹„ì „ í† í°ì´ í…ìŠ¤íŠ¸ë¡œ ì •ë³´ ì „ë‹¬í•˜ì§€ ëª»í•˜ê²Œ í•¨

4. **Vision-to-Vision ìœ ì§€ / Text-to-Text ìœ ì§€**ëŠ” ê·¸ëŒ€ë¡œ ë‘ê³  cross-modal ê²½ë¡œë§Œ ë§‰ìŒ

- ì´ë¥¼ í†µí•´ **ì–´ëŠ ì¸µì—ì„œ ì´ë¯¸ì§€ ì •ë³´ê°€ íš¨ê³¼ì ìœ¼ë¡œ ì–¸ì–´ë¡œ ì „ë‹¬ë˜ëŠ”ì§€ ê³„ì¸µë³„ë¡œ ë¶„ì„**í•¨

#### ğŸ“Œ 3. ì£¼ìš” ê´€ì°° ê²°ê³¼

- **ì¤‘ê°„ì¸µ (~25%~50%)**ì—ì„œ cross-modal attentionì´ ê°€ì¥ í™œë°œí•˜ë©°, ì‹¤ì œë¡œ ì´ ì¸µì—ì„œë§Œ vision accessë¥¼ í—ˆìš©í•´ë„ ì„±ëŠ¥ ì†ì‹¤ì´ ê±°ì˜ ì—†ìŒ.
- ì´ˆ/í›„ë°˜ì¸µì˜ attentionì€ vision tokenì— ëŒ€í•œ ì§‘ì¤‘ë„ê°€ ë‚®ê³ , ê±°ì˜ ëŒ€ë¶€ë¶„ì˜ ì‹œê° ì •ë³´ê°€ **ì¤‘ê°„ ì¸µì—ì„œ ì§‘ì¤‘ì ìœ¼ë¡œ í˜ëŸ¬ë“¤ì–´ê°**.
- í”„ë¡¬í”„íŠ¸ token (e.g., â€œdescribeâ€, â€œin this imageâ€)ì— í•´ë‹¹í•˜ëŠ” text tokenì€ ê±°ì˜ ëª¨ë“  vision tokenì— **low-entropy attention**ì„ ë¶„ì‚°ì ìœ¼ë¡œ ë³´ë‚´ë©°, **ì „ì—­ì  ì‹œê° ìš”ì•½ì„ ìˆ˜í–‰**í•˜ëŠ” ê²ƒìœ¼ë¡œ ë³´ì„.

#### ğŸ“Œ 4. Attention Map ì‹œê°í™”

ì €ìë“¤ì€ ì‹¤ì œ attention mapì„ ë‹¤ìŒê³¼ ê°™ì´ ë¶„ì„:

- íŠ¹ì • ë‹¨ì–´(ì˜ˆ: â€œredâ€ í˜¹ì€ â€œbikeâ€)ë¥¼ ìƒì„±í•˜ëŠ” tokenì´ ì–´ë–¤ image patch (vision token)ì— attentionì„ ì§‘ì¤‘í–ˆëŠ”ì§€ ì¶”ì 
- í•´ë‹¹ tokenì˜ attention vectorì—ì„œ ê°€ì¥ ë†’ì€ weightë¥¼ ê°–ëŠ” top-k vision tokenì„ ì¶”ì¶œí•˜ì—¬ **spatial grounding** ìˆ˜í–‰

â†’ ì´ëŠ” object attribute grounding ë¶„ì„ì— í™œìš©ë¨ (ì˜ˆ: â€œblue shirtâ€ë¼ëŠ” í‘œí˜„ì´ ì‹¤ì œë¡œ íŒŒë€ ì…”ì¸  ìœ„ì¹˜ì— attention ì§‘ì¤‘)

#### ğŸ“Œ 5. ìˆ˜ì‹ ê¸°ë°˜ ì •ë¦¬

- Attention Weight:  
  $\alpha_{t_i, v_j} = \frac{\exp(q_{t_i} \cdot k_{v_j})}{\sum_{j'} \exp(q_{t_i} \cdot k_{v_{j'}})}$
  - $q_{t_i}$: í…ìŠ¤íŠ¸ í† í°ì˜ ì¿¼ë¦¬ ë²¡í„°
  - $k_{v_j}$: ì´ë¯¸ì§€ í† í°ì˜ í‚¤ ë²¡í„°

- Blockingì€ ì´ $\alpha_{t_i, v_j}$ ê°’ì„ ì§ì ‘ 0ìœ¼ë¡œ ë§Œë“œëŠ” ë°©ì‹ìœ¼ë¡œ ìˆ˜í–‰ë¨.

---
## âš–ï¸ ê¸°ì¡´ ëª¨ë¸ê³¼ì˜ ë¹„êµ

| í•­ëª©    | ë³¸ ë…¼ë¬¸                                         | ê¸°ì¡´ ë°©ë²•1 (LLaVA)                        | ê¸°ì¡´ ë°©ë²•2 (BLIP-2)                          |
| ------- | ---------------------------------------------- | ---------------------------------------- | ------------------------------------------- |
| êµ¬ì¡°    | ê¸°ì¡´ VLM ìœ„ì— attention-blocking ì‹¤í—˜ ì„¤ê³„ ì¶”ê°€ | Vision encoder + LLM                     | Q-former + Vision encoder + LLM              |
| í•™ìŠµ ë°©ì‹ | ê¸°ì¡´ ëª¨ë¸ ê¸°ë°˜ zero-shot or finetuned ì‚¬ìš©       | Stage-wise pretraining                   | Pretrained Q-former + Frozen ViT + LLM       |
| ëª©ì     | ì‹œê° ì •ë³´ê°€ ì–´ëŠ ì¸µ, ì–´ë–¤ í† í°ì— ë°˜ì˜ë˜ëŠ”ì§€ ë¶„ì„     | ë©€í‹°ëª¨ë‹¬ ë¬¸ì¥ ìƒì„± (ì´ë¯¸ì§€ ì„¤ëª…, QA ë“±)      | ì‹œê° ì •ë³´ ê¸°ë°˜ ì§ˆë¬¸ì‘ë‹µ ë° caption ìƒì„±        |

---

> **ì‹¤í—˜ ê²°ê³¼ ìš”ì•½ ë° í•´ì„**  
> - Vision token accessë¥¼ ì˜¤ì§ ì¤‘ê°„ì¸µì—ë§Œ í—ˆìš©í•´ë„ ì„±ëŠ¥ì´ ê±°ì˜ ìœ ì§€ë¨ â†’ **ì¤‘ê°„ì¸µì´ ì£¼ìš” ì •ë³´ ì „ë‹¬ ê²½ë¡œ**  
> - Query token (e.g., â€œdescribeâ€)ì€ ì´ë¯¸ì§€ ì „ì²´ì— ëŒ€í•œ low-entropy attentionì„ ë³´ë‚´ë©°, **ì „ì—­ì  ì‹œê° ìš”ì•½ ë‹´ë‹¹**  
> - íŠ¹ì • í…ìŠ¤íŠ¸ ë‹¨ì–´ëŠ” í•´ë‹¹ object ìœ„ì¹˜ì˜ patchì— attention ì§‘ì¤‘ â†’ **object-attribute grounding í™•ì‹¤**

---

## âœ… ì¥ì  ë° í•œê³„

### **ì¥ì **:

- VLM ë‚´ë¶€ì˜ ì •ë³´ íë¦„ì„ **ì‹¤í—˜ì ìœ¼ë¡œ, ê³„ì¸µì ìœ¼ë¡œ ê²€ì¦**í•œ ìµœì´ˆì˜ ì •ëŸ‰ì  ì—°êµ¬ ì¤‘ í•˜ë‚˜
- Query tokenì´ ì‹¤ì œë¡œ ì „ì—­ ì´ë¯¸ì§€ ì •ë³´ë¥¼ ìš”ì•½í•œë‹¤ëŠ” **ìƒˆë¡œìš´ ì‹œì‚¬ì **
- í–¥í›„ ê²½ëŸ‰í™” ë˜ëŠ” pruning ì‹œ **ì¤‘ìš”í•œ ì¸µë§Œ ë‚¨ê¸°ëŠ” êµ¬ì¡° ìµœì í™” ë°©í–¥ ì œì‹œ ê°€ëŠ¥**

### **í•œê³„ ë° ê°œì„  ê°€ëŠ¥ì„±**:

- í•™ìŠµ ìì²´ë¥¼ ë‹¤ë£¨ì§€ ì•Šê³  **ê¸°ì¡´ ëª¨ë¸ì— ë¶„ì„ ì‹¤í—˜ì„ ì¶”ê°€**í•œ êµ¬ì¡° â†’ êµ¬ì¡° ìì²´ì˜ í˜ì‹ ì€ ì•„ë‹˜
- ì‹¤í—˜ ë²”ìœ„ê°€ ì œí•œì ì´ë©°, multimodal generation ì „ë°˜ìœ¼ë¡œ í™•ì¥ í•„ìš”
- attention weightë§Œìœ¼ë¡œ ì •ë³´ íë¦„ì„ ì¶”ë¡ í•˜ëŠ” ë° í•œê³„ê°€ ì¡´ì¬í•  ìˆ˜ ìˆìŒ

---

## ğŸ§  TL;DR â€“ í•œëˆˆì— ìš”ì•½

> ì´ ë…¼ë¬¸ì€ Vision-Language Models ë‚´ë¶€ì—ì„œ **ì‹œê° ì •ë³´ê°€ ì–¸ì–´ë¡œ ì–´ë–»ê²Œ ì „ë‹¬ë˜ëŠ”ì§€ë¥¼ ê³„ì¸µì ìœ¼ë¡œ ë¶„ì„**í•œë‹¤.  
> íŠ¹íˆ ì¤‘ê°„ Transformer ì¸µì—ì„œ vision tokenì˜ ì •ë³´ê°€ query tokenì— ì§‘ì¤‘ ì „ë‹¬ë˜ë©°, object-level groundingì€ í…ìŠ¤íŠ¸ ìƒì„±ì— ê°•í•œ ê³µê°„ì  ì—°ê²°ì„±ì„ ê°–ëŠ”ë‹¤.  
> ì´ë¥¼ í†µí•´ **VLM í•´ì„ ê°€ëŠ¥ì„±, ìµœì í™” ê°€ëŠ¥ì„±, ì „ì—­ vs. êµ­ë¶€ attention êµ¬ì¡°**ì— ëŒ€í•œ ì´í•´ë¥¼ ì œê³µí•œë‹¤.

| êµ¬ì„± ìš”ì†Œ     | ì„¤ëª… |
| ------------ | ---- |
| í•µì‹¬ ëª¨ë“ˆ     | Attention Blocking Layer, Prompt Ablation, Attention Map ë¶„ì„ |
| í•™ìŠµ ì „ëµ     | ê¸°ì¡´ pretrained ëª¨ë¸ í™œìš©, ì¶”ê°€ í•™ìŠµ ì—†ì´ ì‹¤í—˜ì  êµ¬ì¡° ì ìš© |
| ì „ì´ ë°©ì‹     | Vision-to-Text ì •ë³´ íë¦„ ì¶”ì  (íŠ¹íˆ Query token ì¤‘ì‹¬ìœ¼ë¡œ) |
| ì„±ëŠ¥/íš¨ìœ¨ì„±  | ì¤‘ê°„ì¸µë§Œ í™œìš©í•´ë„ ì„±ëŠ¥ ìœ ì§€ â†’ ê³„ì‚°ëŸ‰ ì¤„ì´ë©´ì„œ êµ¬ì¡° ê²½ëŸ‰í™” ê°€ëŠ¥ì„± ìˆìŒ |

---

## ğŸ”— ì°¸ê³  ë§í¬ (References)

* [ğŸ“„ arXiv ë…¼ë¬¸](https://arxiv.org/abs/2411.17491)
* [ğŸ’» GitHub](https://github.com/yanivkaz/vlm_vision_analysis)
* [ğŸ“ˆ Papers with Code](https://paperswithcode.com/paper/what-s-in-the-image-a-deep-dive-into-the)

---

## ë‹¤ìŒ ë…¼ë¬¸:
-"LLaVA: Large Language and Vision Assistant" (2023)
