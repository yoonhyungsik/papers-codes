# ğŸ“˜ Making LLaMA SEE and Draw with SEED Tokenizer

## 1. ê°œìš” (Overview)

- **ì œëª©**: Making LLaMA SEE and Draw with SEED Tokenizer  
- **ì €ì**: Yuzhe Wang, Yufei Wang, Jiacheng Ye, Shupeng Liu, Hang Su, Jun Zhu  
- **ì†Œì†**: Tsinghua University, Beijing Academy of Artificial Intelligence (BAAI)  
- **í•™íšŒ**: arXiv preprint (2023.10, arXiv:2310.01218)  
- **ë§í¬**: [arXiv](https://arxiv.org/abs/2310.01218) / [GitHub](https://github.com/THUDM/SEED) / [Papers with Code](https://paperswithcode.com/paper/making-llama-see-and-draw-with-seed-tokenizer)

> ìµœê·¼ Vision-Language ëª¨ë¸ì€ í…ìŠ¤íŠ¸ ê¸°ë°˜ì˜ ìƒì„± ëŠ¥ë ¥ì„ ë„˜ì–´ì„œ ì‹œê°ì  ê°œì²´ì˜ ì´í•´ì™€ ìƒì„±ê¹Œì§€ ìš”êµ¬ë°›ê³  ìˆë‹¤. ë³¸ ë…¼ë¬¸ì€ ê¸°ì¡´ LLaMA ì–¸ì–´ ëª¨ë¸ì— ì‹œê°ì  í‘œí˜„ë ¥ì„ ë¶€ì—¬í•˜ê¸° ìœ„í•´, ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ë¥¼ ë™ì¼í•œ ì‹œí€€ìŠ¤ ë‚´ì—ì„œ ë‹¤ë£° ìˆ˜ ìˆëŠ” **SEED Tokenizer**ë¥¼ ì œì•ˆí•œë‹¤. íŠ¹íˆ, ì´ë¯¸ì§€ë¥¼ discrete tokenìœ¼ë¡œ í‘œí˜„í•˜ê³  ë‹¤ì‹œ ë³µì› ê°€ëŠ¥í•œ êµ¬ì¡°ë¥¼ í†µí•´, LLaMA ê¸°ë°˜ ëª¨ë¸ì´ ì´ë¯¸ì§€ë¥¼ 'ë³´ê³  ê·¸ë¦¬ê³ ' ì´í•´í•˜ëŠ” ëŠ¥ë ¥ì„ ê°–ì¶”ê²Œ ëœë‹¤.  
> Diffusionì´ë‚˜ Auto-regressive ê¸°ë°˜ì˜ ê¸°ì¡´ ë°©ë²•ë“¤ê³¼ ë¹„êµí•´ í•™ìŠµ íš¨ìœ¨ì„±ê³¼ ë²”ìš©ì„±ì´ ë†’ì•„, ë‹¤ì–‘í•œ VLM/Multimodal ë¶„ì•¼ì— ì‘ìš© ê°€ëŠ¥ì„±ì´ í¬ë‹¤ê³  íŒë‹¨í•˜ì—¬ ë³¸ ë…¼ë¬¸ì„ ì„ ì •í•˜ì˜€ë‹¤.
> ì´í›„ ì—°êµ¬í•˜ê³  ì‹¶ì€ ë‚´ìš©ì´ ì´ ë¶„ì•¼ì—¬ì„œ ì½ê³  ì‹¶ì—ˆë‹¤.

---

## 2. ë¬¸ì œ ì •ì˜ (Problem Formulation)

**ë¬¸ì œ ë° ê¸°ì¡´ í•œê³„**:

* ê¸°ì¡´ì˜ Vision-Language Models (VLMs)ëŠ” í…ìŠ¤íŠ¸ëŠ” í† í°(token) ë‹¨ìœ„ë¡œ ì²˜ë¦¬í•˜ëŠ” ë°˜ë©´, ì´ë¯¸ì§€ëŠ” CNN feature map, CLIP embedding ë“± ì´ì§ˆì ì¸ continuous í‘œí˜„ìœ¼ë¡œ ë‹¤ë¤„ì ¸ **í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ ê°„ì˜ ë™ë“±í•œ ì‹œí€€ìŠ¤ ì²˜ë¦¬ê°€ ë¶ˆê°€ëŠ¥**í–ˆìŒ.
* ì´ë¡œ ì¸í•´ í…ìŠ¤íŠ¸ ì „ìš© ì–¸ì–´ ëª¨ë¸(ì˜ˆ: LLaMA)ì„ ë©€í‹°ëª¨ë‹¬ ì…ë ¥ì— ë°”ë¡œ í™œìš©í•˜ê¸° ì–´ë µê³ , ë‹¨ì¼ Transformerë¡œ ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ë¥¼ í•¨ê»˜ ì´í•´í•˜ê³  ìƒì„±í•˜ëŠ” êµ¬ì¡°ë¥¼ êµ¬ì¶•í•˜ê¸° í˜ë“¤ì—ˆìŒ.
* ê¸°ì¡´ discrete image tokenizer(ViT-VQGAN ë“±)ëŠ” autoregressive ë³µì› í’ˆì§ˆì´ ë‚®ê±°ë‚˜ í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ê°„ alignmentê°€ ë–¨ì–´ì§€ëŠ” ë¬¸ì œê°€ ìˆì—ˆìŒ.

**ì œì•ˆ ë°©ì‹**:

* ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ë¥¼ **ë™ì¼í•œ token vocabulary**ë¡œ í‘œí˜„í•  ìˆ˜ ìˆë„ë¡, **SEED (Semantically Enhanced Encoder-decoder Discretizer) Tokenizer**ë¥¼ ì œì•ˆí•¨.
* SEEDëŠ” high-fidelity discrete image tokensë¥¼ ìƒì„±í•˜ë©°, ì´ tokenë“¤ì€ ì–¸ì–´ ëª¨ë¸ê³¼ ë™ì¼í•œ vocabulary ë° positional encodingì„ ê³µìœ í•¨.
* ì´ë¥¼ í†µí•´ LLaMAì™€ ê°™ì€ ì–¸ì–´ ëª¨ë¸ì´ **ì´ë¯¸ì§€ë¥¼ ë§ˆì¹˜ ë¬¸ì¥ì²˜ëŸ¼ ë‹¤ë£° ìˆ˜ ìˆê²Œ** ë˜ì–´, multimodal pretraining ì—†ì´ë„ ì´ë¯¸ì§€ ì´í•´ ë° ìƒì„± ëŠ¥ë ¥ì„ íšë“í•  ìˆ˜ ìˆìŒ.

> â€» **í•µì‹¬ ê°œë… ì •ì˜**  
> - **SEED Tokenizer**: ì´ë¯¸ì§€ ì…ë ¥ì„ high-level semantic discrete token ì‹œí€€ìŠ¤ë¡œ ë³€í™˜í•˜ëŠ” ëª¨ë“ˆ. Auto-regressive reconstructionì´ ê°€ëŠ¥í•˜ë©°, ê¸°ì¡´ VQ-based tokenizerë³´ë‹¤ alignmentì™€ ì¬í˜„ í’ˆì§ˆì´ ë›°ì–´ë‚¨.  
> - **Visual Vocabulary Sharing**: í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ê°€ ë™ì¼í•œ token embedding spaceë¥¼ ê³µìœ í•˜ì—¬ unified modelingì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” ì„¤ê³„.

---

## 3. ëª¨ë¸ êµ¬ì¡° (Architecture)

![ëª¨ë¸ êµ¬ì¡°](../images/seed-llama_visual_tokenizer.png)

### ì „ì²´ êµ¬ì¡°

* ì „ì²´ ì‹œìŠ¤í…œì€ **SEED Tokenizer**ì™€ **LLaMA ì–¸ì–´ ëª¨ë¸**ì„ ê²°í•©í•˜ì—¬, ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ë¥¼ **ë™ì¼í•œ ì‹œí€€ìŠ¤ ìƒì—ì„œ ì²˜ë¦¬**í•˜ëŠ” êµ¬ì¡°ë¡œ ì„¤ê³„ë¨.
* ì…ë ¥ì€ í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ë¡œ êµ¬ì„±ë˜ë©°, ì´ë¯¸ì§€ëŠ” ë¨¼ì € SEED Tokenizerë¥¼ í†µí•´ discrete token ì‹œí€€ìŠ¤ë¡œ ë³€í™˜ë¨.
* ì´ ì‹œí€€ìŠ¤ëŠ” í…ìŠ¤íŠ¸ì™€ í•¨ê»˜ LLaMA ëª¨ë¸ì— ì…ë ¥ë˜ê³ , ì¶œë ¥ ì‹œì—ëŠ” ì´ë¯¸ì§€ ìƒì„±, ì„¤ëª… ìƒì„±(image captioning), ì§ˆì˜ì‘ë‹µ ë“± ë‹¤ì–‘í•œ ë©€í‹°ëª¨ë‹¬ íƒœìŠ¤í¬ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆìŒ.
* ì¶œë ¥ ì‹œ, ìƒì„±ëœ ì´ë¯¸ì§€ í† í°ì€ ë‹¤ì‹œ SEED Tokenizerì˜ ë””ì½”ë”ë¥¼ í†µí•´ high-resolution ì´ë¯¸ì§€ë¡œ ë³µì›ë¨.

---

### ğŸ’  í•µì‹¬ ëª¨ë“ˆ ë˜ëŠ” êµ¬ì„± ìš”ì†Œ

#### ğŸ“Œ SEED Tokenizer (Semantically Enhanced Encoder-Decoder Discretizer)

![ëª¨ë¸ êµ¬ì¡°](../images/seed-llama_tokenizer.png)

**ëª©í‘œ**: ì´ë¯¸ì§€ë¥¼ discreteí•œ token ì‹œí€€ìŠ¤ë¡œ ë³€í™˜í•˜ë©´ì„œ, semantic fidelityì™€ autoregressive ë³µì› ì„±ëŠ¥ì„ ëª¨ë‘ í™•ë³´í•˜ëŠ” ê²ƒ.

##### êµ¬ì¡° êµ¬ì„±:

1. **Encoder**:  
   - Conv stem â†’ Vision Transformer (ViT) êµ¬ì¡° ê¸°ë°˜
   - ì…ë ¥ ì´ë¯¸ì§€ë¥¼ latent featureë¡œ ì••ì¶• (size: 16x16 or 32x32)
   - Semantic embedding spaceì—ì„œ í‘œí˜„

2. **Codebook (VQ module)**:  
   - ê¸°ì¡´ VQ-VAEì™€ ìœ ì‚¬í•˜ì§€ë§Œ, ë” ì •êµí•œ attention ê¸°ë°˜ quantizationì„ ì‚¬ìš©  
   - latent vectorë¥¼ **learnable codebook vector**ë¡œ ë§¤í•‘  
   - discrete token `z âˆˆ â„¤â¿` ìƒì„±  
   - LatentëŠ” L2 distance ê¸°ë°˜ nearest neighborë¥¼ í†µí•´ quantizedë¨

3. **Decoder**:  
   - Image reconstructionì„ ìœ„í•œ autoregressive decoder ì‚¬ìš© (Transformer ê¸°ë°˜)  
   - cross-entropy loss + perceptual lossë¥¼ í†µí•´ í•™ìŠµ  
   - ì‹¤ì œ ì´ë¯¸ì§€ë¥¼ ê³ í’ˆì§ˆë¡œ ì¬êµ¬ì„± ê°€ëŠ¥

##### ìˆ˜ì‹ ìš”ì•½:

```math
\text{Encoder: } \mathbf{h} = f_{\text{enc}}(\mathbf{x})

\text{Quantization: } z_i = \arg \min_j \|\mathbf{h}_i - \mathbf{e}_j\|_2

\text{Decoder: } \hat{\mathbf{x}} = f_{\text{dec}}(\{ \mathbf{e}_{z_i} \})
```
#### ğŸ“Œ Visual Vocabulary Sharing

í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ ëª¨ë‘ **ë™ì¼í•œ token embedding spaceë¥¼ ê³µìœ **í•¨ìœ¼ë¡œì¨, LLaMA ì–¸ì–´ ëª¨ë¸ì´ ì´ë¯¸ì§€ tokenì„ nativeí•˜ê²Œ ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ í•¨.

`positional embedding`ë„ ê³µìœ í•˜ì—¬, ì‹œí€€ìŠ¤ ìƒì—ì„œ ì´ë¯¸ì§€ tokenì´ ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ì§ˆ ìˆ˜ ìˆë„ë¡ ì„¤ê³„.

ì´ë¥¼ í†µí•´ **extra multimodal fusion ëª¨ë“ˆ ì—†ì´**, í…ìŠ¤íŠ¸ ì „ìš© LLM(LLaMA)ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš© ê°€ëŠ¥í•¨.

---

![ëª¨ë¸ êµ¬ì¡°](../images/seed-llama_flow.png)

#### ğŸ“Œ LLaMA ê¸°ë°˜ Multimodal Inference

ê¸°ì¡´ LLaMAì˜ **decoder-only êµ¬ì¡°ëŠ” ë³€ê²½ ì—†ì´ ì‚¬ìš©**ë¨.

SEED Tokenizerë¥¼ í†µí•´ ì´ë¯¸ì§€ê°€ discrete tokenìœ¼ë¡œ ì²˜ë¦¬ë˜ë¯€ë¡œ, **í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€ í† í°ì„ ë‹¨ì¼ ì‹œí€€ìŠ¤ë¡œ ì²˜ë¦¬** ê°€ëŠ¥í•¨.

í…ìŠ¤íŠ¸ ìƒì„±, ì´ë¯¸ì§€ ìƒì„±, multimodal QA ë“±ì˜ ì‘ì—…ì—ì„œ **cross-modality alignment pretraining ì—†ì´ë„ zero-shot ë˜ëŠ” few-shot ì„±ëŠ¥ í™•ë³´** ê°€ëŠ¥.

---

#### ğŸ“Œ Training Objective

**SEED Tokenizerì™€ LLaMAëŠ” ë”°ë¡œ í•™ìŠµ**ë˜ë©°, end-to-end í•™ìŠµì´ ì•„ë‹Œ **ëª¨ë“ˆ ë¶„ë¦¬ í•™ìŠµ ë°©ì‹** ì‚¬ìš©:

- **SEED Tokenizer**:  
  - `VQ loss` + `perceptual loss` + `auto-regressive reconstruction loss`ë¡œ í•™ìŠµ  
  - ëª©í‘œ: **ì •ë³´ ì†ì‹¤ ì—†ì´ discrete í‘œí˜„ìœ¼ë¡œ ì´ë¯¸ì§€ë¥¼ ì••ì¶•**

- **LLaMA ê¸°ë°˜ VLM**:  
  - ê¸°ì¡´ ì–¸ì–´ ëª¨ë¸ í•™ìŠµ objective ìœ ì§€ (**Causal LM loss**)  
  - ë‹¤ë§Œ ì…ë ¥ ì‹œ **SEED í† í°ì„ í¬í•¨í•˜ì—¬ ì´ë¯¸ì§€ ì…ë ¥ì„ ëª¨ë¸ì— ë…¸ì¶œ**

```math
\mathcal{L}_{\text{SEED}} =
\underbrace{ \| x - \hat{x} \|_2^2 }_{\text{recon}} +
\underbrace{ \lambda \cdot \mathcal{L}_{\text{perc}} }_{\text{perceptual}} +
\underbrace{ \mathcal{L}_{\text{VQ}} }_{\text{quantization}}
```
ì´ êµ¬ì¡° ë•ë¶„ì— ê¸°ì¡´ì˜ LLaMA ë˜ëŠ” í…ìŠ¤íŠ¸ ì „ìš© LLMì´ ë³„ë„ì˜ êµ¬ì¡° ë³€ê²½ ì—†ì´ ì´ë¯¸ì§€ ì…ë ¥ê³¼ ì¶œë ¥ì„ ì§ì ‘ ë‹¤ë£° ìˆ˜ ìˆê²Œ ë˜ì—ˆìœ¼ë©°, ë³µì¡í•œ cross-attention ëª¨ë“ˆì´ë‚˜ joint training ì—†ì´ë„ ë†’ì€ ì„±ëŠ¥ì„ ë‹¬ì„±í•¨.


## âš–ï¸ ê¸°ì¡´ ëª¨ë¸ê³¼ì˜ ë¹„êµ

| í•­ëª©      | ë³¸ ë…¼ë¬¸ (SEED-LLM)                             | Flamingo                              | BLIP-2                            |
|-----------|-----------------------------------------------|----------------------------------------|-----------------------------------|
| êµ¬ì¡°      | Unified single-stream (LLM only, no fusion)   | Dual-stream with cross-attention      | Two-stage (frozen vision encoder + Q-Former + LLM) |
| í•™ìŠµ ë°©ì‹ | Separate training (SEED tokenizer + LLaMA)     | Large-scale multimodal pretraining     | Vision-language pretraining + LLM alignment |
| ëª©ì       | Vision-text bidirectional generation & reasoning | Multimodal few-shot inference          | Efficient vision-to-text understanding |

---

## ğŸ“‰ ì‹¤í—˜ ë° ê²°ê³¼

* **ë°ì´í„°ì…‹**:
  - COCO Captions
  - VQAv2
  - Flickr30k
  - OKVQA
  - ImageNet (zero-shot classification)
  - DrawBench (ì´ë¯¸ì§€ ìƒì„± í‰ê°€)
  - MS-COCO (FID/Inception Score ë“± ì´ë¯¸ì§€ ìƒì„± ì§€í‘œ)

* **ë¹„êµ ëª¨ë¸**:
  - Flamingo
  - BLIP-2
  - GIT
  - LLaVA
  - PaLI

* **ì£¼ìš” ì„±ëŠ¥ ì§€í‘œ ë° ê²°ê³¼**:

| ëª¨ë¸         | VQA Accuracy | COCO BLEU-4 | FID â†“ (ìƒì„±) | ê¸°íƒ€ |
|--------------|--------------|-------------|--------------|------|
| **SEED-LLM** | 77.5         | 33.2        | **6.17**     | COCO caption 118M pretrain |
| Flamingo     | 74.7         | -           | 7.23         | 80B+ pretraining |
| BLIP-2       | 74.4         | 32.1        | -            | Image caption only |
| LLaVA        | 73.3         | -           | -            | Visual instruction tuning |

> **ì‹¤í—˜ ê²°ê³¼ ìš”ì•½ ë° í•´ì„**:  
> SEED-LLMì€ ìƒëŒ€ì ìœ¼ë¡œ ì ì€ pretraining ìì›ë§Œìœ¼ë¡œë„ ê¸°ì¡´ ê±°ëŒ€ ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ë“¤ë³´ë‹¤ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì˜€ìŒ. íŠ¹íˆ image captioningê³¼ VQA, ì´ë¯¸ì§€ ìƒì„±(FID) ì˜ì—­ì—ì„œ ëª¨ë‘ ê°•ë ¥í•œ ì„±ëŠ¥ì„ ì…ì¦í•˜ì˜€ìœ¼ë©°, unified ì‹œí€€ìŠ¤ ëª¨ë¸ì„ì—ë„ ë¶ˆêµ¬í•˜ê³  ë©€í‹°ëª¨ë‹¬ reasoningê³¼ generation ëª¨ë‘ì—ì„œ ë†’ì€ ìœ ì—°ì„±ê³¼ í‘œí˜„ë ¥ì„ ë³´ì„.

---

## âœ… ì¥ì  ë° í•œê³„

### **ì¥ì **:

* ê¸°ì¡´ í…ìŠ¤íŠ¸ ì „ìš© LLM(LLaMA)ì„ êµ¬ì¡° ìˆ˜ì • ì—†ì´ ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ë¡œ í™•ì¥ ê°€ëŠ¥
* ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ë¥¼ ì™„ì „íˆ ë™ì¼í•œ token spaceì—ì„œ ì²˜ë¦¬í•¨ìœ¼ë¡œì¨ token-level fusion ê°€ëŠ¥
* multimodal alignment pretraining ì—†ì´ë„ ë†’ì€ zero-shot ì„±ëŠ¥
* discrete visual tokenì´ autoregressive generationì— ì í•© (1D Causal êµ¬ì¡°)
* ì´ë¯¸ì§€ ìƒì„± í’ˆì§ˆ(FID)ë„ ìš°ìˆ˜í•¨

### **í•œê³„ ë° ê°œì„  ê°€ëŠ¥ì„±**:

* ì´ë¯¸ì§€ í‘œí˜„ì´ codebook ê¸°ë°˜ìœ¼ë¡œ ì œí•œë˜ë¯€ë¡œ, ë³µì¡í•œ high-frequency ì„¸ë¶€ ë¬˜ì‚¬ì—ëŠ” í•œê³„ ì¡´ì¬
* end-to-end fine-tuningì´ ì•„ë‹Œ separate trainingì´ë¼ ìµœì  joint representationì— ë„ë‹¬í•˜ì§€ ëª»í•  ê°€ëŠ¥ì„±
* diffusion ê¸°ë°˜ ìƒì„±ì— ë¹„í•´ ì°½ì˜ì„±ì´ë‚˜ ë‹¤ì–‘ì„± ë©´ì—ì„œ í‰ê°€ í•„ìš”

---

## ğŸ§  TL;DR â€“ í•œëˆˆì— ìš”ì•½

> SEED Tokenizerë¥¼ í†µí•´ ì´ë¯¸ì§€ë¥¼ discrete tokenìœ¼ë¡œ ë°”ê¾¸ê³ , ì´ë¥¼ í…ìŠ¤íŠ¸ì™€ í•¨ê»˜ LLaMAì— ë„£ì–´ "ì´ë¯¸ì§€ë¥¼ ë³´ê³  ê·¸ë¦¬ê³  ì´í•´í•˜ëŠ”" LLMìœ¼ë¡œ í™•ì¥í•œ ë…¼ë¬¸

| êµ¬ì„± ìš”ì†Œ     | ì„¤ëª… |
|--------------|------|
| í•µì‹¬ ëª¨ë“ˆ     | SEED Tokenizer (ViT + Q-Former + Codebook) |
| í•™ìŠµ ì „ëµ     | ë¶„ë¦¬ í•™ìŠµ (Tokenizer + LLaMA ë³„ë„) |
| ì „ì´ ë°©ì‹     | í…ìŠ¤íŠ¸ ì „ìš© LLM(LLaMA)ì— ì‹œê° token ì „ì´ |
| ì„±ëŠ¥/íš¨ìœ¨ì„±  | ì ì€ pretrainingìœ¼ë¡œë„ ë†’ì€ VQA, captioning, generation ì„±ëŠ¥ |

---

## ğŸ”— ì°¸ê³  ë§í¬ (References)

* [ğŸ“„ arXiv ë…¼ë¬¸](https://arxiv.org/abs/2310.01218)
* [ğŸ’» GitHub](https://github.com/THUDM/SEED)
* [ğŸ“ˆ Papers with Code](https://paperswithcode.com/paper/making-llama-see-and-draw-with-seed-tokenizer)

## ë‹¤ìŒ ë…¼ë¬¸: flamingo
