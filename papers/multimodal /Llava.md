# ðŸ“˜ [Visual Instruction Tuning: LLaVA (Large Language and Vision Assistant)]

## 1. ê°œìš” (Overview)

- **ì œëª©**: Visual Instruction Tuning  
- **ì €ìž**: Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee  
- **ì†Œì†**: University of Wisconsin-Madison, Microsoft Research, UC Davis  
- **í•™íšŒ**: arXiv preprint (2023)  
- **ë§í¬**:  
  - [arXiv](https://arxiv.org/abs/2304.08485)  
  - [GitHub](https://github.com/haotian-liu/LLaVA)  
  - [Papers with Code](https://paperswithcode.com/paper/visual-instruction-tuning)

> ìµœê·¼ ë©€í‹°ëª¨ë‹¬ AI ì‹œìŠ¤í…œì— ëŒ€í•œ ìˆ˜ìš”ê°€ ì¦ê°€í•¨ì— ë”°ë¼, GPTì™€ ê°™ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì— ì‹œê°ì  ìž…ë ¥ì„ ê²°í•©í•˜ëŠ” ì‹œë„ê°€ í™œë°œížˆ ì´ë¤„ì§€ê³  ìžˆë‹¤.  
> LLaVAëŠ” ì´ëŸ¬í•œ íŠ¸ë Œë“œ ì†ì—ì„œ, OpenAIì˜ CLIP ë¹„ì „ ì¸ì½”ë”ì™€ Metaì˜ LLaMA ì–¸ì–´ ëª¨ë¸ì„ ì—°ê²°í•˜ì—¬ ë©€í‹°ëª¨ë‹¬ ì¸ìŠ¤íŠ¸ëŸ­ì…˜ íŠœë‹ì„ ì ìš©í•œ ëŒ€í‘œì ì¸ ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸ì´ë‹¤.  
> ì´ ë…¼ë¬¸ì€ LLaVAë¥¼ í†µí•´ ì‹œê° ì •ë³´ì™€ ìžì—°ì–´ ëª…ë ¹ì„ ê²°í•©í•œ ì§ˆì˜ì‘ë‹µ, ì‹œê°ì  ì„¤ëª…, ì´ë¯¸ì§€ ê¸°ë°˜ ëŒ€í™” ë“± ë‹¤ì–‘í•œ ìž‘ì—…ì—ì„œ ê°•ë ¥í•œ ì„±ëŠ¥ì„ ë³´ì´ë©°, ë¹„ê³µê°œ ëª¨ë¸(GPT-4 with Vision ë“±)ì— ëŒ€ì‘í•  ìˆ˜ ìžˆëŠ” ê³ ì„±ëŠ¥ ì˜¤í”ˆ ëŒ€ì•ˆìž„ì„ ë³´ì—¬ì¤€ë‹¤.

---

## 2. ë¬¸ì œ ì •ì˜ (Problem Formulation)

**ë¬¸ì œ ë° ê¸°ì¡´ í•œê³„**:

* ê¸°ì¡´ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì€ ìžì—°ì–´ ì²˜ë¦¬ì— ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì˜€ìœ¼ë‚˜, ì‹œê° ì •ë³´ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ì—†ì–´ **ì´ë¯¸ì§€ì™€ ì–¸ì–´ë¥¼ ê²°í•©í•œ ë©€í‹°ëª¨ë‹¬ ìƒí˜¸ìž‘ìš©**ì—ëŠ” í•œê³„ê°€ ìžˆìŒ.  
* CLIP ë“± ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ì€ ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ìŒì˜ í‘œí˜„ í•™ìŠµì—ëŠ” ê°•ì ì´ ìžˆìœ¼ë‚˜, **ëŒ€í™”í˜• ì§ˆì˜ì‘ë‹µ, ë‹¤ë‹¨ê³„ ì¶”ë¡ , ê³ ì°¨ì› ë©€í‹°í„´ ìƒí˜¸ìž‘ìš©**ì—ëŠ” ì í•©í•˜ì§€ ì•ŠìŒ.  
* ê¸°ì¡´ ë¹„ì „-ì–¸ì–´ ëª¨ë¸ ëŒ€ë¶€ë¶„ì€ **ì œí•œëœ task-specific í•™ìŠµ** ë˜ëŠ” **ì‚¬ì „ ì •ì˜ëœ ì‘ë‹µë§Œ ìƒì„±**í•˜ë©°, ë²”ìš©ì„±ê³¼ ìœ ì—°ì„±ì´ ë¶€ì¡±í–ˆìŒ.

**ì œì•ˆ ë°©ì‹**:

* LLaVAëŠ” **LLaMA ì–¸ì–´ ëª¨ë¸ê³¼ CLIP ë¹„ì „ ì¸ì½”ë”ë¥¼ ì—°ê²°**í•˜ì—¬, ì‹œê°ì  ì •ë³´ë¥¼ ìžì—°ì–´ë¡œ ì´í•´í•˜ê³  ì‘ë‹µí•˜ëŠ” **ë²”ìš© ë©€í‹°ëª¨ë‹¬ ì–´ì‹œìŠ¤í„´íŠ¸**ë¥¼ ì œì•ˆí•¨.  
* íŠ¹ížˆ, í…ìŠ¤íŠ¸ ê¸°ë°˜ instruction tuning ë°©ì‹ì„ ì‹œê° ì •ë³´ì— í™•ìž¥í•œ **Visual Instruction Tuning**ì„ ë„ìž…í•˜ì—¬, ì‚¬ìš©ìž ì§ˆì˜ì— ë§žì¶° ì´ë¯¸ì§€ ê¸°ë°˜ ìžì—°ì–´ ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ìžˆë„ë¡ í•™ìŠµí•¨.  
* ì‹œê° ì¸ì½”ë”ì˜ ì¶œë ¥ì„ **íˆ¬ëª…í•˜ê²Œ language modelì˜ ìž…ë ¥ í† í° ê³µê°„ìœ¼ë¡œ íˆ¬ì˜(project)**í•˜ì—¬, **ê¸°ì¡´ LLMì„ ìˆ˜ì • ì—†ì´ í™œìš©**í•  ìˆ˜ ìžˆëŠ” êµ¬ì¡°ë¥¼ ì„¤ê³„í•¨.

> â€» **í•µì‹¬ ê°œë… ì •ì˜**  
> - **Visual Instruction Tuning**: í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ìŒì„ ê¸°ë°˜ìœ¼ë¡œ instruction-following í˜•íƒœë¡œ ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ” ë°©ì‹.  
> - **Projection Layer**: CLIPì˜ ì‹œê° í”¼ì²˜(768-d)ë¥¼ LLaMAì˜ í…ìŠ¤íŠ¸ ìž„ë² ë”© ì°¨ì›(4096-d)ìœ¼ë¡œ ì„ í˜• ë§¤í•‘í•˜ëŠ” ê³„ì¸µ.  
> - **Multimodal Dialogue**: í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€ ìž…ë ¥ì„ ë°”íƒ•ìœ¼ë¡œ ë‹¤ì¤‘ í„´(ë©€í‹°í„´)ì˜ ìžì—°ì–´ ëŒ€í™” ìƒì„±.

---

## 3. ëª¨ë¸ êµ¬ì¡° (Architecture)

### ì „ì²´ êµ¬ì¡°

![LLaVA êµ¬ì¡°](../images/llava_structure.png)

LLaVAëŠ” "Large Language and Vision Assistant"ë¼ëŠ” ì´ë¦„ ê·¸ëŒ€ë¡œ, ì‹œê° ì •ë³´(ì´ë¯¸ì§€)ì™€ ì–¸ì–´(í…ìŠ¤íŠ¸)ë¥¼ ë™ì‹œì— ì´í•´í•˜ê³  ì‘ë‹µí•  ìˆ˜ ìžˆëŠ” ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ì´ë‹¤.  
ì´ ëª¨ë¸ì€ ì„¸ ê°€ì§€ ì£¼ìš” êµ¬ì„± ìš”ì†Œë¡œ ì´ë£¨ì–´ì§„ë‹¤:

1. **Vision Encoder (CLIP ViT-L/14)**: ì´ë¯¸ì§€ë¥¼ ë²¡í„°ë¡œ ìž„ë² ë”©
2. **Projection Layer**: ì‹œê° ìž„ë² ë”©ì„ ì–¸ì–´ ëª¨ë¸ ìž…ë ¥ì— ë§žê²Œ ì„ í˜• ë³€í™˜
3. **Language Model (LLaMA)**: ë©€í‹°ëª¨ë‹¬ ìž…ë ¥ì„ ë°”íƒ•ìœ¼ë¡œ ìžì—°ì–´ ì‘ë‹µ ìƒì„±

ìž…ë ¥ì€ `(ì´ë¯¸ì§€, í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸)` í˜•íƒœì´ë©°, ì¶œë ¥ì€ ìˆœìˆ˜ í…ìŠ¤íŠ¸ ì‘ë‹µì´ë‹¤. ì´ ì „ì²´ íë¦„ì€ **instruction-following ë©€í‹°ëª¨ë‹¬ ëŒ€í™”**ë¥¼ êµ¬í˜„í•œë‹¤.

---

### ðŸ’  í•µì‹¬ ëª¨ë“ˆ ë˜ëŠ” êµ¬ì„± ìš”ì†Œ

#### ðŸ“Œ Vision Encoder: CLIP ViT-L/14

* OpenAIì˜ **CLIP (Contrastive Language-Image Pretraining)** ëª¨ë¸ ì¤‘ Vision Transformer (ViT-L/14)ë¥¼ ì‚¬ìš©.
* ì´ë¯¸ì§€ $I$ëŠ” $224 \times 224$ í•´ìƒë„ë¡œ ìž…ë ¥ë˜ë©°, patch ë‹¨ìœ„ë¡œ ë¶„í•´ëœ í›„ ìž„ë² ë”©ë¨.
* ìµœì¢…ì ìœ¼ë¡œ ì–»ëŠ” ê²ƒì€ **[CLS] í† í°ì˜ ìž„ë² ë”© ë²¡í„°** $v \in \mathbb{R}^{768}$ë¡œ, ì´ë¯¸ì§€ ì „ì²´ë¥¼ ëŒ€í‘œí•˜ëŠ” ì••ì¶• í‘œí˜„ì´ë‹¤.
* ì´ í‘œí˜„ì€ CLIP í…ìŠ¤íŠ¸ ì¸ì½”ë”ì™€ì˜ ê³µë™ í•™ìŠµì„ í†µí•´ ìžì—° ì–¸ì–´ ê³µê°„ê³¼ ì˜ë¯¸ì ìœ¼ë¡œ ì •ë ¬(aligned)ëœ ìƒíƒœìž„.

> ê¸°ì¡´ ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ë“¤ê³¼ ë‹¬ë¦¬, LLaVAëŠ” **ì´ë¯¸ í•™ìŠµëœ CLIPì˜ í‘œí˜„ë ¥ì„ ì§ì ‘ í™œìš©**í•˜ì—¬ zero-shot generalizationì„ ê°•í™”í•¨.

---

#### ðŸ“Œ Projection Layer: Vision-to-Text Token Embedding

CLIPì˜ ì¶œë ¥ $v$ëŠ” LLaMAì˜ ìž…ë ¥ í† í°ê³¼ ì°¨ì›ì´ ë‹¤ë¥´ê¸° ë•Œë¬¸ì—, ì§ì ‘ ì—°ê²°í•  ìˆ˜ ì—†ë‹¤.  
ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ LLaVAëŠ” í•™ìŠµ ê°€ëŠ¥í•œ **ì„ í˜• ë³€í™˜ ê³„ì¸µ (projection layer)**ë¥¼ ë„ìž…í•œë‹¤.

* ìˆ˜ì‹ìœ¼ë¡œ í‘œí˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤:

  $z = W \cdot v + b$

  ì—¬ê¸°ì„œ  
  - $v \in \mathbb{R}^{768}$: CLIPì˜ [CLS] ì¶œë ¥  
  - $W \in \mathbb{R}^{4096 \times 768}$: íˆ¬ì˜(weight) í–‰ë ¬  
  - $b \in \mathbb{R}^{4096}$: ë°”ì´ì–´ìŠ¤  
  - $z \in \mathbb{R}^{4096}$: LLaMA ìž„ë² ë”© ì°¨ì›ìœ¼ë¡œ í™•ìž¥ëœ ë²¡í„°

* ì´ ë³€í™˜ì€ CLIPì˜ ì‹œê° ì •ë³´ê°€ ë§ˆì¹˜ **í•˜ë‚˜ì˜ ê°€ìƒ í† í°(token)**ì²˜ëŸ¼ LLaMA ìž…ë ¥ì— ì‚½ìž…ë  ìˆ˜ ìžˆë„ë¡ í•´ì¤€ë‹¤.
* ì‹¤ì œë¡œëŠ” ì—¬ëŸ¬ ê°œì˜ íˆ¬ì˜ ë²¡í„°ë¥¼ ë§Œë“¤ ìˆ˜ ìžˆìœ¼ë©°, ì´ ìˆ˜ë¥¼ $n_{image\ tokens}$ì´ë¼ í•˜ë©´:

  $\text{Image tokens} = \{ z_1, z_2, \dots, z_{n} \} \subset \mathbb{R}^{4096}$

---

#### ðŸ“Œ Language Model: LLaMA (7B ë˜ëŠ” 13B)

* Metaì˜ **LLaMA-7B ë˜ëŠ” 13B** ëª¨ë¸ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•œë‹¤.
* ì´ë¯¸ì§€ íˆ¬ì˜ í† í°ê³¼ í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ê°€ ì‹œí€€ìŠ¤ë¡œ ì´ì–´ì§„ í›„, LLaMAëŠ” ì´ë¥¼ ìž…ë ¥ìœ¼ë¡œ ë°›ì•„ ìžì—°ì–´ ì‘ë‹µì„ ìƒì„±í•œë‹¤:

  $\text{Input to LLaMA: } \quad [\text{Image Tokens}] + [\text{Text Prompt Tokens}]$

* ëª¨ë¸ì€ ê¸°ì¡´ LLaMA ì•„í‚¤í…ì²˜ë¥¼ ìˆ˜ì •í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì—, ì´ë¯¸ í•™ìŠµëœ LLMì˜ ì–¸ì–´ì  ì¶”ë¡  ëŠ¥ë ¥ì„ ê·¸ëŒ€ë¡œ ìœ ì§€í•  ìˆ˜ ìžˆë‹¤.

> ê²°ê³¼ì ìœ¼ë¡œ, LLaVAëŠ” ë©€í‹°ëª¨ë‹¬ taskë¥¼ ìœ„í•´ **ìµœì†Œí•œì˜ ë³€ê²½ë§Œ ê°€í•´ì§„ ë§¤ìš° íš¨ìœ¨ì ì¸ êµ¬ì¡°**ë¥¼ ê°–ëŠ”ë‹¤.

---

### ðŸ” ì²˜ë¦¬ íë¦„ ìš”ì•½

1. ìž…ë ¥ ì´ë¯¸ì§€ $I$ â†’ CLIP ViT â†’ ì‹œê° í”¼ì²˜ $v$
2. $v$ â†’ Projection Layer â†’ ì´ë¯¸ì§€ í† í° $z$
3. í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ $T$ì™€ í•¨ê»˜ $z$ë¥¼ LLaMAì— ìž…ë ¥
4. LLaMA â†’ ìµœì¢… ì‘ë‹µ í…ìŠ¤íŠ¸ $\hat{y}$ ìƒì„±

$\hat{y} = \text{LLaMA}([z_1, z_2, ..., z_n] + [T_1, T_2, ..., T_m])$

---

> ðŸ“Œ **ìš”ì•½**:  
> LLaVAëŠ” **ì´ë¯¸ì§€ë¥¼ ì–¸ì–´ ëª¨ë¸ì˜ ìž…ë ¥ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” êµ¬ì¡°ì  ì¸í„°íŽ˜ì´ìŠ¤ë¥¼ ì„¤ê³„**í•˜ê³ , CLIPê³¼ LLaMAë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ê²°í•©í•˜ì—¬ instruction-following ê¸°ë°˜ì˜ ê°•ë ¥í•œ ë¹„ì „-ì–¸ì–´ ì–´ì‹œìŠ¤í„´íŠ¸ë¥¼ êµ¬í˜„í•˜ì˜€ë‹¤.  
> ì´ êµ¬ì¡°ëŠ” ë‹¨ìˆœí•˜ë©´ì„œë„ í™•ìž¥ì„± ë†’ì€ ì ‘ê·¼ ë°©ì‹ì´ë©°, ë‹¤ì–‘í•œ downstream ë©€í‹°ëª¨ë‹¬ ì‘ìš©ì— ì†ì‰½ê²Œ ì ìš© ê°€ëŠ¥í•˜ë‹¤.


---
## âš–ï¸ ê¸°ì¡´ ëª¨ë¸ê³¼ì˜ ë¹„êµ

| í•­ëª©        | ë³¸ ë…¼ë¬¸ (LLaVA)                     | BLIP-2                          | Flamingo                          |
| --------- | ----------------------------------- | ------------------------------ | --------------------------------- |
| êµ¬ì¡°        | CLIP ViT + Projection + LLaMA         | ViT-G + Q-Former + OPT/FlanT5   | Perceiver Resampler + Chinchilla |
| í•™ìŠµ ë°©ì‹     | Visual Instruction Tuning (Supervised) | Two-stage Pretrain + Fine-tune | Frozen LM + Few-shot Adaptation  |
| ëª©ì         | Open-source V-L Assistant             | Vision-to-Language Generation  | General Multimodal Few-shot Tasks |

> LLaVAëŠ” êµ¬ì¡°ì  ë‹¨ìˆœì„±ê³¼ ê³µê°œì„±(Open-Source)ì„ ë™ì‹œì— ì¶”êµ¬í•˜ë©°, instruction-followingì— ìµœì í™”ë˜ì–´ ìžˆìŒ.  
> ë°˜ë©´, BLIP-2ëŠ” ë‹¤ì–‘í•œ Vision-to-Language taskì— ë²”ìš©ì ì´ë©°, FlamingoëŠ” large-scale, few-shot ì„¤ì •ì— ê°•ì ì„ ê°€ì§.

---

## ðŸ“‰ ì‹¤í—˜ ë° ê²°ê³¼

* **ë°ì´í„°ì…‹**:
  - COCO Captioning
  - VQAv2 (Visual Question Answering)
  - ScienceQA (ì´ë¯¸ì§€ ê¸°ë°˜ ë¬¸ì œ í’€ì´)
  - GPT-4 ê¸°ë°˜ Human Evaluation

* **ë¹„êµ ëª¨ë¸**:
  - BLIP-2 (OPT/FlanT5)
  - MiniGPT-4
  - Flamingo (Chinchilla ê¸°ë°˜)
  - OpenAI GPT-4 with Vision (ì°¸ê³ ìš©)

* **ì£¼ìš” ì„±ëŠ¥ ì§€í‘œ ë° ê²°ê³¼**:

| ëª¨ë¸           | VQAv2 Accuracy | ScienceQA Accuracy | GPT-4 í‰ê°€ (ìƒëŒ€ ì ìˆ˜) |
| ------------- | --------------- | ------------------- | -------------------- |
| ë³¸ ë…¼ë¬¸ (LLaVA) | 56.9%          | **92.5%**            | **+1.0 (vs MiniGPT-4)** |
| BLIP-2         | 55.3%          | 83.0%               | -                    |
| MiniGPT-4      | 56.1%          | 89.5%               | baseline             |
| Flamingo       | 56.3%          | -                   | -                    |

> LLaVAëŠ” instruction tuning ê¸°ë°˜ì˜ ê°„ë‹¨í•œ êµ¬ì¡°ìž„ì—ë„ ë¶ˆêµ¬í•˜ê³  **ScienceQA ê°™ì€ reasoning ì¤‘ì‹¬ì˜ ë©€í‹°ëª¨ë‹¬ ê³¼ì œì—ì„œ ê°€ìž¥ ë†’ì€ ì •í™•ë„**ë¥¼ ë³´ì˜€ë‹¤.  
> íŠ¹ížˆ, **GPT-4 ê¸°ë°˜ ì‚¬ëžŒ í‰ê°€ì—ì„œ MiniGPT-4 ëŒ€ë¹„ ì‘ë‹µì˜ ì§ˆì´ ë†’ê²Œ í‰ê°€**ë˜ì—ˆìœ¼ë©°, ì‹¤ìš©ì„± ë° ì‘ë‹µ ì¼ê´€ì„±ì—ì„œë„ ìš°ìœ„ë¥¼ ë³´ìž„.

---

## âœ… ìž¥ì  ë° í•œê³„

### **ìž¥ì **:

* ðŸ’¡ **ì‹¬í”Œí•œ êµ¬ì¡°**: ê¸°ì¡´ LLMì„ ê±°ì˜ ìˆ˜ì •í•˜ì§€ ì•Šê³  vision ìž…ë ¥ë§Œ ì—°ë™í•´ í™œìš© ê°€ëŠ¥  
* ðŸ“š **ì˜¤í”ˆì†ŒìŠ¤ ê¸°ë°˜**: CLIP + LLaMA ëª¨ë‘ ê³µê°œëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í•™ê³„/ì‚°ì—…ì—ì„œ ìž¬í˜„ ìš©ì´  
* ðŸ“ˆ **ë‹¤ì–‘í•œ ìž‘ì—…ì—ì„œ ê°•ë ¥í•œ ì„±ëŠ¥**: captioning, QA, instruction-followingê¹Œì§€ ì»¤ë²„  
* ðŸ”„ **Instruction Tuning í™œìš©**: ì¼ë°˜ì ì¸ ìžì—°ì–´ ëª…ë ¹í˜• taskì— ìœ ì—°í•˜ê²Œ ëŒ€ì‘ ê°€ëŠ¥

### **í•œê³„ ë° ê°œì„  ê°€ëŠ¥ì„±**:

* ðŸ§  **ë©€í‹° ì´ë¯¸ì§€, ë¹„ë””ì˜¤ ë“± ì‹œê³„ì—´ ìž…ë ¥ì—ëŠ” ë¯¸ëŒ€ì‘**  
* âš ï¸ **ë³µìž¡í•œ ì‹œê° reasoningì€ í•œê³„ ìžˆìŒ (e.g., spatial reasoning, object counting)**  
* ðŸ§ª CLIP representationì´ ê³ ì •ë˜ì–´ ìžˆì–´ ì‹œê° ì •ë³´ì˜ fine-grained ì¡°ì •ì´ ì–´ë ¤ì›€  
* ðŸ”§ Multimodal alignment ë¬¸ì œê°€ ì™„ì „ížˆ í•´ê²°ëœ ê²ƒì€ ì•„ë‹˜ (íˆ¬ì˜ ë‹¨ì¼ ì¸µ ì‚¬ìš©)

---

## ðŸ§  TL;DR â€“ í•œëˆˆì— ìš”ì•½

> LLaVAëŠ” CLIP + LLaMAë¥¼ projection layerë¡œ ì—°ê²°í•˜ê³ , visual instruction tuningì„ í†µí•´ GPT-4 ìˆ˜ì¤€ì˜ ë©€í‹°ëª¨ë‹¬ ì„±ëŠ¥ì„ ì§€í–¥í•˜ëŠ” ì˜¤í”ˆì†ŒìŠ¤ ë©€í‹°ëª¨ë‹¬ ì–´ì‹œìŠ¤í„´íŠ¸ì´ë‹¤.

| êµ¬ì„± ìš”ì†Œ    | ì„¤ëª… |
| ----------- | ---- |
| í•µì‹¬ ëª¨ë“ˆ    | CLIP Vision Encoder + Linear Projection + LLaMA |
| í•™ìŠµ ì „ëžµ    | Text-Image ê¸°ë°˜ Instruction Tuning (Supervised) |
| ì „ì´ ë°©ì‹    | Vision â†’ LLM Input Embedding ê³µê°„ìœ¼ë¡œ ì„ í˜• íˆ¬ì˜ |
| ì„±ëŠ¥/íš¨ìœ¨ì„± | êµ¬ì¡°ê°€ ë‹¨ìˆœí•˜ê³  ìž¬í˜„ ê°€ëŠ¥í•˜ë©°, ë‹¤ì–‘í•œ ë©€í‹°ëª¨ë‹¬ taskì—ì„œ ê²½ìŸë ¥ í™•ë³´ |

---

## ðŸ”— ì°¸ê³  ë§í¬ (References)

* [ðŸ“„ arXiv ë…¼ë¬¸](https://arxiv.org/abs/2304.08485)
* [ðŸ’» GitHub](https://github.com/haotian-liu/LLaVA)
* [ðŸ“ˆ Papers with Code](https://paperswithcode.com/paper/visual-instruction-tuning)

---

## ë‹¤ìŒ ë…¼ë¬¸:

> flamingo
