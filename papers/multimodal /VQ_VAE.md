# ğŸ“˜ [Neural Discrete Representation Learning]

## 1. ê°œìš” (Overview)

* **ì œëª©**: Neural Discrete Representation Learning  
* **ì €ì**: Aaron van den Oord, Oriol Vinyals, Koray Kavukcuoglu  
* **ì†Œì†**: DeepMind  
* **í•™íšŒ**: NeurIPS 2017  
* **ë§í¬**: [arXiv](https://arxiv.org/abs/1711.00937) / [GitHub](https://github.com/deepmind/sonnet/blob/v2/examples/vqvae_example.ipynb) / [Papers with Code](https://paperswithcode.com/paper/neural-discrete-representation-learning)

> ì´ ë…¼ë¬¸ì€ Variational Autoencoderì˜ ì—°ì†ì ì¸ latent ê³µê°„ì„ ëŒ€ì²´í•˜ê¸° ìœ„í•´ **discrete latent space**ë¥¼ ì‚¬ìš©í•˜ëŠ” ìƒˆë¡œìš´ êµ¬ì¡°ì¸ **VQ-VAE(Vector Quantized VAE)**ë¥¼ ì œì•ˆí•œë‹¤.  
> ì´ë¯¸ì§€, ìŒì„±, ë¹„ë””ì˜¤ ë“± ë‹¤ì–‘í•œ ë°ì´í„°ì— ì ìš© ê°€ëŠ¥í•˜ë©°, latent ê³µê°„ì—ì„œ **ì‹œí€€ìŠ¤ ëª¨ë¸**ì„ í•™ìŠµí•  ìˆ˜ ìˆë‹¤ëŠ” ì ì—ì„œ ì¤‘ìš”í•œ ê¸°ì—¬ë¥¼ í•œë‹¤.  
> **Discrete latent + strong prior**ì˜ ì¡°í•©ì´ ì´í›„ VQ-GAN, DALLÂ·E ë“± ë‹¤ì–‘í•œ ìƒì„± ëª¨ë¸ì˜ ê¸°ë°˜ì´ ë˜ë¯€ë¡œ í•„ë… ê°€ì¹˜ê°€ ìˆë‹¤.


## 2. ë¬¸ì œ ì •ì˜ (Problem Formulation)

**ë¬¸ì œ ë° ê¸°ì¡´ í•œê³„**:

* ê¸°ì¡´ VAE(Variational AutoEncoder) êµ¬ì¡°ì—ì„œëŠ” latent spaceê°€ **ì—°ì†ì (continuous)**ì´ê¸° ë•Œë¬¸ì—, latent ë²¡í„°ì— ëŒ€í•œ **í•´ì„ ê°€ëŠ¥ì„±**ì´ ë‚®ê³ , **prior**ë¥¼ í•™ìŠµí•˜ê¸°ê°€ ì–´ë µë‹¤.
* ì—°ì†ì ì¸ latent spaceëŠ” ì‹œí€€ìŠ¤ ëª¨ë¸(PixelCNN, WaveNet ë“±)ì„ ì ìš©í•˜ê¸° ì–´ë µê³ , **ìƒ˜í”Œì˜ ë‹¤ì–‘ì„±ê³¼ í’ˆì§ˆ**ì„ ëª¨ë‘ í™•ë³´í•˜ê¸° ì–´ë ¤ì› ë‹¤.
* RNNì´ë‚˜ CNN ê¸°ë°˜ì˜ ì‹œí€€ìŠ¤ ëª¨ë¸ì€ ì…ë ¥ ì „ì²´ë¥¼ ì••ì¶•í•œ continuous representationì— ì˜ì¡´í•˜ë©°, ì´ë¡œ ì¸í•´ ì •ë³´ ì†ì‹¤ì´ ë°œìƒí•˜ê±°ë‚˜ í•™ìŠµì´ ë¶ˆì•ˆì •í•´ì§ˆ ìˆ˜ ìˆë‹¤.

**ì œì•ˆ ë°©ì‹**:

* encoderê°€ ì¶œë ¥í•œ continuous latent vectorë¥¼ **ê³ ì •ëœ codebook entryë“¤ ì¤‘ ê°€ì¥ ê°€ê¹Œìš´ discrete token**ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ë°©ì‹(= **Vector Quantization**)ì„ ë„ì…í•œë‹¤.
* decoderëŠ” ì´ **quantizedëœ discrete latent token**ì„ ê¸°ë°˜ìœ¼ë¡œ ì›ë˜ ì…ë ¥ì„ ë³µì›í•˜ê³ , ì¶”ê°€ë¡œ **autoregressive prior ëª¨ë¸**(ex. PixelCNN)ì„ í†µí•´ token ì‹œí€€ìŠ¤ì˜ êµ¬ì¡°ë¥¼ í•™ìŠµí•  ìˆ˜ ìˆë‹¤.
* ì´ êµ¬ì¡°ëŠ” discrete latent ê³µê°„ì—ì„œ **íš¨ìœ¨ì ì¸ í•™ìŠµ**ê³¼ **ìƒ˜í”Œ í’ˆì§ˆ í–¥ìƒ**, **ì‹œí€€ìŠ¤ ëª¨ë¸ê³¼ì˜ ê²°í•©**ì„ ëª¨ë‘ ê°€ëŠ¥í•˜ê²Œ í•œë‹¤.

> â€» **í•µì‹¬ ê°œë… ì •ì˜**:

* **Vector Quantization (VQ)**:  
  continuous vectorë¥¼ ê°€ì¥ ê°€ê¹Œìš´ codebook entryë¡œ ì¹˜í™˜í•˜ëŠ” ê³¼ì •. ì´ ê³¼ì •ì„ í†µí•´ discrete representationì„ ì–»ê²Œ ë¨.

* **Discrete Latent Representation**:  
  ì—°ì†ì ì¸ ê³µê°„ì´ ì•„ë‹ˆë¼, ì‚¬ì „ ì •ì˜ëœ ë²¡í„° ì§‘í•©(codebook)ì—ì„œ ì„ íƒëœ ë²¡í„°ë“¤ë¡œ êµ¬ì„±ëœ í‘œí˜„. ì´ë¥¼ í†µí•´ ì‹œí€€ìŠ¤ ëª¨ë¸ë§ì— ì í•©í•œ êµ¬ì¡° í™•ë³´.

* **Commitment Loss**:  
  encoderê°€ íŠ¹ì • codebook entryì— "ì±…ì„ì§€ê³ " ë§¤í•‘ë˜ë„ë¡ ìœ ë„í•˜ëŠ” ì†ì‹¤ í•­. encoderì™€ codebook ê°„ì˜ ì•ˆì •ì ì¸ í•™ìŠµì„ ìœ ë„í•¨.

* **Straight-Through Estimator**:  
  backpropagationì´ ë¶ˆê°€ëŠ¥í•œ quantization ë‹¨ê³„ì—ì„œ gradientë¥¼ ê·¼ì‚¬ì ìœ¼ë¡œ ì „ë‹¬í•˜ê¸° ìœ„í•œ ê¸°ë²•.



## 3. ëª¨ë¸ êµ¬ì¡° (Architecture)

### ì „ì²´ êµ¬ì¡°

![ëª¨ë¸ êµ¬ì¡°](https://raw.githubusercontent.com/deepmind/sonnet/master/docs/_images/vqvae.png)  
> ì¶œì²˜: DeepMind Sonnet

VQ-VAE(Vector Quantized Variational Autoencoder)ëŠ” ê¸°ë³¸ì ì¸ autoencoder êµ¬ì¡°ì—ì„œ ì—°ì†ì ì¸ latent vector ëŒ€ì‹ , **ê³ ì •ëœ ë²¡í„° ì§‘í•©(codebook)**ì—ì„œ ì„ íƒëœ **ì´ì‚°ì ì¸(discrete) í† í°**ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ í•µì‹¬ì´ë‹¤.  

ì „ì²´ íŒŒì´í”„ë¼ì¸ì€ ë‹¤ìŒê³¼ ê°™ì€ ì„¸ ê°€ì§€ ì£¼ìš” ë‹¨ê³„ë¡œ êµ¬ì„±ëœë‹¤:

1. **Encoder**: ì…ë ¥ ë°ì´í„°ë¥¼ continuous latent vectorë¡œ ë³€í™˜  
2. **Vector Quantizer**: ì´ë¥¼ ê°€ì¥ ê°€ê¹Œìš´ discrete codebook entryë¡œ ì¹˜í™˜  
3. **Decoder**: discrete latentë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì›ë³¸ ë°ì´í„°ë¥¼ ë³µì›  

ì´ëŸ¬í•œ êµ¬ì¡°ëŠ” discrete latent spaceì— ì í•©í•œ **prior ëª¨ë¸(ì˜ˆ: PixelCNN)**ì„ ê²°í•©í•  ìˆ˜ ìˆê²Œ í•˜ì—¬, ë” ë‚˜ì€ generative ëª¨ë¸ì„ êµ¬ì„±í•œë‹¤.

---

### ğŸ’  í•µì‹¬ ëª¨ë“ˆ ë° ìˆ˜ì‹ ì„¤ëª…

#### ğŸ“Œ Encoder: $x \rightarrow z_e(x)$

* ì…ë ¥ $x$ (ì˜ˆ: ì´ë¯¸ì§€, ìŒì„± ë“±)ë¥¼ ë°›ì•„, encoder ë„¤íŠ¸ì›Œí¬ëŠ” ì´ë¥¼ continuous latent vector $z_e(x)$ë¡œ ë§¤í•‘í•œë‹¤.
* $z_e(x) \in \mathbb{R}^D$ëŠ” ì ì¬ê³µê°„(latent space) ìƒì˜ ë²¡í„°ë¡œ, ì´í›„ ì–‘ìí™”(quantization)ë¥¼ ê±°ì³ discrete í‘œí˜„ìœ¼ë¡œ ë³€í™˜ëœë‹¤.

#### ğŸ“Œ Vector Quantizer: $z_e(x) \rightarrow z_q(x)$

* codebook (ë˜ëŠ” embedding space) $e = \{ e_k \}_{k=1}^K$, $e_k \in \mathbb{R}^D$ëŠ” í•™ìŠµ ê°€ëŠ¥í•œ Kê°œì˜ ë²¡í„°ë¡œ êµ¬ì„±ëœë‹¤.
* encoderì˜ ì¶œë ¥ $z_e(x)$ëŠ” í•´ë‹¹ codebookì—ì„œ ê°€ì¥ ê°€ê¹Œìš´ entry $e_k$ë¡œ ì¹˜í™˜ë˜ë©°, ì´ë¥¼ í†µí•´ discrete latent í‘œí˜„ $z_q(x)$ë¥¼ ì–»ëŠ”ë‹¤:

$$
z_q(x) = e_k \quad \text{where} \quad k = \arg\min_j \| z_e(x) - e_j \|_2
$$

* ì´ ê³¼ì •ì€ **ë²¡í„° ì–‘ìí™” (Vector Quantization)**ë¼ê³  í•˜ë©°, continuous latent spaceë¥¼ discreteí•œ í‘œí˜„ ê³µê°„ìœ¼ë¡œ íˆ¬ì˜í•˜ëŠ” ì—­í• ì„ í•œë‹¤.

#### ğŸ“Œ Decoder: $z_q(x) \rightarrow \hat{x}$

* decoderëŠ” ì–‘ìí™”ëœ discrete latent vector $z_q(x)$ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„, ì›ë˜ ì…ë ¥ $x$ë¥¼ ë³µì›í•œ $\hat{x}$ë¥¼ ì¶œë ¥í•œë‹¤.
* ì¬êµ¬ì„± ì†ì‹¤ì€ ì•„ë˜ì™€ ê°™ì€ ë¡œê·¸ ê°€ëŠ¥ë„(loss likelihood)ë¥¼ ìµœëŒ€í™”í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì •ì˜ëœë‹¤:

$$
\mathcal{L}_{\text{recon}} = -\log p(x \mid z_q(x))
$$

---

### ğŸ“Œ ì „ì²´ í•™ìŠµ ëª©í‘œ: Loss Function

VQ-VAEëŠ” ë‹¤ìŒì˜ ì„¸ êµ¬ì„± ìš”ì†Œë¡œ ì´ë£¨ì–´ì§„ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•œë‹¤:

$\mathcal{L} = \log p(x \mid z_q(x)) + \| \text{sg}[z_e(x)] - e \|_2^2 + \beta \| z_e(x) - \text{sg}[e] \|_2^2$


#### (1) ì¬êµ¬ì„± ì†ì‹¤ $\log p(x \mid z_q(x))$

* decoderê°€ ì–‘ìí™”ëœ latentë¡œë¶€í„° ì›ë³¸ ì…ë ¥ $x$ë¥¼ ì˜ ë³µì›í•  ìˆ˜ ìˆë„ë¡ ìœ ë„í•¨.

#### (2) codebook ì†ì‹¤ $\| \text{sg}[z_e(x)] - e \|_2^2$

* ì„ íƒëœ codebook entry $e_k$ê°€ encoderì˜ ì¶œë ¥ $z_e(x)$ì— ê°€ê¹Œì›Œì§€ë„ë¡ codebookì„ í•™ìŠµì‹œí‚´.
* ì—¬ê¸°ì„œ $\text{sg}[\cdot]$ëŠ” **stop-gradient** ì—°ì‚°ìë¡œ, í•´ë‹¹ ë³€ìˆ˜ì— ëŒ€í•´ ì—­ì „íŒŒê°€ ì´ë£¨ì–´ì§€ì§€ ì•Šë„ë¡ ì„¤ì •í•œë‹¤.
â†’ codebook entryê°€ encoder ì¶œë ¥ì„ ë”°ë¼ê°€ë„ë¡ ìœ ë„ (codebook updateìš©)

#### (3) commitment ì†ì‹¤ $\| z_e(x) - \text{sg}[e] \|_2^2$

* encoderê°€ íŠ¹ì • codebook entryì— "ì±…ì„ì„ ì§€ë„ë¡(commit)" ê°•ì œí•¨ìœ¼ë¡œì¨, ì–‘ìí™”ì˜ ì•ˆì •ì„±ê³¼ í‘œí˜„ ì¼ê´€ì„±ì„ ìœ ì§€í•œë‹¤.
* í•˜ì´í¼íŒŒë¼ë¯¸í„° $\beta$ëŠ” encoderê°€ ì–¼ë§ˆë‚˜ ê°•í•˜ê²Œ commitmentì— ì±…ì„ì„ ì§€ë„ë¡ í• ì§€ ì¡°ì ˆí•˜ëŠ” ê³„ìˆ˜ì´ë‹¤.

---

### ğŸ“Œ Straight-Through Estimator (STE)

* ì–‘ìí™”ëŠ” ë¹„ë¶„í™”(discontinuous) ì—°ì‚°ì´ë¯€ë¡œ backpropagationì´ ë¶ˆê°€ëŠ¥í•˜ë‹¤.
* ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ **straight-through estimator**ë¥¼ ì‚¬ìš©í•œë‹¤:
  
$$
\frac{\partial \mathcal{L}}{\partial z_e(x)} \approx \frac{\partial \mathcal{L}}{\partial z_q(x)}
$$

* ì¦‰, forward passì—ì„œëŠ” ì–‘ìí™”ë¥¼ ìˆ˜í–‰í•˜ë˜, backwardì—ì„œëŠ” ì–‘ìí™”ëœ ë²¡í„° $z_q(x)$ê°€ ë§ˆì¹˜ $z_e(x)$ì¸ ê²ƒì²˜ëŸ¼ gradientë¥¼ ì „ë‹¬í•œë‹¤.

---

### ğŸ“Œ Prior Network (ì„ íƒì  êµ¬ì„±)

* discrete latent token $z_q(x)$ ì‹œí€€ìŠ¤ì— ëŒ€í•œ **ì˜¤í† ë¦¬ê·¸ë ˆì‹œë¸Œ prior ëª¨ë¸**ì„ í•™ìŠµí•  ìˆ˜ ìˆë‹¤.
* ì˜ˆ: PixelCNN, WaveNet ë“±ì„ ì‚¬ìš©í•˜ì—¬ ë‹¤ìŒê³¼ ê°™ì€ ë¶„í¬ë¥¼ í•™ìŠµ:

$p(z_1, z_2, \dots, z_n) = \prod_{i=1}^n p(z_i \mid z_{<i})$

* ì´ë¥¼ í†µí•´ í•™ìŠµëœ latent tokenì„ ê¸°ë°˜ìœ¼ë¡œ **ìƒ˜í”Œ ìƒì„± ë˜ëŠ” ì˜ˆì¸¡**ì´ ê°€ëŠ¥í•´ì§„ë‹¤.

---

### ğŸ” ìš”ì•½ ì •ë¦¬

| êµ¬ì„± ìš”ì†Œ        | ì„¤ëª… |
|------------------|------|
| **Encoder**      | ì…ë ¥ $x$ë¥¼ continuous latent vector $z_e(x)$ë¡œ ì¸ì½”ë”© |
| **Quantizer**    | $z_e(x)$ë¥¼ ê°€ì¥ ê°€ê¹Œìš´ codebook entry $e_k$ë¡œ ë§¤í•‘í•˜ì—¬ $z_q(x)$ ìƒì„± |
| **Decoder**      | $z_q(x)$ë¡œë¶€í„° ì…ë ¥ ë³µì› $\hat{x}$ |
| **Loss**         | ì¬êµ¬ì„± ì†ì‹¤ + codebook í•™ìŠµ + commitment ê°•ì œ |
| **STE**          | ì–‘ìí™” êµ¬ê°„ì—ì„œë„ gradient íë¦„ ìœ ì§€ |
| **Prior (ì„ íƒ)** | discrete latentì— ëŒ€í•´ ì‹œí€€ìŠ¤ ëª¨ë¸ë§ ê°€ëŠ¥ |



## âš–ï¸ ê¸°ì¡´ ëª¨ë¸ê³¼ì˜ ë¹„êµ

| í•­ëª©    | ë³¸ ë…¼ë¬¸ (VQ-VAE) | ê¸°ì¡´ ë°©ë²•1 (VAE) | ê¸°ì¡´ ë°©ë²•2 (Autoencoder + PixelCNN) |
|--------|------------------|------------------|----------------------------|
| êµ¬ì¡°    | Encoder â†’ Vector Quantizer â†’ Decoder | Encoder â†’ Î¼, Ïƒ â†’ Sampling â†’ Decoder | Encoder â†’ Continuous latent â†’ PixelCNN |
| í•™ìŠµ ë°©ì‹ | Non-differentiable quantization with STE | Variational inference (reparameterization trick) | Deterministic encoding + pixel-wise autoregressive decoder |
| ëª©ì     | Discrete latent spaceë¡œ íš¨ìœ¨ì ì¸ ìƒì„± ë° ì‹œí€€ìŠ¤ ëª¨ë¸ë§ | Continuous latent space ê¸°ë°˜ ë°€ë„ ì¶”ì • | ê³ í•´ìƒë„ ì´ë¯¸ì§€ ëª¨ë¸ë§ (priorë§Œ autoregressive)

---

## ğŸ“‰ ì‹¤í—˜ ë° ê²°ê³¼

* **ë°ì´í„°ì…‹**:
  - CIFAR-10 (ì´ë¯¸ì§€)
  - VCTK (ìŒì„±)
  - DeepMind ë¹„ë””ì˜¤ ë°ì´í„°ì…‹

* **ë¹„êµ ëª¨ë¸**:
  - Variational Autoencoder (VAE)
  - PixelCNN
  - WaveNet
  - JPEG (ì••ì¶•ë¥  ë¹„êµ ì‹œ baseline)

* **ì£¼ìš” ì„±ëŠ¥ ì§€í‘œ ë° ê²°ê³¼**:

| ëª¨ë¸            | Accuracy | F1 | BLEU | ê¸°íƒ€ (Perplexity, PSNR ë“±) |
|-----------------|----------|----|------|-----------------------------|
| ë³¸ ë…¼ë¬¸ (VQ-VAE) | N/A      | N/A| N/A  | ìŒì„± ì¬ìƒ í’ˆì§ˆ ìš°ìˆ˜ (MOS ê¸°ì¤€), ì´ë¯¸ì§€ ì••ì¶•ë¥  ìš°ìˆ˜ |
| ê¸°ì¡´ VAE        | N/A      | N/A| N/A  | reconstruction blur ì‹¬í•¨, ìƒ˜í”Œ ë‹¤ì–‘ì„± ë–¨ì–´ì§ |
| JPEG            | N/A      | N/A| N/A  | ì••ì¶•ë¥  ëŒ€ë¹„ ì‹œê°ì  í’ˆì§ˆ ë‚®ìŒ |

> ì‹¤í—˜ ê²°ê³¼ ìš”ì•½ ë° í•´ì„:

- VQ-VAEëŠ” VAEì— ë¹„í•´ **ë³µì› í’ˆì§ˆì´ í›¨ì”¬ ìš°ìˆ˜í•˜ë©°**, latent ê³µê°„ì—ì„œ discrete tokenì„ ìƒì„±í•˜ê¸° ë•Œë¬¸ì— **PixelCNN, WaveNet ë“±ê³¼ì˜ ê²°í•©ì´ ìš©ì´í•¨**.
- ìŒì„± ì‹¤í—˜ì—ì„œëŠ” WaveNet decoderì™€ì˜ ê²°í•©ì„ í†µí•´ ê³ í’ˆì§ˆ ìŒì„± ë³µì›ì´ ê°€ëŠ¥í–ˆê³ , ì´ë¯¸ì§€ ì‹¤í—˜ì—ì„œëŠ” JPEGë³´ë‹¤ **ë†’ì€ ì••ì¶•ë¥ ì—ì„œ ë” ë‚˜ì€ ì‹œê°ì  í’ˆì§ˆ**ì„ ë³´ì„.
- íŠ¹íˆ latent ê³µê°„ì˜ ì´ì‚°ì  êµ¬ì¡° ë•ë¶„ì— **ì˜¤í† ë¦¬ê·¸ë ˆì‹œë¸Œ prior í•™ìŠµì´ ìì—°ìŠ¤ëŸ½ê²Œ ê°€ëŠ¥**í•¨.

---

## âœ… ì¥ì  ë° í•œê³„

### **ì¥ì **:

* Continuous â†’ Discrete ë³€í™˜ì„ í†µí•´ **ì‹œí€€ìŠ¤ ëª¨ë¸ë§(autogressive prior)** ì´ ê°€ëŠ¥
* VAE ëŒ€ë¹„ **ë³µì› í’ˆì§ˆì´ ë†’ê³  ë¸”ëŸ¬ í˜„ìƒì´ ì ìŒ**
* Codebookì„ í†µí•´ **ê³ ì •ëœ í‘œí˜„ ê³µê°„(discrete latent space)**ì—ì„œ **íš¨ìœ¨ì ì¸ ì¶”ë¡  ë° ìƒì„±** ê°€ëŠ¥
* ë‹¤ì–‘í•œ ë„ë©”ì¸(ì´ë¯¸ì§€, ìŒì„±, ë¹„ë””ì˜¤)ì— ëª¨ë‘ ì ìš© ê°€ëŠ¥

### **í•œê³„ ë° ê°œì„  ê°€ëŠ¥ì„±**:

* Codebook collapse: ì¼ë¶€ codeë§Œ ê³„ì† ì‚¬ìš©ë˜ì–´ ë‹¤ì–‘ì„±ì´ ì‚¬ë¼ì§ˆ ìˆ˜ ìˆìŒ
* Quantizationì€ ë³¸ì§ˆì ìœ¼ë¡œ ì •ë³´ ì†ì‹¤ì„ ë™ë°˜í•¨
* Straight-Through EstimatorëŠ” gradient ê·¼ì‚¬ì´ë¯€ë¡œ í•™ìŠµ ë¶ˆì•ˆì • ê°€ëŠ¥
* Codebook í¬ê¸°ë‚˜ $\beta$ ê°’ì— ë”°ë¼ ì„±ëŠ¥ì´ ë¯¼ê°í•˜ê²Œ ë³€í•¨ â†’ íŠœë‹ í•„ìš”

---

## ğŸ§  TL;DR â€“ í•œëˆˆì— ìš”ì•½

> ì—°ì†ì ì¸ latent ê³µê°„ì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´, encoder ì¶œë ¥ì„ ê³ ì •ëœ codebook entryë¡œ ì–‘ìí™”í•˜ì—¬ **ì´ì‚°ì ì¸ latent í† í°(discrete latent token)**ì„ í•™ìŠµí•˜ëŠ” **VQ-VAE** êµ¬ì¡°ë¥¼ ì œì•ˆí•¨.  
> ì´ë¥¼ í†µí•´ high-fidelity data generation + autoregressive prior í•™ìŠµì´ ê°€ëŠ¥í•´ì¡Œê³ , ì´ë¯¸ì§€/ìŒì„±/ë¹„ë””ì˜¤ ì „ë°˜ì— ê°•ë ¥í•œ ìƒì„± ì„±ëŠ¥ì„ ë³´ì„.

| êµ¬ì„± ìš”ì†Œ     | ì„¤ëª… |
|--------------|------|
| í•µì‹¬ ëª¨ë“ˆ     | Vector Quantizer, Discrete Latent, Codebook |
| í•™ìŠµ ì „ëµ     | Reconstruction + Codebook update + Commitment loss + STE |
| ì „ì´ ë°©ì‹     | Encoder-decoder êµ¬ì¡°ì— prior (PixelCNN ë“±) ì¶”ê°€ ê°€ëŠ¥ |
| ì„±ëŠ¥/íš¨ìœ¨ì„±  | ê³ í•´ìƒë„ ë³µì› + ë‚®ì€ ë¹„íŠ¸ ìˆ˜ì˜ latent í‘œí˜„ + ë¹ ë¥¸ inference ê°€ëŠ¥ |

---

## ğŸ”— ì°¸ê³  ë§í¬ (References)

* [ğŸ“„ arXiv ë…¼ë¬¸](https://arxiv.org/abs/1711.00937)
* [ğŸ’» GitHub (DeepMind Sonnet)](https://github.com/deepmind/sonnet/blob/v2/examples/vqvae_example.ipynb)
* [ğŸ“ˆ Papers with Code](https://paperswithcode.com/paper/neural-discrete-representation-learning)

---

## ë‹¤ìŒ ë…¼ë¬¸:

> ğŸ”œ **VQ-VAE-2: Generating Diverse High-Fidelity Images with VQ-VAE-2 (2019)**  
> Hierarchical êµ¬ì¡° + powerful prior (PixelSNAIL)ë¡œ ê³ í•´ìƒë„ ì´ë¯¸ì§€ ìƒì„±ê¹Œì§€ í™•ì¥í•œ follow-up ë…¼ë¬¸
