# ğŸ“˜ Swin Transformer: Hierarchical Vision Transformer using Shifted Windows

## 1. ê°œìš” (Overview)

- **ì œëª©**: Swin Transformer: Hierarchical Vision Transformer using Shifted Windows  
- **ì €ì**: Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo  
- **ì†Œì†**: Microsoft Research Asia  
- **í•™íšŒ**: ICCV 2021 (International Conference on Computer Vision)  
- **ë§í¬**: [arXiv](https://arxiv.org/abs/2103.14030) / [GitHub](https://github.com/microsoft/Swin-Transformer) / [Papers with Code](https://paperswithcode.com/paper/swin-transformer-hierarchical-vision)


## 2. ë…¼ë¬¸ ì„ ì • ì´ìœ  ë° ë„ì…ë¶€

ìì—°ì–´ ì²˜ë¦¬(NLP) ë¶„ì•¼ì—ì„œ TransformerëŠ” ë›°ì–´ë‚œ ì„±ëŠ¥ìœ¼ë¡œ ì£¼ëª©ë°›ì•„ì™”ìœ¼ë©°, BERT, GPT ë“±ì˜ ëª¨ë¸ì´ ê·¸ ê°€ëŠ¥ì„±ì„ ì…ì¦í•´ì™”ë‹¤. ì´ëŸ¬í•œ Transformer êµ¬ì¡°ë¥¼ ì´ë¯¸ì§€ ì²˜ë¦¬ì— ì ìš©í•œ ViT(Vision Transformer)ëŠ” CNN ì—†ì´ë„ ì´ë¯¸ì§€ ë¶„ë¥˜ì—ì„œ ê²½ìŸë ¥ ìˆëŠ” ì„±ëŠ¥ì„ ë³´ì„.

ê·¸ëŸ¬ë‚˜ ViTëŠ” ë‹¤ìŒê³¼ ê°™ì€ í•œê³„ë¥¼ ê°–ê³  ìˆì—ˆë‹¤:

- **Hierarchical feature extractionì´ ë¶ˆê°€ëŠ¥**í•˜ì—¬ ë‹¤ì–‘í•œ ë¹„ì „ ê³¼ì œì— í™œìš©í•˜ê¸° ì–´ë µê³ ,
- **Global self-attention êµ¬ì¡°**ë¡œ ì¸í•´ **ê³ í•´ìƒë„ ì…ë ¥ì— ë¹„íš¨ìœ¨ì **ì´ë©°,
- **Local information learningì´ ë¶€ì¡±**í•˜ì—¬ ì„¸ë°€í•œ ê°ì²´ ì¸ì‹ì— ì•½í•œ ê²½í–¥ì´ ìˆìŒ.

ì´ì— ë”°ë¼, í•´ë‹¹ ë…¼ë¬¸ì¸ **Swin Transformer**ëŠ” ìœ„ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì œì•ˆëœ ëª¨ë¸ë¡œ, ë‹¤ìŒê³¼ ê°™ì€ ì´ìœ ì—ì„œ í¥ë¯¸ë¥¼ ëŒì—ˆë‹¤:

- Transformer êµ¬ì¡°ë¥¼ **local window-based attention**ìœ¼ë¡œ ë³€í˜•í•˜ì—¬ CNNì²˜ëŸ¼ ê³„ì¸µì ì´ê³  ì§€ì—­ì ì¸ ì •ë³´ë¥¼ í•™ìŠµ ê°€ëŠ¥í•˜ê²Œ í•¨
- **Shifted window mechanism**ì„ í†µí•´ ì •ë³´ íë¦„ì„ íš¨ìœ¨ì ìœ¼ë¡œ í™•ì¥
- ViTë³´ë‹¤ ë²”ìš©ì„±ì´ ë›°ì–´ë‚˜ **ê°ì²´ ê²€ì¶œ, ë¶„í• , ì¶”ë¡  ë“± ë‹¤ì–‘í•œ ë¹„ì „ ì‘ì—…ì— ìœ ì—°í•˜ê²Œ ì ìš© ê°€ëŠ¥**

ì´ì²˜ëŸ¼ Swin TransformerëŠ” ViTì˜ êµ¬ì¡°ì  í•œê³„ë¥¼ í•´ê²°í•˜ë©°, ë¹„ì „ ë¶„ì•¼ì—ì„œ Transformer ê³„ì—´ ëª¨ë¸ì˜ ì‹¤ìš©ì„±ì„ í•œ ë‹¨ê³„ ëŒì–´ì˜¬ë ¸ë‹¤ëŠ” ì ì—ì„œ ê¼­ ì½ì–´ì•¼í•  ë…¼ë¬¸ì´ë¼ íŒë‹¨í–ˆë‹¤.

---

## 2. ë¬¸ì œ ì •ì˜ (Problem Formulation)

### ğŸ“Œ ë¬¸ì œ ë° ê¸°ì¡´ í•œê³„

- ê¸°ì¡´ ì»´í“¨í„° ë¹„ì „ ëª¨ë¸ì€ ëŒ€ë¶€ë¶„ **CNN ê¸°ë°˜ êµ¬ì¡°**ë¡œ ë°œì „í•´ ì™”ìœ¼ë©°, AlexNet ì´í›„ ë‹¤ì–‘í•œ ë³€í˜•ì´ ë“±ì¥í•˜ë©° ì„±ëŠ¥ì„ í–¥ìƒì‹œì¼œ ì™”ìŒ.
- ìµœê·¼ NLPì—ì„œëŠ” **Transformer**ê°€ ì£¼ë¥˜ ì•„í‚¤í…ì²˜ë¡œ ìë¦¬ ì¡ì•˜ìœ¼ë©°, ì´ë¥¼ Vision ë¶„ì•¼ë¡œ í™•ì¥í•˜ë ¤ëŠ” ì‹œë„ë“¤ì´ ì´ë£¨ì–´ì§ (ì˜ˆ: ViT).
- ê·¸ëŸ¬ë‚˜ Visionì— Transformerë¥¼ ì§ì ‘ ì ìš©í•˜ëŠ” ë°ì—ëŠ” ë‹¤ìŒê³¼ ê°™ì€ **êµ¬ì¡°ì  í•œê³„**ê°€ ì¡´ì¬:

  1. **Token í¬ê¸° ê³ ì • ë¬¸ì œ**  
     - NLPì—ì„œëŠ” word tokenì´ ê³ ì •ëœ ë‹¨ìœ„ì¸ ë°˜ë©´, Visionì—ì„œëŠ” ê°ì²´ì˜ **í¬ê¸°ê°€ ë‹¤ì–‘**í•¨.  
     - ViTëŠ” ê³ ì •ëœ í¬ê¸°ì˜ patchë§Œ ì‚¬ìš©í•˜ë¯€ë¡œ, **ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ê°ì²´ í‘œí˜„ì— ë¶€ì í•©**.

  2. **ë†’ì€ í•´ìƒë„ ì²˜ë¦¬ì˜ ë¹„íš¨ìœ¨ì„±**  
     - Visionì—ì„œëŠ” ê³ í•´ìƒë„ ì´ë¯¸ì§€ê°€ ì¼ë°˜ì ì´ë©°, semantic segmentationì²˜ëŸ¼ **dense prediction**ì´ í•„ìš”í•œ ê³¼ì œë„ ë§ìŒ.  
     - ê¸°ì¡´ Vision TransformerëŠ” **global self-attention**ì„ ì‚¬ìš©í•˜ë¯€ë¡œ **ê³„ì‚° ë³µì¡ë„ $O(N^2)$**ë¡œ ë¹„íš¨ìœ¨ì ì„.

  3. **ê³„ì¸µì  í‘œí˜„ ë¶€ì¡±**  
     - ê¸°ì¡´ Vision TransformerëŠ” **ë‹¨ì¼ í•´ìƒë„ì˜ feature map**ë§Œ ìƒì„±í•¨.  
     - CNNì²˜ëŸ¼ **ê³„ì¸µì ìœ¼ë¡œ ì •ë³´ ì¶”ì¶œí•˜ëŠ” êµ¬ì¡°ê°€ ë¶€ì¬**í•˜ì—¬, FPNì´ë‚˜ U-Net ë“±ì˜ ê¸°ì¡´ vision ê¸°ë²•ê³¼ì˜ ì—°ê³„ê°€ ì–´ë ¤ì›€.

---

### ğŸ’¡ ì œì•ˆ ë°©ì‹ (Swin Transformer)

Swin TransformerëŠ” ë‹¤ìŒê³¼ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ê¸°ì¡´ í•œê³„ë¥¼ ê·¹ë³µí•¨:

1. **Hierarchical Feature Map êµ¬ì„±**  
   - ì…ë ¥ ì´ë¯¸ì§€ë¥¼ ì‘ì€ patchë¡œ ë¶„í•  í›„, ê° ê³„ì¸µì—ì„œ **ì¸ì ‘ patchë¥¼ ë³‘í•©**í•˜ì—¬ ì ì§„ì ìœ¼ë¡œ í•´ìƒë„ë¥¼ ë‚®ì¶¤.
   - CNNì²˜ëŸ¼ ê³„ì¸µì  í‘œí˜„ì´ ê°€ëŠ¥í•˜ì—¬, **ë‹¤ì–‘í•œ ë¹„ì „ ì‘ì—… (ë¶„ë¥˜, íƒì§€, ë¶„í• )**ì— íš¨ê³¼ì .

2. **Local Window ê¸°ë°˜ Self-Attention**  
   - Globalì´ ì•„ë‹Œ **local window ë‹¨ìœ„**ë¡œ self-attention ìˆ˜í–‰ â†’ **ê³„ì‚° ë³µì¡ë„ $O(N)$ë¡œ ê°ì†Œ**.
   - ê° windowëŠ” ê³ ì •ëœ í¬ê¸°ë¡œ ë‚˜ë‰˜ë©°, ë³‘ë ¬ ì²˜ë¦¬ì™€ latency ì¸¡ë©´ì—ì„œë„ íš¨ìœ¨ì .

3. **Shifted Window Mechanism**  
   - ì¸ì ‘ ê³„ì¸µ ê°„ì˜ window ê²½ê³„ë¥¼ **í•œ ì¹¸ì”© ë°€ì–´** ë°°ì¹˜í•¨ìœ¼ë¡œì¨, **cross-window ì—°ê²°**ì„ í˜•ì„±.  
   - ì •ë³´ë¥¼ ì „ì—­ì ìœ¼ë¡œ ì „ë‹¬í•˜ë©´ì„œë„ íš¨ìœ¨ì ì¸ êµ¬ì¡° ìœ ì§€.

4. **ë²”ìš© ë°±ë³¸ìœ¼ë¡œì„œì˜ í™œìš©ì„± í™•ë³´**  
   - FPN, U-Netê³¼ ì—°ê³„ ê°€ëŠ¥í•œ êµ¬ì¡°ë¡œ **dense prediction** ì‘ì—…ì— ì í•©.  
   - Classification, Detection, Segmentation ëª¨ë‘ì—ì„œ **SOTA ìˆ˜ì¤€ ì„±ëŠ¥**ì„ ë‹¬ì„±.

---

### ğŸ§  í•µì‹¬ ê°œë… ì •ì˜

- **Hierarchical Feature Map**
  
  ![swin_vit_comp](/papers/images/swin_vit_comp.png)
  
  ViTì™€ ë‹¬ë¦¬, ì—¬ëŸ¬ ë‹¨ê³„ì˜ resolutionì„ ê°€ì§€ëŠ” feature mapì„ ìƒì„±í•˜ì—¬ ë‹¤ì–‘í•œ ìŠ¤ì¼€ì¼ì˜ ì‹œê° ì •ë³´ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì²˜ë¦¬í•¨.
 

- **Window-based Multi-head Self-Attention (W-MSA)**
  
  ![batch_computation](/papers/images/batch_computation.png)
  
  ì´ë¯¸ì§€ ì „ì²´ê°€ ì•„ë‹Œ **ì‘ì€ ì°½(window)** ë‹¨ìœ„ì—ì„œë§Œ self-attention ìˆ˜í–‰ â†’ ì„ í˜• ê³„ì‚° ë³µì¡ë„ í™•ë³´.
  

- **Shifted Window**
  
  ![shift_window](/papers/images/shifted_window.png)
  
  window ê²½ê³„ë¥¼ ë‹¤ìŒ layerì—ì„œ í•œ ì¹¸ì”© ì´ë™ì‹œì¼œ **cross-window dependency**ë¥¼ í˜•ì„±.  
  ëª¨ë¸ì˜ í‘œí˜„ë ¥ì„ ì¦ê°€ì‹œí‚¤ë©´ì„œë„ ì—°ì‚° íš¨ìœ¨ì„± ìœ ì§€.

- **General-purpose Backbone**  
  ì´ë¯¸ì§€ ë¶„ë¥˜ë¿ ì•„ë‹ˆë¼ ê°ì²´ íƒì§€, ì‹œë§¨í‹± ë¶„í•  ë“± **ë‹¤ì–‘í•œ CV ê³¼ì œì—ì„œ í™œìš© ê°€ëŠ¥í•œ ë°±ë³¸ êµ¬ì¡°**ë¡œ ì„¤ê³„ë¨.

---

## 3. ëª¨ë¸ êµ¬ì¡° (Architecture)

### ğŸ—ï¸ ì „ì²´ êµ¬ì¡°

![swin_architecture](/papers/images/swin_architecture.png)

Swin TransformerëŠ” ì „ì²´ì ìœ¼ë¡œ CNNê³¼ ìœ ì‚¬í•œ **ê³„ì¸µì  ì•„í‚¤í…ì²˜(hierarchical architecture)**ë¥¼ ë”°ë¥´ë©°, ì…ë ¥ ì´ë¯¸ì§€ë¥¼ ì‘ì€ íŒ¨ì¹˜ë¡œ ë¶„í• í•˜ê³ , ì´í›„ ì—¬ëŸ¬ ë‹¨ê³„ì— ê±¸ì³ feature resolutionì„ ì ì§„ì ìœ¼ë¡œ ì¶•ì†Œì‹œí‚¤ë©´ì„œ ì±„ë„ ìˆ˜ë¥¼ ì¦ê°€ì‹œí‚¨ë‹¤.

- ì…ë ¥ ì´ë¯¸ì§€ëŠ” $H \times W \times 3$ í¬ê¸°ì˜ RGB ì´ë¯¸ì§€
- $4 \times 4$ patch ë¶„í•  â†’ ê° patchëŠ” flattenë˜ì–´ ì„ë² ë”©ë¨
- ì´ 4ë‹¨ê³„ (Stage 1 ~ 4)ë¡œ êµ¬ì„±ë˜ë©°, ê° ë‹¨ê³„ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì—°ì‚° íë¦„ì„ ë”°ë¥¸ë‹¤:

Patch Partition â†’ Linear Embedding â†’ Swin Transformer Blocks â†’ Patch Merging â†’ ...

ê° ë‹¨ê³„ì—ì„œëŠ” Swin Transformer Blockì„ ì—¬ëŸ¬ ê°œ ìŒ“ì•„ì„œ local-context ê¸°ë°˜ì˜ í‘œí˜„ì„ í•™ìŠµí•˜ë©°, ë‹¤ìŒ ë‹¨ê³„ë¡œ ë„˜ì–´ê°ˆ ë•Œ **Patch Merging**ì„ í†µí•´ spatial resolutionì„ ì ˆë°˜ìœ¼ë¡œ ì¤„ì´ê³  channel ìˆ˜ë¥¼ ì¦ê°€ì‹œí‚´.

---

### ğŸ’  í•µì‹¬ ëª¨ë“ˆ ë˜ëŠ” êµ¬ì„± ìš”ì†Œ

#### ğŸ“Œ Patch Partition & Linear Embedding

- ì…ë ¥ ì´ë¯¸ì§€ë¥¼ $4 \times 4$ ë¹„ì¤‘ì²©(non-overlapping) patchë¡œ ë¶„í• 
- ê° patchëŠ” flattenë˜ì–´ ê¸¸ì´ $4 \times 4 \times 3 = 48$ ë²¡í„°ê°€ ë˜ê³ , Linear Projectionì„ í†µí•´ $C$ì°¨ì›ìœ¼ë¡œ ë§¤í•‘ë¨

> ìˆ˜ì‹ í‘œí˜„:  
> ì´ë¯¸ì§€ $x \in \mathbb{R}^{H \times W \times 3}$ â†’ patch sequence $z_0 \in \mathbb{R}^{\frac{HW}{16} \times C}$

---

#### ğŸ“Œ Swin Transformer Block

ê° StageëŠ” Swin Transformer Blockìœ¼ë¡œ êµ¬ì„±ë˜ë©°, ì´ëŠ” ë‹¤ìŒì˜ ë‘ Attention ëª¨ë“ˆ ìŒìœ¼ë¡œ êµ¬ì„±ë¨:

1. **Window-based Multi-head Self-Attention (W-MSA)**
2. **Shifted Window-based Multi-head Self-Attention (SW-MSA)**

ê° ë¸”ë¡ì€ PreNorm êµ¬ì¡°ë¡œ LayerNorm, MLP, Residual Connectionìœ¼ë¡œ êµ¬ì„±ëœë‹¤.

> ìˆ˜ì‹ íë¦„ (í•œ block ë‚´ W-MSAì™€ SW-MSA ìŒ):

$$
\begin{aligned}
\hat{z}^{l} &= \text{W-MSA}(\text{LN}(z^{l-1})) + z^{l-1} \\
\tilde{z}^{l} &= \text{MLP}(\text{LN}(\hat{z}^{l})) + \hat{z}^{l} \\
\hat{z}^{l+1} &= \text{SW-MSA}(\text{LN}(\tilde{z}^{l})) + \tilde{z}^{l} \\
z^{l+1} &= \text{MLP}(\text{LN}(\hat{z}^{l+1})) + \hat{z}^{l+1}
\end{aligned}
$$

ë‹¤ìŒ blockì—ì„œëŠ” W-MSA ëŒ€ì‹  SW-MSAê°€ ì‚¬ìš©ë˜ë©°, ë™ì¼í•œ êµ¬ì¡°ì—ì„œ ìœˆë„ìš° ìœ„ì¹˜ë§Œ ì´ë™ì‹œí‚¨ë‹¤.

---

#### ğŸ“Œ Window-based Multi-head Self-Attention (W-MSA)

- ì…ë ¥ feature mapì„ $M \times M$ í¬ê¸°ì˜ local windowë¡œ ë¶„í• í•˜ì—¬ **ê° window ë‚´ì—ì„œë§Œ Self-Attention ìˆ˜í–‰**
- ì—°ì‚° ë³µì¡ë„ëŠ” $O(HW)$ë¡œ ê°ì†Œí•˜ë©°, ì „ì²´ ì´ë¯¸ì§€ì— ëŒ€í•œ global attentionì„ í”¼í•¨

> ê¸°ë³¸ Attention ìˆ˜ì‹:

$$
\text{Attention}(Q, K, V) = \text{Softmax}\left(\frac{QK^T}{\sqrt{d}}\right)V
$$

- $Q, K, V$ëŠ” ê° window ë‹¨ìœ„ì—ì„œ ê³„ì‚°ë¨

---

#### ğŸ“Œ Shifted Window Multi-head Self-Attention (SW-MSA)

- ë‹¤ìŒ ë¸”ë¡ì—ì„œëŠ” ê¸°ì¡´ windowë¥¼ **$\lfloor \frac{M}{2} \rfloor$ ë§Œí¼ shift**í•˜ì—¬ ìƒˆë¡œìš´ ìœˆë„ìš° ìƒì„±
- ì´ë¥¼ í†µí•´ ì´ì „ window ê°„ **cross-window dependency**ë¥¼ í•™ìŠµ ê°€ëŠ¥
- Shiftë¡œ ì¸í•œ ì—°ì‚° ë¬¸ì œëŠ” **maskingê³¼ cyclic shift** ê¸°ë²•ìœ¼ë¡œ í•´ê²°

> íš¨ê³¼:
> - Window ê°„ ì •ë³´ ì—°ê²° (global receptive field ë³´ì™„)
> - íš¨ìœ¨ì„± ìœ ì§€í•˜ë©´ì„œ í‘œí˜„ë ¥ í–¥ìƒ

> ìˆ˜ì‹ì  ì°¨ì´ëŠ” ì—†ìœ¼ë©°, ë‹¨ì§€ window partition ìœ„ì¹˜ë§Œ ë‹¬ë¼ì§

---

#### ğŸ“Œ Patch Merging Layer

- ë‹¤ìŒ ê³„ì¸µìœ¼ë¡œ ë„˜ì–´ê°ˆ ë•Œ, $2 \times 2$ ì¸ì ‘í•œ patchë“¤ì„ ë³‘í•©í•˜ì—¬ resolutionì„ ì ˆë°˜ìœ¼ë¡œ ì¤„ì´ê³ , ì±„ë„ ìˆ˜ëŠ” 2~4ë°°ë¡œ ì¦ê°€
- CNNì˜ Downsamplingê³¼ ìœ ì‚¬í•œ ì—­í•  ìˆ˜í–‰
- ì´ ê³¼ì •ì„ í†µí•´ **ê³„ì¸µì ìœ¼ë¡œ í‘œí˜„ ì¶”ì¶œ ê°€ëŠ¥**

> ìˆ˜ì‹ í‘œí˜„:  
> $z^{(i)} \in \mathbb{R}^{\frac{H}{2^i} \times \frac{W}{2^i} \times C_i}$  
> â†’ patch merging â†’  
> $z^{(i+1)} \in \mathbb{R}^{\frac{H}{2^{i+1}} \times \frac{W}{2^{i+1}} \times C_{i+1}}$  
> where $C_{i+1} = 2 \times C_i$ or $4 \times C_i$

---

#### ğŸ“Œ MLP & Normalization

- ê° Swin Block ë‚´ì—ëŠ” Feed-Forward Network (MLP)ì™€ LayerNormì´ í¬í•¨ë¨
- Residual êµ¬ì¡°ì™€ í•¨ê»˜ ì•ˆì •ì ì¸ í•™ìŠµ ë³´ì¥

> MLP êµ¬ì„±:
> LayerNorm â†’ Linear (C â†’ 4C) â†’ GELU â†’ Linear (4C â†’ C)

> ìˆ˜ì‹:  
\( \text{MLP}(x) = W_2 \cdot \text{GELU}(W_1 x) \)

---

### ğŸ”„ Stage êµ¬ì¡° ìš”ì•½

| Stage | Resolution        | Patch ìˆ˜        | Channel ìˆ˜ | Block ìˆ˜ |
|-------|-------------------|------------------|-------------|-----------|
| 1     | H/4 Ã— W/4         | HW/16            | 96          | 2         |
| 2     | H/8 Ã— W/8         | HW/64            | 192         | 2         |
| 3     | H/16 Ã— W/16       | HW/256           | 384         | 6         |
| 4     | H/32 Ã— W/32       | HW/1024          | 768         | 2         |


---

### ğŸ”„ ì•„í‚¤í…ì²˜ì˜ ë°˜ë³µ êµ¬ì¡°

- Swin-T, Swin-S, Swin-BëŠ” block ìˆ˜ì™€ hidden dimë§Œ ë‹¤ë¦„
- ì˜ˆ:  
  - **Swin-Tiny**: $(2, 2, 6, 2)$ blocks per stage  
  - **Swin-Small**: $(2, 2, 18, 2)$  
  - **Swin-Base**: $(2, 2, 18, 2)$ with larger embedding

---
## âš–ï¸ ê¸°ì¡´ ëª¨ë¸ê³¼ì˜ ë¹„êµ

| í•­ëª©        | Swin Transformer (ë³¸ ë…¼ë¬¸) | ViT (Vision Transformer) | CNN ê¸°ë°˜ ëª¨ë¸ (ResNet ë“±) |
| ----------- | -------------------------- | ------------------------- | -------------------------- |
| êµ¬ì¡°        | Hierarchical Transformer êµ¬ì¡°, Local-Global ìœˆë„ìš° ê¸°ë°˜ Attention | ë‹¨ì¼ í•´ìƒë„, Global Self-Attention | Convolution ê³„ì¸µ ê¸°ë°˜ í”¼ì²˜ ì¶”ì¶œ |
| í•™ìŠµ ë°©ì‹   | Local window-based attention â†’ Shifted window â†’ ê³„ì¸µì  í•™ìŠµ | ì „ì²´ patch ê°„ global attention | ì»¤ë„ ê¸°ë°˜ ì§€ì—­ receptive field í™•ëŒ€ |
| ëª©ì         | ë²”ìš© ë¹„ì „ ë°±ë³¸ (Classification, Detection, Segmentation ë“±) | ì´ë¯¸ì§€ ë¶„ë¥˜ ì¤‘ì‹¬ | ì´ë¯¸ì§€ ë¶„ë¥˜, íƒì§€ (task-specific tuning) |

---

## ğŸ“‰ ì‹¤í—˜ ë° ê²°ê³¼

- **ë°ì´í„°ì…‹**:
  - ImageNet-1K (Classification)
  - COCO (Object Detection, Instance Segmentation)
  - ADE20K (Semantic Segmentation)

- **ë¹„êµ ëª¨ë¸**:
  - ViT / DeiT (Transformer ê³„ì—´)
  - ResNet, ResNeXt (CNN ê³„ì—´)
  - SETR, DetectoRS ë“± (SOTA ëª¨ë¸ë“¤)

- **ì£¼ìš” ì„±ëŠ¥ ì§€í‘œ ë° ê²°ê³¼ ìš”ì•½**:

| ëª¨ë¸               | Top-1 Accuracy (ImageNet) | Box AP (COCO) | Mask AP (COCO) | mIoU (ADE20K) |
| ------------------ | -------------------------- | ------------- | -------------- | ------------- |
| **Swin-T (ours)**   | 81.3%                      | 50.5          | 43.7           | 44.5          |
| ViT-B/16           | 77.9%                      | -             | -              | -             |
| ResNet-50          | 76.2%                      | 38.0          | 33.2           | 36.7          |
| DetectoRS          | -                          | 55.7          | 48.4           | -             |
| SETR-PUP           | -                          | -             | -              | 50.3          |
| **Swin-L (ours)**   | **87.3%**                  | **58.7**      | **51.1**       | **53.5**      |

> ğŸ” **ì‹¤í—˜ ê²°ê³¼ ìš”ì•½**:
> - Swin TransformerëŠ” ë‹¤ì–‘í•œ ë¹„ì „ ì‘ì—…ì— ëŒ€í•´ ê¸°ì¡´ SOTA ëŒ€ë¹„ **1~3% ì´ìƒì˜ ì„±ëŠ¥ í–¥ìƒ**ì„ ë³´ì„
> - íŠ¹íˆ COCOì™€ ADE20Kì—ì„œ **ê°ì²´ ê²€ì¶œ ë° ë¶„í•  ì„±ëŠ¥ì´ í¬ê²Œ í–¥ìƒ**
> - íš¨ìœ¨ì„±ê³¼ ì •í™•ë„ ëª¨ë‘ë¥¼ ë§Œì¡±ì‹œí‚¤ëŠ” ë²”ìš© ë°±ë³¸ìœ¼ë¡œì„œì˜ ê°€ëŠ¥ì„±ì„ ì…ì¦

---

## âœ… ì¥ì  ë° í•œê³„

### **ì¥ì **:

- âœ… **ì„ í˜•ì  ì—°ì‚° ë³µì¡ë„**: Local Window ê¸°ë°˜ Self-Attention ë•ë¶„ì— ê³ í•´ìƒë„ ì´ë¯¸ì§€ì—ì„œë„ íš¨ìœ¨ì 
- âœ… **Hierarchical êµ¬ì¡°**: CNNì²˜ëŸ¼ ë‹¤ë‹¨ê³„ feature ì¶”ì¶œì´ ê°€ëŠ¥í•˜ì—¬ ë‹¤ì–‘í•œ downstream taskì— ì í•©
- âœ… **ë²”ìš©ì„±**: Classification, Detection, Segmentation ë“± ë‹¤ì–‘í•œ Taskì— ëª¨ë‘ í™œìš© ê°€ëŠ¥
- âœ… **SOTA ì„±ëŠ¥**: ê¸°ì¡´ ViT, ResNet ê³„ì—´ ëŒ€ë¹„ ë›°ì–´ë‚œ ì„±ëŠ¥ í™•ë³´

### **í•œê³„ ë° ê°œì„  ê°€ëŠ¥ì„±**:

- âš ï¸ **Shifted Window êµ¬í˜„ ë³µì¡ì„±**: HW ë³‘ë ¬í™”ë¥¼ ê³ ë ¤í•œ ìµœì  êµ¬í˜„ì´ í•„ìš”
- âš ï¸ **Local ì •ë³´ ì¤‘ì‹¬ì˜ í•œê³„**: ì™„ì „í•œ ì „ì—­ ê´€ê³„ íŒŒì•…ì—ëŠ” ì œí•œì ì¼ ìˆ˜ ìˆìŒ
- âš ï¸ **ëŒ€ê·œëª¨ ì‚¬ì „ í•™ìŠµ í•„ìš”**: ì„±ëŠ¥ì„ ìµœëŒ€ë¡œ ëŒì–´ì˜¬ë¦¬ê¸° ìœ„í•´ì„  ë§ì€ ë°ì´í„°ì™€ ì—°ì‚° ìì›ì´ ìš”êµ¬ë¨
- âš ï¸ **Self-attention ì°½ í¬ê¸° ê³ ì •**: ë‹¤ì–‘í•œ scaleì— ì™„ì „íˆ ìœ ì—°í•˜ì§„ ì•ŠìŒ

---


## ğŸ§  TL;DR â€“ í•œëˆˆì— ìš”ì•½

> **Swin TransformerëŠ” ê³„ì¸µì  êµ¬ì¡°ì™€ Shifted Window ê¸°ë°˜ Self-Attentionì„ ë„ì…í•˜ì—¬, ê³ í•´ìƒë„ ì´ë¯¸ì§€ ì²˜ë¦¬ì— íš¨ìœ¨ì ì´ê³  ë‹¤ì–‘í•œ ì»´í“¨í„° ë¹„ì „ ê³¼ì œì— ë²”ìš©ì ìœ¼ë¡œ í™œìš© ê°€ëŠ¥í•œ Transformer ê¸°ë°˜ ë°±ë³¸ì„ ì œì•ˆí•œ ë…¼ë¬¸ì´ë‹¤.**

| êµ¬ì„± ìš”ì†Œ     | ì„¤ëª… |
| ------------ | ----------------------------------------------------------------------------- |
| í•µì‹¬ ëª¨ë“ˆ     | **Window-based Multi-head Self-Attention (W-MSA)** + **Shifted Window (SW-MSA)**ë¥¼ ì¡°í•©í•œ ë¸”ë¡ êµ¬ì¡° |
| í•™ìŠµ ì „ëµ     | **Hierarchical learning** â€“ Patch â†’ Block ë°˜ë³µ + Patch Mergingì„ í†µí•´ ë‹¤ì¤‘ í•´ìƒë„ feature í•™ìŠµ |
| ì „ì´ ë°©ì‹     | ë‹¤ì–‘í•œ Downstream Task (Classification, Detection, Segmentation)ì— FPN, U-Net ë“±ì˜ êµ¬ì¡°ì™€ ì—°ê³„ |
| ì„±ëŠ¥/íš¨ìœ¨ì„±  | ê¸°ì¡´ ViT/ResNet ëŒ€ë¹„ **Top-1 ì •í™•ë„ ë° COCO/AP/mIoU ëª¨ë‘ì—ì„œ SOTA ë‹¬ì„±**, ê³„ì‚°ëŸ‰ì€ ì„ í˜• ìˆ˜ì¤€ ìœ ì§€ |

---

## ğŸ”— ì°¸ê³  ë§í¬ (References)

* [ğŸ“„ arXiv ë…¼ë¬¸](https://arxiv.org/abs/2103.14030)
* [ğŸ’» GitHub](https://github.com/microsoft/Swin-Transformer)
* [ğŸ“ˆ Papers with Code](https://paperswithcode.com/paper/swin-transformer-hierarchical-vision)

---

## ë‹¤ìŒ ë…¼ë¬¸:
ğŸ‘‰Masked AutoEncoder (MAE) â€“ ViT ê¸°ë°˜ ìê¸°ì§€ë„ í•™ìŠµ
