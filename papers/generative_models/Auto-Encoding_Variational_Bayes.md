ğŸ“˜ Auto-Encoding Variational Bayes

## 1. ê°œìš” (Overview)

- **ì œëª©**: Auto-Encoding Variational Bayes  
- **ì €ì**: Kingma, Diederik P.; Welling, Max  
- **ì†Œì†**: University of Amsterdam  
- **í•™íšŒ**: ICLR 2014  
- **ë§í¬**:  
  - [arXiv](https://arxiv.org/abs/1312.6114)  
  - [GitHub (VAE example)](https://github.com/pytorch/examples/tree/main/vae)  
  - [Papers with Code](https://paperswithcode.com/paper/auto-encoding-variational-bayes)

### ë…¼ë¬¸ ì„ ì • ì´ìœ  ë° ë„ì…ë¶€

ì´ ë…¼ë¬¸ì€ **Variational Autoencoder (VAE)**ë¼ëŠ” ìƒì„± ëª¨ë¸ì˜ ê¸°ì´ˆë¥¼ ì œì‹œí•œ ëŒ€í‘œì ì¸ ì—°êµ¬ë¡œ, ë³µì¡í•œ í™•ë¥  ë¶„í¬ë¥¼ ê·¼ì‚¬í•˜ê¸° ìœ„í•œ **Variational Inference**ì™€ ë”¥ëŸ¬ë‹ì„ ê²°í•©í•œ ìµœì´ˆì˜ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.  

ì¬íŒŒë¼ë¯¸í„°ë¼ì´ì œì´ì…˜ íŠ¸ë¦­ì„ í™œìš©í•´ **gradient flowê°€ ê°€ëŠ¥í•œ ì ì¬ ë³€ìˆ˜ ëª¨ë¸ í•™ìŠµ**ì„ ê°€ëŠ¥ì¼€ í•˜ì˜€ìœ¼ë©°, ì´í›„ GAN, Flow, Diffusion ëª¨ë¸ ë“± ë‹¤ì–‘í•œ ìƒì„±ëª¨ë¸ ì—°êµ¬ì— í° ì˜í–¥ì„ ë¼ì³¤ìŠµë‹ˆë‹¤.  
ë”¥ëŸ¬ë‹ ê¸°ë°˜ ìƒì„±ëª¨ë¸ì˜ ì¶œë°œì ì´ ëœ í•µì‹¬ ë…¼ë¬¸ì…ë‹ˆë‹¤.

## 2. ë¬¸ì œ ì •ì˜ (Problem Formulation)

### ğŸ”§ ë¬¸ì œ ë° ê¸°ì¡´ í•œê³„

- **ì ì¬ ë³€ìˆ˜ ëª¨ë¸(latent variable model)**ì—ì„œëŠ” ê´€ì¸¡ ë°ì´í„° $x$ì— ëŒ€í•´ ê·¸ ì›ì¸ì„ ì„¤ëª…í•˜ëŠ” ì ì¬ ë³€ìˆ˜ $z$ë¥¼ ê°€ì •í•˜ê³ ,  
  $p(x) = \int p(x \mid z) p(z) \, dz$
  ì™€ ê°™ì€ í˜•íƒœë¡œ ë°ì´í„°ë¥¼ ëª¨ë¸ë§í•©ë‹ˆë‹¤.
  
- ê·¸ëŸ¬ë‚˜ ì´ ëª¨ë¸ì˜ **í›„ë°© ë¶„í¬ $p(z \mid x)$**ëŠ” ë³µì¡í•˜ê³ , ì •í™•í•œ ê³„ì‚°ì´ ì–´ë µê¸° ë•Œë¬¸ì— ì¼ë°˜ì ìœ¼ë¡œ ê·¼ì‚¬ë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.
  
- ê¸°ì¡´ì—ëŠ” **Markov Chain Monte Carlo (MCMC)**ë‚˜ **ë³€ë¶„ ì¶”ë¡ (Variational Inference)** ê¸°ë°˜ ê¸°ë²•ë“¤ì´ ì‚¬ìš©ë˜ì—ˆìœ¼ë‚˜, ê³„ì‚°ëŸ‰ì´ í¬ê³  ìƒ˜í”Œë§ì´ ëŠë¦¬ë©° ë”¥ëŸ¬ë‹ê³¼ ê²°í•©í•˜ê¸° ì–´ë µë‹¤ëŠ” í•œê³„ê°€ ìˆì—ˆìŠµë‹ˆë‹¤.

---

### ğŸ’¡ ì œì•ˆ ë°©ì‹ (Auto-Encoding Variational Bayes)

- **Encoder-Decoder êµ¬ì¡°**ë¥¼ ê°–ëŠ” **ë³€ë¶„ ì˜¤í† ì¸ì½”ë”(Variational Autoencoder, VAE)**ë¥¼ ì œì•ˆ

- ê·¼ì‚¬ posterior ë¶„í¬ $q_\phi(z \mid x)$ë¥¼ ì‹ ê²½ë§ìœ¼ë¡œ ëª¨ë¸ë§í•˜ê³ , ì´ë¥¼ í†µí•´ **ELBO (Evidence Lower Bound)**ë¥¼ ìµœëŒ€í™”í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ í•™ìŠµí•¨:\
$$\log p(x) \geq E_{q_\phi(z | x)}[\log p_\theta(x | z)] - D_{KL}(q_\phi(z | x) \parallel p(z))$$
- ë˜í•œ, **Reparameterization Trick**ì„ ë„ì…í•˜ì—¬ ìƒ˜í”Œë§ ê³¼ì •ì„ ë¯¸ë¶„ ê°€ëŠ¥í•˜ê²Œ ë§Œë“¤ì–´ backpropagationìœ¼ë¡œ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ í•¨

---

### ğŸ§  í•µì‹¬ ê°œë… ì •ì˜

- **Latent Variable**: ê´€ì¸¡ë˜ì§€ ì•Šì€ ì ì¬ ìš”ì¸. ì˜ˆ: ì´ë¯¸ì§€ ìƒì„± ì‹œ ìŠ¤íƒ€ì¼, ë‚´ìš©, ë°°ê²½ ë“±

- **Variational Inference**: ë³µì¡í•œ posteriorë¥¼ tractableí•œ $q(z \mid x)$ë¡œ ê·¼ì‚¬í•˜ì—¬ ìµœì í™”

- **ELBO (Evidence Lower Bound)**:  
  $$\mathcal{L} = E_{q_\phi(z | x)}[\log p_\theta(x | z)] - D_{KL}(q_\phi(z | x) \parallel p(z))$$
  â†’ ëª¨ë¸ í•™ìŠµ ì‹œ ìµœì í™”ë˜ëŠ” ëª©í‘œ í•¨ìˆ˜

- **Reparameterization Trick**:  
  $$z = \mu + \sigma \cdot \epsilon,\quad \epsilon \sim \mathcal{N}(0, I)$$
  â†’ ìƒ˜í”Œë§ì„ deterministicí•˜ê²Œ ì¬êµ¬ì„±í•˜ì—¬ gradient ì „íŒŒ ê°€ëŠ¥


## 3. ëª¨ë¸ êµ¬ì¡° (Architecture)

### ğŸ— Overall Architecture

Auto-Encoding Variational Bayes (VAE)ëŠ” í™•ë¥ ì  ìƒì„± ëª¨ë¸ì˜ ì ì¬ ë³€ìˆ˜ $z$ì— ëŒ€í•´, ê´€ì¸¡ëœ ë°ì´í„° $x$ë¥¼ ìµœëŒ€í•œ ì˜ ì„¤ëª…í•  ìˆ˜ ìˆë„ë¡ **approximate inference**ë¥¼ í•™ìŠµí•˜ëŠ” í”„ë ˆì„ì›Œí¬ë‹¤. 

ëª¨ë¸ì€ ë‹¤ìŒê³¼ ê°™ì€ êµ¬ì„±ìœ¼ë¡œ ì´ë£¨ì–´ì§„ë‹¤:

$$
x \xrightarrow{\text{Encoder } q_\phi(z \mid x)} (\mu, \sigma^2) \xrightarrow{\text{Sampling}} z \xrightarrow{\text{Decoder } p_\theta(x \mid z)} \hat{x}
$$

í•™ìŠµì˜ ëª©í‘œëŠ” $\log p_\theta(x)$ë¥¼ ì§ì ‘ ìµœëŒ€í™”í•˜ëŠ” ê²ƒì´ì§€ë§Œ, ì´ í•­ì€ ë‹¤ìŒê³¼ ê°™ì€ ë‚œí•´í•œ ì ë¶„ì„ í¬í•¨í•œë‹¤:

$$
\log p_\theta(x) = \log \int p_\theta(x \mid z) p(z) \, dz
$$

ì´ ë•Œë¬¸ì—, tractableí•œ ë¶„í¬ $q_\phi(z \mid x)$ë¥¼ í†µí•´ ì•„ë˜ì™€ ê°™ì€ **Evidence Lower Bound (ELBO)**ë¥¼ ìœ ë„í•˜ê³  ì´ë¥¼ ìµœëŒ€í™”í•¨ìœ¼ë¡œì¨ ê·¼ì‚¬ í•™ìŠµì„ ìˆ˜í–‰í•œë‹¤:

$$ \log p_\theta(x) \geq E_{q_\phi(z | x)}[\log p_\theta(x | z)] - D_{KL}(q_\phi(z | x) \parallel p(z)) $$

---

### ğŸ”· Encoder: $q_\phi(z|x)$
- ì…ë ¥ $x$ë¡œë¶€í„° ì ì¬ ë³€ìˆ˜ $z$ì˜ posterior ë¶„í¬ë¥¼ ê·¼ì‚¬í•˜ëŠ” ì¸í¼ëŸ°ìŠ¤ ëª¨ë¸ì´ë‹¤.
- ì‹ ê²½ë§ì„ í†µí•´ í‰ê·  $\mu_\phi(x)$ì™€ ë¡œê·¸ ë¶„ì‚° $\log \sigma_\phi^2(x)$ë¥¼ ì¶œë ¥í•˜ë©°, ì´ë¥¼ í†µí•´ ë‹¤ìŒê³¼ ê°™ì€ ë‹¤ë³€ëŸ‰ ì •ê·œë¶„í¬ë¥¼ ì •ì˜í•œë‹¤:\
  $$q_\phi(z | x) = \mathcal{N}(z; \mu_\phi(x), \text{diag}(\sigma^2_\phi(x)))$$
- ì´ ë¶„í¬ëŠ” ì‹¤ì œ posterior $p(z|x)$ë¥¼ ê·¼ì‚¬í•˜ë©°, KL ë°œì‚° í•­ì„ í†µí•´ prior $p(z)$ì™€ì˜ ì •ë ¬ì„ í•™ìŠµí•œë‹¤.

---

### ğŸ”· Latent Variable: $z$
- ì ì¬ ë³€ìˆ˜ $z$ëŠ” ë°ì´í„° $x$ì˜ ìƒì„± ì›ì¸ì„ ë‚´í¬í•œ ì €ì°¨ì›ì˜ ì¶”ìƒ í‘œí˜„ì´ë©°, prior ë¶„í¬ë¡œëŠ” isotropic Gaussianì„ ì‚¬ìš©:\
  $$p(z) = \mathcal{N}(0, I)$$
- VAEì˜ ëª©ì ì€ í•™ìŠµëœ posterior $q_\phi(z|x)$ê°€ ì´ priorì™€ ì •ë ¬ë˜ë„ë¡ ìœ ë„í•˜ëŠ” ê²ƒ

---

### ğŸ”· Reparameterization Trick
- $z \sim q_\phi(z|x)$ì—ì„œ ì§ì ‘ ìƒ˜í”Œë§í•˜ë©´ gradient ì „íŒŒê°€ ë¶ˆê°€ëŠ¥í•˜ë¯€ë¡œ, ì´ë¥¼ ë¯¸ë¶„ ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜:\
  $$z = \mu_\phi(x) + \sigma_\phi(x) \cdot \epsilon,\quad \epsilon \sim \mathcal{N}(0, I)$$
- ì´ ë°©ì‹ì€ ìƒ˜í”Œë§ì„ deterministicí•œ ì—°ì‚°ì²˜ëŸ¼ ë‹¤ë£¨ì–´ backpropagationì´ ê°€ëŠ¥í•˜ê²Œ ë§Œë“ ë‹¤.
- ì´ ê¸°ë²• ë•ë¶„ì— VAEëŠ” end-to-endë¡œ í•™ìŠµ ê°€ëŠ¥í•˜ë©°, ì´ëŠ” ë³¸ ë…¼ë¬¸ì˜ í•µì‹¬ ê¸°ì—¬ ì¤‘ í•˜ë‚˜ë‹¤.

---

### ğŸ”· Decoder: $p_\theta(x|z)$
- ìƒì„± ëª¨ë¸ë¡œ, ì ì¬ ë³€ìˆ˜ $z$ë¥¼ ì…ë ¥ë°›ì•„ ê´€ì¸¡ ë°ì´í„° $x$ë¥¼ ë³µì›í•œë‹¤.
- ì¼ë°˜ì ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ì€ ì¡°ê±´ë¶€ ì •ê·œë¶„í¬ í˜•íƒœë¥¼ ê°€ì •í•œë‹¤:\
  $$p_\theta(x | z) = \mathcal{N}(x; f_\theta(z), \sigma^2 I)$$
- ì—¬ê¸°ì„œ $f_\theta(z)$ëŠ” ë””ì½”ë” ì‹ ê²½ë§ìœ¼ë¡œ, ì ì¬ í‘œí˜„ $z$ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ì¬êµ¬ì„±ëœ $x$ë¥¼ ì¶œë ¥í•œë‹¤.
- Reconstruction lossëŠ” ë‹¤ìŒê³¼ ê°™ì´ ìœ ë„ëœë‹¤:\
  $$E_{q_\phi(z | x)}[\log p_\theta(x | z)] \approx -\frac{1}{2\sigma^2} \| x - f_\theta(z) \|^2 + \text{const}$$

---

### ğŸ”· Overall Objective: ELBO
- ì „ì²´ í•™ìŠµ ëª©í‘œëŠ” ë‹¤ìŒì˜ ELBOë¥¼ ìµœëŒ€í™”í•˜ëŠ” ê²ƒì´ë‹¤:\
  $$\mathcal{L}(x; \theta, \phi) = E_{q_\phi(z | x)}[\log p_\theta(x | z)] - D_{KL}(q_\phi(z | x) \parallel p(z))$$
- ì²« ë²ˆì§¸ í•­ì€ $z$ë¥¼ í†µí•´ $x$ë¥¼ ì–¼ë§ˆë‚˜ ì˜ ë³µì›í–ˆëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚´ë©° (likelihood),
- ë‘ ë²ˆì§¸ í•­ì€ posteriorê°€ priorì™€ ì–¼ë§ˆë‚˜ ê°€ê¹Œìš´ì§€ë¥¼ ì¸¡ì •í•˜ëŠ” ì •ê·œí™” í•­ì´ë‹¤.
- KL divergence í•­ì€ ì •ê·œë¶„í¬ ê°„ ë‹«íŒ í˜•íƒœ(closed form)ë¡œ ê³„ì‚°ëœë‹¤:\
  $$D_{KL}(q_\phi(z | x) \parallel p(z)) = \frac{1}{2} \sum_{i=1}^d \left( \mu_i^2 + \sigma_i^2 - \log \sigma_i^2 - 1 \right)$$
  
---

### ë‚´ ë§ë¡œ ì •ë¦¬í•˜ëŠ” VAE

## 1. VAEë€ ë¬´ì—‡ì¸ê°€?

**VAE (Variational Autoencoder)**ëŠ” ê¸°ì¡´ì˜ Autoencoder êµ¬ì¡°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ë˜, **í™•ë¥ ì ì¸(latent) ë¶„í¬ë¥¼ í•™ìŠµ**í•˜ì—¬ ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ìƒì„±í•  ìˆ˜ ìˆë„ë¡ ê³ ì•ˆëœ **ìƒì„± ëª¨ë¸(Generative Model)**ì´ë‹¤.

> ëª©í‘œ: í˜„ì‹¤ ë°ì´í„°ì™€ ìœ ì‚¬í•œ ë°ì´í„°ë¥¼ ìƒì„±í•˜ëŠ” ê²ƒ â†’ ì…ë ¥ ë°ì´í„°ì˜ **ì‹¤ì œ ë¶„í¬ p(x)** ë¥¼ ê·¼ì‚¬í•˜ëŠ” ê²ƒ

---

## 2. VAEì˜ ì „ì²´ êµ¬ì¡°

VAEëŠ” í¬ê²Œ **ì„¸ ê°€ì§€ êµ¬ì„± ìš”ì†Œ**ë¡œ ì´ë£¨ì–´ì§„ë‹¤:

- **Encoder**: ì…ë ¥ ë°ì´í„°ë¥¼ ì ì¬ ê³µê°„(latent space)ì˜ í™•ë¥  ë¶„í¬ë¡œ ë§¤í•‘  
- **Latent Space**: í•™ìŠµëœ ì ì¬ ë²¡í„° zë¥¼ ì €ì¥í•˜ëŠ” ê³µê°„  
- **Decoder**: ì ì¬ ë²¡í„° zë¥¼ ì´ìš©í•´ ì›ë˜ ë°ì´í„°ë¥¼ ë³µì›í•˜ê±°ë‚˜ ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ìƒì„±

Input x â†’ [Encoder] â†’ z (latent) â†’ [Decoder] â†’ Output x'

## 3. Encoder: ì…ë ¥ì„ í™•ë¥  ë¶„í¬ë¡œ ë³€í™˜

### ì—­í• 
- ì…ë ¥ $x$ë¥¼ ë°›ì•„ì„œ ì ì¬ ë³€ìˆ˜ $z$ì˜ ë¶„í¬ $q(z|x)$ë¥¼ ì¶”ì •
- VAEì—ì„œëŠ” ì´ ë¶„í¬ë¥¼ **ì •ê·œë¶„í¬ $\mathcal{N}(\mu, \sigma^2)$**ë¡œ ê°€ì •

### í•™ìŠµ ë‚´ìš©
- ì…ë ¥ ë°ì´í„°ë¥¼ ì¸ì½”ë”©í•œ ë’¤,
  - í‰ê·  $\mu$
  - í‘œì¤€í¸ì°¨ $\sigma$
  ë¥¼ ì¶”ì •í•˜ì—¬ latent variableì˜ í™•ë¥  ë¶„í¬ë¥¼ ì •ì˜í•¨

$q(z|x) = \mathcal{N}(\mu(x), \sigma^2(x))$

## 4. ğŸ§­ Latent Space: ì ì¬ ê³µê°„ì˜ ë²¡í„° í‘œí˜„

### â— ë¬¸ì œì 
- ì¼ë°˜ì ì¸ AutoencoderëŠ” latent spaceê°€ **ë¶ˆì—°ì†ì **ì´ê³  **ì˜ë¯¸ ì—†ëŠ” ê³µê°„**ì´ ë  ìˆ˜ ìˆìŒ
- ë”°ë¼ì„œ ìƒˆë¡œìš´ zë¥¼ ìƒ˜í”Œë§í•´ë„ ìœ íš¨í•œ xë¥¼ ìƒì„±í•˜ì§€ ëª»í•  ìˆ˜ ìˆìŒ

### âœ… í•´ê²°ì±…: Reparameterization Trick

ì ì¬ ë²¡í„° zëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì¬ì •ì˜ë¨:

$$
z = \mu + \sigma \cdot \epsilon, \quad \epsilon \sim \mathcal{N}(0, 1)
$$
- $\epsilon$: í‘œì¤€ ì •ê·œë¶„í¬ì—ì„œ ìƒ˜í”Œë§í•œ ë…¸ì´ì¦ˆ
- $\mu, \sigma$: Encoderì—ì„œ ì¶”ì¶œí•œ í‰ê· ê³¼ í‘œì¤€í¸ì°¨
> ì´ ë°©ì‹ì€ í™•ë¥ ì  samplingì„ deterministic í•¨ìˆ˜ë¡œ ë³€í˜•í•˜ì—¬ **ì—­ì „íŒŒ(Backpropagation)**ì´ ê°€ëŠ¥í•˜ê²Œ í•¨
---
## 5. ğŸ›  Decoder: zë¥¼ xë¡œ ë³µì›
- DecoderëŠ” ì ì¬ ë²¡í„° $z$ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ì›ë˜ ì…ë ¥ $x$ë¥¼ ë³µì›
- ìš°ë¦¬ê°€ ì¶”ì •í•˜ê³ ì í•˜ëŠ” ë¶„í¬ëŠ”:
$$p(x|z)$$
> í•™ìŠµì´ ì™„ë£Œë˜ë©´ **Decoderë§Œìœ¼ë¡œë„ ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ìƒì„±**í•  ìˆ˜ ìˆê²Œ ë¨
---
## 6. ğŸ¯ VAE í•™ìŠµ ëª©í‘œ: Likelihood ìµœëŒ€í™”
### ê¸°ë³¸ ëª©í‘œ
ì…ë ¥ ë°ì´í„° $x$ì˜ marginal likelihoodë¥¼ ìµœëŒ€í™”:
$$\log p(x) = \log \int p(x|z) p(z) \, dz$$
â†’ í•˜ì§€ë§Œ ìœ„ í•­ì€ ì§ì ‘ ê³„ì‚°ì´ ë¶ˆê°€ëŠ¥
### â–¶ í•´ê²°ì±…: Evidence Lower Bound (ELBO)
---
## 7. ğŸ“‰ Evidence Lower Bound (ELBO)
ë§ˆë¥´ì½”í”„ ë¶€ë“±ì‹ê³¼ ë³€ë¶„ ì¶”ì •ì„ í†µí•´ ë‹¤ìŒ ì‹ì„ ì–»ì„ ìˆ˜ ìˆìŒ:\
$$
\log p(x) \geq \mathrm{E}_{q(z|x)}[\log p(x|z)] - D_{\mathrm{KL}}(q(z|x) \| p(z))
$$
### í•­ëª©ë³„ ì˜ë¯¸
| í•­ | ì´ë¦„ | ì„¤ëª… |
|----|------|------|
| $\mathbb{E}_{q(z\|x)}[\log p(x \| z)]$ | Reconstruction Term | Decoderê°€ xë¥¼ ì–¼ë§ˆë‚˜ ì˜ ë³µì›í•˜ëŠ”ì§€ë¥¼ ì¸¡ì •í•˜ëŠ” í•­. Autoencoderì˜ Reconstruction Lossì™€ ìœ ì‚¬ |
| $D_{KL}[q(z \| x) \| p(z)]$ | Regularization Term | ì ì¬ ê³µê°„ì—ì„œ $q(z \| x)$ê°€ prior ë¶„í¬ $p(z) \sim \mathcal{N}(0,1)$ì™€ ì–¼ë§ˆë‚˜ ìœ ì‚¬í•œì§€ë¥¼ ì¸¡ì •í•˜ëŠ” í•­. ëª¨ë¸ì´ ì˜ë¯¸ ìˆëŠ” latent spaceë¥¼ ê°–ë„ë¡ í•¨ |

> ë”°ë¼ì„œ **ELBOë¥¼ ìµœëŒ€í™”í•˜ëŠ” ê²ƒì´ VAEì˜ í•™ìŠµ ëª©í‘œ**ê°€ ë¨

---

## 8. ğŸ” ELBO ìœ ë„ ê°„ë‹¨ ì„¤ëª…
ELBOëŠ” ë‹¤ìŒ ì‹ì—ì„œ ìœ ë„ë¨:

$$\log p(x) = \mathbb{E}_{q(z|x)}[\log p(x|z)] - D_{KL}(q(z|x)\|p(z)) + D_{KL}(q(z|x)\|p(z|x))$$

- ë§ˆì§€ë§‰ í•­ $D_{KL}(q(z|x)\|p(z|x)) \geq 0$
- ë”°ë¼ì„œ ìš°ë³€ì˜ ë‚˜ë¨¸ì§€ ë‘ í•­ì´ **Evidence Lower Bound (ELBO)**ê°€ ë¨

$$\log p(x) \geq \mathbb{E}_{q(z|x)}[\log p(x|z)] - D_{KL}[q(z|x)\|p(z)]$$

---

## 9. ğŸ¨ í•™ìŠµ í›„ ë°ì´í„° ìƒì„± (Sampling)
í•™ìŠµì´ ì™„ë£Œëœ í›„, ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ìƒì„±í•˜ëŠ” ê³¼ì •:

1. ì ì¬ ê³µê°„ì—ì„œ ìƒˆë¡œìš´ ë²¡í„° $z$ë¥¼ ìƒ˜í”Œë§:

$z \sim \mathcal{N}(0, 1)$

2. ì´ë¥¼ decoderì— ì…ë ¥:

$x_{\text{new}} = \text{Decoder}(z)$

â†’ ì´ ë°©ì‹ìœ¼ë¡œ **ì™„ì „íˆ ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ìƒì„±**í•  ìˆ˜ ìˆìŒ

---## ğŸ”š ìµœì¢… ì •ë¦¬
| êµ¬ì„± ìš”ì†Œ | ì—­í•  |
|-----------|------|
| **Encoder** | ì…ë ¥ $x$ â†’ ì ì¬ ë¶„í¬ $q(z|x)$ ì¶”ì • |
| **Latent Space** | $z$ ë²¡í„° ê³µê°„. Reparameterization trick ì ìš© |
| **Decoder** | $z$ â†’ $x$ ë³µì›. $p(x|z)$ í•™ìŠµ |
| **ëª©ì  í•¨ìˆ˜** | ELBO = Reconstruction Term + Regularization Term |
| **í•™ìŠµ ë°©ì‹** | ELBOë¥¼ ìµœëŒ€í™”í•˜ì—¬ $\log p(x)$ë¥¼ ê·¼ì‚¬ |
| **ìƒì„± ë°©ì‹** | $z \sim \mathcal{N}(0,1) \Rightarrow x = \text{Decoder}(z)$ |
---
### âš–ï¸ Comparison with Deterministic Autoencoder
| Component     | Traditional Autoencoder         | Variational Autoencoder (VAE)             |
|---------------|----------------------------------|--------------------------------------------|
| Latent $z$    | Deterministic vector             | Probabilistic latent variable              |
| Encoder       | $z = f(x)$                       | $q_\phi(z \mid x) = \mathcal{N}(\mu, \sigma^2)$ |
| Decoder       | $\hat{x} = g(z)$                 | $p_\theta(x \mid z)$ (likelihood model)    |
| Objective     | $\|x - \hat{x}\|^2$              | ELBO (reconstruction + KL divergence)      |
| Regularization| L2 weight decay (optional)       | Prior-matching via KL divergence           |
## 4. ì‹¤í—˜ ë° ê²°ê³¼ (Experiments & Results)

| í•­ëª©       | ë³¸ ë…¼ë¬¸ (VAE)                                      | ê¸°ì¡´ ë°©ë²• 1 (AE)                          | ê¸°ì¡´ ë°©ë²• 2 (MCMC ê¸°ë°˜ Variational Inference) |
|------------|----------------------------------------------------|-------------------------------------------|----------------------------------------------|
| êµ¬ì¡°       | Probabilistic encoder-decoder + latent sampling   | Deterministic encoder-decoder             | Probabilistic graphical model + sampling     |
| í•™ìŠµ ë°©ì‹  | ELBO maximization via SGD + reparameterization    | MSE minimization via SGD                 | Variational EM or Gibbs sampling              |
| ëª©ì        | Efficient variational inference with deep nets    | Dimensionality reduction & reconstruction | Exact posterior inference                    |

---

### ğŸ“‰ ì‹¤í—˜ ë° ê²°ê³¼

- **ë°ì´í„°ì…‹**:
  - MNIST (handwritten digits)
  - Frey Faces (face image sequences)
  - Synthetic 2D data (for visualization of latent space)

- **ë¹„êµ ëª¨ë¸**:
  - Traditional Autoencoder (AE)
  - Deep Belief Network (DBN)
  - Factorial Mixture Models (for probabilistic comparison)

- **ì£¼ìš” ì„±ëŠ¥ ì§€í‘œ**:
  - ì •ëŸ‰ ì§€í‘œ: **Negative log-likelihood (NLL)** (lower is better)
  - ì •ì„± ì§€í‘œ: **ìƒ˜í”Œë§ëœ ì´ë¯¸ì§€ì˜ ì‹œê°ì  í’ˆì§ˆ**, latent space interpolation

| ëª¨ë¸         | Accuracy | F1 Score | NLL â†“       | ê¸°íƒ€ |
|--------------|----------|----------|-------------|------|
| ë³¸ ë…¼ë¬¸ (VAE)| -        | -        | ~86.6 (MNIST)| Latent interpolation, smooth manifold |
| ê¸°ì¡´ AE      | -        | -        | ~103         | Latent space not smooth               |
| ê¸°ì¡´ DBN     | -        | -        | ~84.6        | Better NLL, but harder to train       |

> **Note**: VAEëŠ” classification ì„±ëŠ¥ë³´ë‹¤ëŠ” **ìƒì„± í™•ë¥  ëª¨ë¸ë¡œì„œì˜ ì ì ˆì„±**(NLL, ìƒ˜í”Œ í’ˆì§ˆ, latent êµ¬ì¡° ë“±)ì— ì´ˆì ì„ ë‘” ì‹¤í—˜ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

---

### ì‹¤í—˜ ê²°ê³¼ ìš”ì•½ ë° í•´ì„

- VAEëŠ” MNISTì—ì„œ ê¸°ì¡´ AEë³´ë‹¤ í›¨ì”¬ ë‚®ì€ NLLì„ ê¸°ë¡í•˜ë©°, probabilistic ëª¨ë¸ë¡œì„œ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì„
- latent spaceê°€ ì—°ì†ì ì´ë©° ì˜ë¯¸ ìˆëŠ” interpolationì„ ê°€ëŠ¥ì¼€ í•¨
- ìƒ˜í”Œë§ëœ ì´ë¯¸ì§€ëŠ” DBN ëŒ€ë¹„ ì•½ê°„ í’ˆì§ˆì´ ë‚®ì§€ë§Œ, í•™ìŠµê³¼ ìƒ˜í”Œë§ì˜ íš¨ìœ¨ì„±ê³¼ í•´ì„ ê°€ëŠ¥ì„±ì´ ë›°ì–´ë‚¨
- Frey Facesì—ì„œë„ smoothí•œ face transition ê°€ëŠ¥

---

## âœ… ì¥ì  ë° í•œê³„

### ì¥ì 

- í™•ë¥  ëª¨ë¸ ê¸°ë°˜ì˜ end-to-end í•™ìŠµ ê°€ëŠ¥ (SGD + backprop)
- ë³µì¡í•œ posterior ë¶„í¬ë¥¼ neural networkë¡œ ê·¼ì‚¬ ê°€ëŠ¥
- Reparameterization trickì„ í†µí•´ ìƒ˜í”Œë§ ê¸°ë°˜ ì¶”ë¡ ì´ gradient-friendlyí•˜ê²Œ ì²˜ë¦¬ë¨
- latent spaceê°€ smoothí•˜ê³  ì˜ë¯¸ ìˆëŠ” êµ¬ì¡°ë¥¼ ê°€ì§ (generative priorì™€ì˜ ì •ë ¬)

### í•œê³„ ë° ê°œì„  ê°€ëŠ¥ì„±

- Gaussian ê°€ì • í•˜ì˜ ë³µì› í’ˆì§ˆ ì œí•œ (ìƒ˜í”Œì´ blurry)
- Likelihood ê¸°ë°˜ í•™ìŠµì€ ì •í™•í•œ ë¶„í¬ ëª¨ë¸ë§ì— í•œê³„
- Posterior expressivenessê°€ ì œí•œì  (ë‹¨ì¼ Gaussian)
- ì´í›„ ë…¼ë¬¸ì—ì„œëŠ” ì´ë¥¼ ê°œì„ í•˜ê¸° ìœ„í•´ VAE-GAN, Flow-based VAE, Hierarchical VAE ë“±ì´ ë“±ì¥í•¨


## ğŸ§  TL;DR â€“ í•œëˆˆì— ìš”ì•½

**ğŸ”¹ í•µì‹¬ ì•„ì´ë””ì–´ ìš”ì•½**

> Auto-Encoding Variational Bayes (VAE)ëŠ” **intractableí•œ ë² ì´ì§€ì•ˆ ì¶”ë¡ **ì„ **ì‹ ê²½ë§ ê¸°ë°˜ ê·¼ì‚¬ posteriorì™€ reparameterization trick**ì„ í†µí•´ íš¨ìœ¨ì ìœ¼ë¡œ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ ë§Œë“  **ìµœì´ˆì˜ end-to-end deep generative model**ì´ë‹¤.

- ê¸°ì¡´ì˜ MCMC ë˜ëŠ” Variational EM ê¸°ë°˜ ì¶”ë¡ ì€ ê³„ì‚° ë³µì¡ë„ì™€ gradient íë¦„ ë¬¸ì œë¡œ deep learningê³¼ ê²°í•©ì´ ì–´ë ¤ì› ìŒ
- VAEëŠ” ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ **(1) ê·¼ì‚¬ posterior ë¶„í¬ë¥¼ ì¸ì½”ë” ë„¤íŠ¸ì›Œí¬ë¡œ ëª¨ë¸ë§**í•˜ê³ , **(2) ìƒ˜í”Œë§ì„ ë¯¸ë¶„ ê°€ëŠ¥í•˜ê²Œ ë§Œë“œëŠ” reparameterization trick**ì„ ë„ì…í•˜ì—¬ í•™ìŠµ ê°€ëŠ¥ì„±ì„ í™•ë³´í•¨
- í•™ìŠµ ëª©í‘œëŠ” evidence lower bound (ELBO)ë¥¼ ìµœëŒ€í™”í•˜ëŠ” ê²ƒ:
  $$
  \log p(x) \geq \mathbb{E}_{q(z \mid x)}[\log p(x \mid z)] - D_{\mathrm{KL}}(q(z \mid x) \parallel p(z))
  $$
- ê²°ê³¼ì ìœ¼ë¡œ VAEëŠ” **í™•ë¥ ì  ìƒì„± ëª¨ë¸ê³¼ representation learningì„ ìì—°ìŠ¤ëŸ½ê²Œ í†µí•©**í•˜ë©°, ì´í›„ ìˆ˜ë§ì€ generative modelì˜ ê¸°ë°˜ì´ ë¨

---

### ğŸ“¦ êµ¬ì„± ìš”ì†Œ ì •ë¦¬

| êµ¬ì„± ìš”ì†Œ       | ì„¤ëª… |
|----------------|------|
| **í•µì‹¬ ëª¨ë“ˆ**     | - Encoder $q_\phi(z \mid x)$: ì…ë ¥ì„ ì •ê·œë¶„í¬ íŒŒë¼ë¯¸í„° $(\mu, \sigma)$ë¡œ ë§¤í•‘<br>- Decoder $p_\theta(x \mid z)$: ìƒ˜í”Œë§ëœ $z$ë¡œë¶€í„° $x$ ë³µì›<br>- Reparameterization Trick: $z = \mu + \sigma \cdot \epsilon$, $\epsilon \sim \mathcal{N}(0, I)$ |
| **í•™ìŠµ ì „ëµ**     | - ELBO maximization<br>- SGD + backpropagation<br>- Closed-form KL divergence ì‚¬ìš© |
| **ì „ì´ ë°©ì‹**     | - Posterior-to-prior regularization<br>- Latent spaceë¥¼ $p(z) = \mathcal{N}(0, I)$ë¡œ ì •ë ¬<br>- Interpolationê³¼ samplingì´ ìì—°ìŠ¤ëŸ½ê²Œ ê°€ëŠ¥ |
| **ì„±ëŠ¥/íš¨ìœ¨ì„±**   | - MNIST ë“±ì—ì„œ low NLL<br>- AEë³´ë‹¤ generalization ë° latent smoothness ìš°ìˆ˜<br>- DBN ìˆ˜ì¤€ ì„±ëŠ¥ + ë” ë‚˜ì€ í•™ìŠµ ì•ˆì •ì„± |

---

### ğŸ”— ì°¸ê³  ë§í¬ (References)

- ğŸ“„ [arXiv ë…¼ë¬¸](https://arxiv.org/abs/1312.6114)
- ğŸ’» [PyTorch VAE ì˜ˆì œ ì½”ë“œ](https://github.com/pytorch/examples/tree/main/vae)
- ğŸ“ˆ [Papers with Code â€“ VAE](https://paperswithcode.com/paper/auto-encoding-variational-bayes)

---

ë‹¤ìŒ ë…¼ë¬¸:**[Generative Adversarial Nets (Goodfellow et al., 2014)]**  aka GAN
