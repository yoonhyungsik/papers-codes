# ğŸ“˜ Masked Autoencoders Are Scalable Vision Learners

## 1. ê°œìš” (Overview)

- **ì œëª©**: Masked Autoencoders Are Scalable Vision Learners  
- **ì €ì**: Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr DollÃ¡r, Ross Girshick  
- **ì†Œì†**: Meta AI (Facebook AI Research, FAIR)  
- **í•™íšŒ**: CVPR 2022  
- **ë§í¬**:  
  - [arXiv](https://arxiv.org/abs/2111.06377)  
  - [GitHub](https://github.com/facebookresearch/mae)  
  - [Papers with Code](https://paperswithcode.com/paper/masked-autoencoders-are-scalable-vision)

> ë…¼ë¬¸ ì„ ì • ì´ìœ  ë° ê°„ë‹¨í•œ ë„ì…ë¶€ ì‘ì„±

ìµœê·¼ ë”¥ëŸ¬ë‹ ë¶„ì•¼ëŠ” ëª¨ë¸ ê·œëª¨ì™€ ì„±ëŠ¥ì´ í­ë°œì ìœ¼ë¡œ ì„±ì¥í•˜ë©°, ìˆ˜ì–µ ê°œì˜ ë¼ë²¨ì´ í•„ìš”í•œ ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ì— ëŒ€í•œ ì˜ì¡´ë„ê°€ ì¦ê°€í•˜ê³  ìˆë‹¤. ìì—°ì–´ ì²˜ë¦¬(NLP) ë¶„ì•¼ì—ì„œëŠ” ì´ëŸ¬í•œ ë¬¸ì œë¥¼ ìê¸°ì§€ë„í•™ìŠµ(self-supervised learning)ì„ í†µí•´ ì„±ê³µì ìœ¼ë¡œ í•´ê²°í–ˆë‹¤. 

ëŒ€í‘œì ìœ¼ë¡œ GPTì˜ ì˜¤í† ë ˆê·¸ë ˆì‹œë¸Œ í•™ìŠµê³¼ BERTì˜ ë§ˆìŠ¤í‚¹ ê¸°ë°˜ ì˜¤í† ì¸ì½”ë”©(Masked Autoencoding)ì€ ë°ì´í„°ì˜ ì¼ë¶€ë¥¼ ì œê±°í•œ í›„ ì´ë¥¼ ë³µì›í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ íš¨ê³¼ì ì¸ í‘œí˜„ í•™ìŠµì„ ê°€ëŠ¥í•˜ê²Œ í–ˆë‹¤. ì´ëŸ¬í•œ ë°©ì‹ì€ í˜„ì¬ ìˆ˜ë°±ì–µ íŒŒë¼ë¯¸í„° ê·œëª¨ì˜ ë²”ìš© ì–¸ì–´ ëª¨ë¸ í•™ìŠµì—ë„ ì ìš©ë˜ê³  ìˆë‹¤.

ì´ ë…¼ë¬¸ì—ì„œëŠ” **Masked Autoencoders (MAE)** ë¼ëŠ” ë°©ì‹ìœ¼ë¡œ, NLPì—ì„œ ê²€ì¦ëœ ë§ˆìŠ¤í‚¹ ê¸°ë°˜ ì‚¬ì „í•™ìŠµì„ **Vision Transformer (ViT)** êµ¬ì¡°ì— íš¨ê³¼ì ìœ¼ë¡œ ì ìš©í•œë‹¤. íŠ¹íˆ ì´ë¯¸ì§€ì˜ ê³µê°„ì  ì¤‘ë³µì„±ê³¼ ë‚®ì€ ì •ë³´ ë°€ë„ë¥¼ ê³ ë ¤í•˜ì—¬, ì „ì²´ íŒ¨ì¹˜ ì¤‘ 75% ì´ìƒì˜ ë†’ì€ ë¹„ìœ¨ì„ ë§ˆìŠ¤í‚¹í•˜ê³ , ë‚¨ì€ ì†Œìˆ˜ì˜ íŒ¨ì¹˜ë§Œìœ¼ë¡œ ë³µì›ì„ ìˆ˜í–‰í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ê³ ë‚œì´ë„ì˜ ìê°€í•™ìŠµ ê³¼ì œë¥¼ ë§Œë“ ë‹¤.

ë˜í•œ, MAEëŠ” **ë¹„ëŒ€ì¹­ ì¸ì½”ë”-ë””ì½”ë” êµ¬ì¡°**ë¥¼ í†µí•´ ê³„ì‚° íš¨ìœ¨ì„±ê¹Œì§€ í™•ë³´í•œë‹¤. ì¸ì½”ë”ëŠ” ê°€ì‹œ íŒ¨ì¹˜(visible patches)ì—ë§Œ ì‘ë™í•˜ë©°, ë§ˆìŠ¤í¬ í† í°(mask token)ì€ ë””ì½”ë”ì—ì„œë§Œ ì²˜ë¦¬í•œë‹¤. ì´ë¡œ ì¸í•´ **ì‚¬ì „í•™ìŠµ ì‹œê°„ê³¼ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ 3ë°° ì´ìƒ ì ˆê°**í•˜ë©´ì„œë„ ë†’ì€ ì„±ëŠ¥ì„ ìœ ì§€í•  ìˆ˜ ìˆë‹¤.

### ğŸ“Œ ë…¼ë¬¸ ì„ ì • ì´ìœ 
- **BERTì˜ ë§ˆìŠ¤í‚¹ ì „ëµì„ Vision ë„ë©”ì¸ì— ì„±ê³µì ìœ¼ë¡œ ì´ì‹**
- **75% ì´ìƒ ë§ˆìŠ¤í‚¹**ì´ë¼ëŠ” ë„ì „ì  í•™ìŠµ ê³¼ì œë¥¼ í†µí•´ ê³ ì°¨ì› í‘œí˜„ í•™ìŠµ ìœ ë„
- **ë¹„ëŒ€ì¹­ ì„¤ê³„**ë¥¼ í†µí•´ ê³„ì‚° íš¨ìœ¨ì„±ê³¼ ëª¨ë¸ í™•ì¥ì„± ë™ì‹œì— í™•ë³´
- ë‹¨ì¼ ImageNet-1K ë°ì´í„°ì…‹ë§Œìœ¼ë¡œë„ SOTA ë‹¬ì„±
- ê°ì²´ íƒì§€, ë¶„í•  ë“± ë‹¤ì–‘í•œ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ë¹„ì „ íƒœìŠ¤í¬ì—ì„œ ì„±ëŠ¥ í–¥ìƒ ì…ì¦

---
## 2. ë¬¸ì œ ì •ì˜ (Problem Formulation)

### ğŸ”¹ ë¬¸ì œ ë° ê¸°ì¡´ í•œê³„

* ìµœê·¼ Vision Transformer(ViT)ì˜ ë¶€ìƒê³¼ í•¨ê»˜, ëŒ€ìš©ëŸ‰ ì´ë¯¸ì§€ ëª¨ë¸ì„ í•™ìŠµí•˜ê¸° ìœ„í•œ íš¨ê³¼ì ì¸ **ì‚¬ì „í•™ìŠµ ì „ëµ**ì˜ í•„ìš”ì„±ì´ ëŒ€ë‘ë˜ê³  ìˆìŒ
* ê¸°ì¡´ì˜ ê°ë…í•™ìŠµ(supervised learning)ì€ ìˆ˜ì–µ ê°œ ìˆ˜ì¤€ì˜ **ë¼ë²¨ë§ëœ ë°ì´í„°ì— ëŒ€í•œ ì˜ì¡´ë„**ê°€ ë†’ì•„, ì¼ë°˜ì ì¸ ì‚¬ìš©ì— í•œê³„ê°€ ì¡´ì¬
* ìì—°ì–´ ì²˜ë¦¬(NLP)ì—ì„œëŠ” ë§ˆìŠ¤í‚¹ ê¸°ë°˜ì˜ ìê¸°ì§€ë„í•™ìŠµ ë°©ì‹(BERT, GPT)ì´ ëŒ€ê·œëª¨ ë°ì´í„° ì—†ì´ë„ **ë²”ìš© í‘œí˜„ í•™ìŠµì— ì„±ê³µ**í•˜ì˜€ìœ¼ë‚˜,
  - ì»´í“¨í„° ë¹„ì „ì—ì„œëŠ” ì•„ì§ **ë§ˆìŠ¤í‚¹ ê¸°ë°˜ í•™ìŠµì´ ì¶©ë¶„íˆ í™•ì¥ë˜ì§€ ëª»í•¨**
  - ì´ëŠ” êµ¬ì¡°ì  í•œê³„(ê³¼ê±°ì—ëŠ” CNN ê¸°ë°˜), ì •ë³´ ë°€ë„ ì°¨ì´(ì´ë¯¸ì§€ vs. í…ìŠ¤íŠ¸), ë””ì½”ë” ì—­í• ì˜ ì°¨ì´ ë“±ì´ ì£¼ëœ ì›ì¸

### ğŸ”¹ ì œì•ˆ ë°©ì‹ (Masked Autoencoder, MAE)

* MAEëŠ” ì´ë¯¸ì§€ì˜ ëŒ€ë¶€ë¶„(ì˜ˆ: 75%)ì„ ë¬´ì‘ìœ„ë¡œ **ë§ˆìŠ¤í‚¹í•œ í›„**, ë‚˜ë¨¸ì§€ ì¼ë¶€ íŒ¨ì¹˜ë§Œì„ ì¸ì½”ë”ì— ì…ë ¥
* **ë¹„ëŒ€ì¹­ ì¸ì½”ë”-ë””ì½”ë” êµ¬ì¡°**ë¥¼ í†µí•´ íš¨ìœ¨ì„± í™•ë³´
  - ì¸ì½”ë”: ê°€ì‹œ íŒ¨ì¹˜(visible patches)ë§Œ ì²˜ë¦¬ â†’ ê³„ì‚°ëŸ‰ ì ˆê°
  - ë””ì½”ë”: ë§ˆìŠ¤í¬ëœ ìœ„ì¹˜ë¥¼ í¬í•¨í•œ ì „ì²´ íŒ¨ì¹˜ë¥¼ ë³µì›
* ë³µì› ëŒ€ìƒì€ **í”½ì…€ ë‹¨ìœ„ì˜ ì›ë³¸ ì´ë¯¸ì§€**ë¡œ, ë‚®ì€ ìˆ˜ì¤€ì˜ ì¬êµ¬ì„±ì´ì§€ë§Œ í•™ìŠµëœ í‘œí˜„ì€ ê³ ìˆ˜ì¤€ ì¸ì‹ íƒœìŠ¤í¬ì— íš¨ê³¼ì 
* ë§ˆìŠ¤í‚¹ ë¹„ìœ¨ì„ ë†’ê²Œ ì„¤ì •í•¨ìœ¼ë¡œì¨, ë‹¨ìˆœí•œ ë³µì›ì´ ì•„ë‹Œ **ì „ì²´ ì´ë¯¸ì§€ì— ëŒ€í•œ ì´í•´ì™€ ì¶”ë¡ **ì„ ìš”êµ¬

> â€» **í•µì‹¬ ê°œë… ì •ì˜**
>
> - **Masked Autoencoding (MAE)**: ì…ë ¥ ì´ë¯¸ì§€ì˜ ì¼ë¶€ íŒ¨ì¹˜ë¥¼ ë§ˆìŠ¤í‚¹í•˜ê³ , ë‚˜ë¨¸ì§€ ì •ë³´ë¥¼ í†µí•´ ì›ë³¸ì„ ë³µì›í•˜ëŠ” ìê¸°ì§€ë„ í•™ìŠµ ë°©ì‹  
> - **Visible Patches**: ë§ˆìŠ¤í‚¹ë˜ì§€ ì•Šì€ ì…ë ¥ íŒ¨ì¹˜ë¡œ, ì¸ì½”ë”ì˜ ì…ë ¥ì´ ë¨  
> - **Mask Tokens**: ë§ˆìŠ¤í‚¹ëœ ìœ„ì¹˜ì— ì‚½ì…ë˜ì–´ ë””ì½”ë”ê°€ ë³µì› ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” íŠ¹ìˆ˜ í† í°  
> - **Asymmetric Encoder-Decoder**: ê³„ì‚° íš¨ìœ¨ì„±ì„ ìœ„í•´ encoderì™€ decoderì˜ ì—­í• /í¬ê¸°ë¥¼ êµ¬ë¶„í•˜ëŠ” ì„¤ê³„ ì „ëµ  


---

## 3. ëª¨ë¸ êµ¬ì¡° (Architecture)

### ğŸ”· ì „ì²´ êµ¬ì¡°

![ëª¨ë¸ êµ¬ì¡°](/papers/images/mae_architecture.png)

MAE (Masked Autoencoders)ëŠ” **ë¹„ëŒ€ì¹­ì  ì¸ì½”ë”-ë””ì½”ë” êµ¬ì¡°(asymmetric encoder-decoder architecture)**ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” ìê¸°ì§€ë„ í•™ìŠµ(self-supervised learning) í”„ë ˆì„ì›Œí¬ë¡œ, **ì…ë ¥ ì´ë¯¸ì§€ì˜ ì¼ë¶€ë§Œì„ ì¸ì½”ë”ì— ì œê³µí•˜ê³ , ë§ˆìŠ¤í‚¹ëœ ì •ë³´ë¥¼ ë””ì½”ë”ê°€ ë³µì›**í•˜ë„ë¡ í•™ìŠµëœë‹¤. ì´ ì„¤ê³„ëŠ” í•™ìŠµ íš¨ìœ¨ì„±ê³¼ í‘œí˜„ë ¥ í™•ë³´ë¼ëŠ” ë‘ ê°€ì§€ ëª©í‘œë¥¼ ë™ì‹œì— ë‹¬ì„±í•˜ëŠ” ë° ì´ˆì ì„ ë§ì¶”ê³  ìˆë‹¤.

ì…ë ¥ ì´ë¯¸ì§€ $x \in \mathbb{R}^{H \times W \times C}$ëŠ” ê³ ì • í¬ê¸°ì˜ íŒ¨ì¹˜(patch)ë¡œ ë¶„í• ë˜ë©°, ê° íŒ¨ì¹˜ëŠ” ì„ í˜• ì„ë² ë”© ë° ìœ„ì¹˜ ì¸ì½”ë”©ì„ í†µí•´ í† í°í™”ëœë‹¤. ì „ì²´ íŒ¨ì¹˜ ì¤‘ **75% ì´ìƒì„ ë§ˆìŠ¤í‚¹**í•˜ê³ , ë‚˜ë¨¸ì§€ 25%ì˜ **ê°€ì‹œ íŒ¨ì¹˜(visible patches)**ë§Œ ì¸ì½”ë”ì— ì…ë ¥ëœë‹¤. ì´í›„, **ë§ˆìŠ¤í¬ í† í°(mask token)**ì„ ì‚½ì…í•˜ì—¬ ì „ì²´ ì‹œí€€ìŠ¤ë¥¼ êµ¬ì„±í•˜ê³ , ë””ì½”ë”ê°€ í”½ì…€ ë³µì›ì„ ìˆ˜í–‰í•œë‹¤.

---

### ğŸ’  í•µì‹¬ ëª¨ë“ˆ ë° êµ¬ì„± ìš”ì†Œ ìƒì„¸ ë¶„ì„

#### ğŸ“Œ Patch Embedding Module

* ì…ë ¥ ì´ë¯¸ì§€ëŠ” $P \times P$ í¬ê¸°ì˜ non-overlapping íŒ¨ì¹˜ë¡œ ë¶„í• ë˜ë©°, ì´ íŒ¨ì¹˜ ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤:
  
  $$N = \frac{H \cdot W}{P^2}$$

* ê° íŒ¨ì¹˜ $x_i$ëŠ” í”Œë˜íŠ¼ëœ í›„, ê³ ì • ì°¨ì› $D$ì˜ latent spaceë¡œ ì„ë² ë”©ëœë‹¤:
  
  $$z_i = E_{\text{patch}}(x_i) = W_e \cdot \text{Flatten}(x_i) + p_i$$

  - ì—¬ê¸°ì„œ $W_e$ëŠ” í•™ìŠµ ê°€ëŠ¥í•œ ì„ í˜• ì„ë² ë”© ê°€ì¤‘ì¹˜ì´ê³ , $p_i$ëŠ” learnable positional embeddingì´ë‹¤.

* ì „ì²´ íŒ¨ì¹˜ ì¤‘ ë¬´ì‘ìœ„ë¡œ ì„ íƒëœ 75%ëŠ” ì œê±°ë˜ë©°, ë‚˜ë¨¸ì§€ $25\%$ë§Œ ì¸ì½”ë”ì— ì…ë ¥ë¨.

#### ğŸ“Œ Encoder (Representation Learner)

* Transformer ê¸°ë°˜ì˜ ì¸ì½”ë”ëŠ” ê°€ì‹œ íŒ¨ì¹˜ì— ëŒ€í•´ì„œë§Œ ì‘ë™í•˜ë©°, **ë§ˆìŠ¤í¬ëœ ìœ„ì¹˜ì— ëŒ€í•œ ì •ë³´ëŠ” ì…ë ¥ë˜ì§€ ì•ŠìŒ**. ì´ëŠ” ê³„ì‚°ëŸ‰ì„ ì¤„ì´ê³  íš¨ìœ¨ì ì¸ í‘œí˜„ í•™ìŠµì„ ìœ ë„í•¨.
  
* ì¸ì½”ë”ëŠ” $L$ê°œì˜ íŠ¸ëœìŠ¤í¬ë¨¸ ë¸”ë¡ìœ¼ë¡œ êµ¬ì„±ë˜ë©°, ê° ë¸”ë¡ì€ multi-head self-attentionê³¼ MLPë¡œ êµ¬ì„±ëœë‹¤.

* ì¸ì½”ë” ì¶œë ¥ì€ ì ì¬ í‘œí˜„ $z_{\text{vis}} \in \mathbb{R}^{n \times D}$ë¡œ êµ¬ì„±ë˜ë©°, ì´ëŠ” ë””ì½”ë”ì˜ ì…ë ¥ ì¤‘ ì¼ë¶€ê°€ ë¨.

#### ğŸ“Œ Mask Token ë° Sequence Reassembly

* ë””ì½”ë”ì— ì…ë ¥í•˜ê¸° ìœ„í•´, ë§ˆìŠ¤í‚¹ëœ ìœ„ì¹˜ì—ëŠ” **ê³µí†µì ì¸ learnable mask token** $m \in \mathbb{R}^D$ì´ ì‚½ì…ëœë‹¤.
  
* ìœ„ì¹˜ ì •ë³´ ë³´ì¡´ì„ ìœ„í•´ positional encodingì„ ë‹¤ì‹œ ë¶€ì—¬í•˜ì—¬ **full-length sequence**ë¥¼ ì¬êµ¬ì„±í•œë‹¤:

  $$\tilde{z} = \text{Reassemble}(z_{\text{vis}}, m) + p$$

* ì´ ì‹œí€€ìŠ¤ $\tilde{z}$ëŠ” ë””ì½”ë”ì˜ ì…ë ¥ì´ ëœë‹¤.

#### ğŸ“Œ Decoder (Pixel Reconstruction Network)

* ë””ì½”ë”ëŠ” ìƒëŒ€ì ìœ¼ë¡œ ì–•ì€ êµ¬ì¡° (ì˜ˆ: 4-layer transformer)ë¡œ êµ¬ì„±ë˜ë©°, ì „ì²´ íŒ¨ì¹˜ ìœ„ì¹˜ì— ëŒ€í•œ ë³µì› ì‘ì—…ì„ ìˆ˜í–‰í•œë‹¤.

* ë³µì› ëŒ€ìƒì€ **í”½ì…€ ë ˆë²¨ì˜ ì›ë³¸ ì´ë¯¸ì§€ ì¡°ê°**ì´ë©°, ë””ì½”ë”ì˜ ì¶œë ¥ $z_j$ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ë‹¤ì‹œ í”½ì…€ ê³µê°„ìœ¼ë¡œ íˆ¬ì˜ëœë‹¤:

  $$\hat{x}_j = W_d \cdot z_j, \quad \text{for masked patch } j$$

  - ì—¬ê¸°ì„œ $W_d$ëŠ” ì„ í˜• ë³µì› ë§¤í•‘ì— í•´ë‹¹í•œë‹¤.
  - ì†ì‹¤ í•¨ìˆ˜ëŠ” ë§ˆìŠ¤í‚¹ëœ ìœ„ì¹˜ì— ëŒ€í•´ì„œë§Œ ê³„ì‚°ë˜ë©°, ë³´í†µ MSE lossê°€ ì‚¬ìš©ë¨:
  
  $$ \mathcal{L}_{\text{MAE}} = \frac{1}{|\mathcal{M}|} \sum_{j \in \mathcal{M}} \left\| \hat{x}_j - x_j \right\|_2^2 $$

  - $\mathcal{M}$: ë§ˆìŠ¤í‚¹ëœ íŒ¨ì¹˜ ì¸ë±ìŠ¤ ì§‘í•©

---

### ğŸ” ì„¤ê³„ ìƒì˜ í•µì‹¬ ì°¨ë³„ì 

| í•­ëª© | MAE | BERT-style Vision Models |
|------|-----|---------------------------|
| ì…ë ¥ í† í° | visible patch only | masked + visible patches |
| ì¸ì½”ë” ì²˜ë¦¬ | partial input only | full sequence |
| ë§ˆìŠ¤í¬ í† í° | ë””ì½”ë”ì—ì„œë§Œ ì‚¬ìš© | ì¸ì½”ë” ì…ë ¥ì— í¬í•¨ë¨ |
| ë³µì› ëŒ€ìƒ | RGB ì´ë¯¸ì§€ í”½ì…€ | discrete token (e.g., VQ-VAE) |
| ëª©ì  | reconstruction loss ê¸°ë°˜ representation í•™ìŠµ | classification-oriented ë˜ëŠ” ë””ìŠ¤í¬ë¦¬íŠ¸ ë³µì› |

---

### ğŸ“Œ ìš”ì•½

* MAEëŠ” ë³µì› ì¤‘ì‹¬ì˜ ìê¸°ì§€ë„ í•™ìŠµì„ í†µí•´ ê³ ì°¨ì› í‘œí˜„ í•™ìŠµì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” íš¨ìœ¨ì ì¸ ë¹„ì „ í”„ë ˆì„ì›Œí¬
* **ì¸ì½”ë”ëŠ” ê°€ì‹œ íŒ¨ì¹˜ì—ë§Œ ì§‘ì¤‘**í•˜ì—¬ ì—°ì‚°ì„ ì¤„ì´ê³  í•™ìŠµ í‘œí˜„ì˜ íš¨ìœ¨ì„±ì„ ê·¹ëŒ€í™”í•¨
* **ë””ì½”ë”ëŠ” ë§ˆìŠ¤í¬ ìœ„ì¹˜ ì •ë³´ë¥¼ ë³´ê°•í•˜ê³  ì›ë³¸ì„ ë³µì›**í•¨ìœ¼ë¡œì¨ í•™ìŠµ ëª©í‘œë¥¼ ì™„ì„±
* ë†’ì€ ë§ˆìŠ¤í‚¹ ë¹„ìœ¨ê³¼ ë¹„ëŒ€ì¹­ êµ¬ì¡° ì„¤ê³„ë¥¼ í†µí•´ **ì¼ë°˜í™” ì„±ëŠ¥ê³¼ ê³„ì‚° ìì› ê°„ì˜ ê· í˜•**ì„ ì„±ê³µì ìœ¼ë¡œ ë‹¬ì„±



---

## âš–ï¸ ê¸°ì¡´ ëª¨ë¸ê³¼ì˜ ë¹„êµ

| í•­ëª©        | ë³¸ ë…¼ë¬¸ (MAE)                            | BEiT (Bao et al., 2021)                        | SimMIM (Xie et al., 2021)                     |
|-------------|-------------------------------------------|------------------------------------------------|-----------------------------------------------|
| êµ¬ì¡°        | ë¹„ëŒ€ì¹­ encoder-decoder êµ¬ì¡°               | Transformer ê¸°ë°˜ encoder-only                 | encoder-only êµ¬ì¡°                              |
| í•™ìŠµ ë°©ì‹   | 75% ë§ˆìŠ¤í‚¹ í›„ í”½ì…€ ë³µì›                   | discrete token ë³µì› (VQ tokenizer í•„ìš”)        | í”½ì…€ ë³µì›                                     |
| ëª©ì         | íš¨ìœ¨ì  representation í•™ìŠµ ë° ì „ì´        | BERT-style ì‚¬ì „í•™ìŠµ â†’ downstream ì „ì´          | ì´ë¯¸ì§€ ë³µì› ê¸°ë°˜ pretraining                 |

---

## ğŸ“‰ ì‹¤í—˜ ë° ê²°ê³¼

### ğŸ§ª **ë°ì´í„°ì…‹**
- Pretraining: ImageNet-1K (100ë§Œ ì´ë¯¸ì§€, ë¼ë²¨ ë¶ˆì‚¬ìš©)
- Finetuning/Transfer: ImageNet-1K, ADE20K, COCO

### ğŸ¤– **ë¹„êµ ëª¨ë¸**
- ViT-B / ViT-L / ViT-H ëª¨ë¸ ê¸°ë°˜
- Supervised Pretraining, MoCo-v3, BEiT ë“±ê³¼ ë¹„êµ

### ğŸ“Š **ì£¼ìš” ì„±ëŠ¥ ì§€í‘œ ë° ê²°ê³¼**

| ëª¨ë¸             | Top-1 Accuracy (ImageNet-1K) | ê¸°íƒ€ ì„±ëŠ¥ |
|------------------|------------------------------|------------|
| MAE (ViT-B)      | 83.6%                        | -          |
| MAE (ViT-L)      | 85.9%                        | -          |
| MAE (ViT-H)      | **87.8%**                    | -          |
| BEiT (ViT-B)     | 83.2%                        | -          |
| SimMIM (ViT-B)   | 83.8%                        | -          |
| Supervised (ViT-H) | 85.1%                      | -          |

> **í•´ì„**: MAEëŠ” **ImageNet-1Kë§Œìœ¼ë¡œ ì‚¬ì „í•™ìŠµ**í•˜ê³ ë„ ê¸°ì¡´ supervised ë° self-supervised ë°©ë²•ë³´ë‹¤ ë†’ì€ ì •í™•ë„ë¥¼ ê¸°ë¡. íŠ¹íˆ ViT-H ëª¨ë¸ì—ì„œ **SOTA ë‹¬ì„±**.

---

## âœ… ì¥ì  ë° í•œê³„

### âœ… **ì¥ì **
- ë§¤ìš° ë†’ì€ masking ratio (75%)ì—ë„ ì„±ëŠ¥ ìœ ì§€ â†’ **íš¨ìœ¨ì ì¸ pretraining**
- ì¸ì½”ë”ê°€ ì „ì²´ í† í°ì„ ì²˜ë¦¬í•˜ì§€ ì•Šì•„ë„ ë˜ì–´ **ë©”ëª¨ë¦¬ ë° ê³„ì‚° ë¹„ìš© ì ˆê°**
- ì¶”ê°€ì ì¸ í† í¬ë‚˜ì´ì €(VQ ë“±) ì—†ì´ **í”½ì…€ ë³µì›ë§Œìœ¼ë¡œ ê°•í•œ í‘œí˜„ í•™ìŠµ**
- ë‹¤ì–‘í•œ downstream task (ë¶„ë¥˜, ê²€ì¶œ, ë¶„í• )ì—ì„œ ê°•ë ¥í•œ ì„±ëŠ¥

### âš ï¸ **í•œê³„ ë° ê°œì„  ê°€ëŠ¥ì„±**
- ë³µì› ëŒ€ìƒì´ í”½ì…€ì´ê¸° ë•Œë¬¸ì— **semantic richness ë¶€ì¡±**
- í•™ìŠµ ì´ˆê¸°ì—ëŠ” ë³µì› ê³¼ì œê°€ ë„ˆë¬´ ë‹¨ìˆœí•˜ê±°ë‚˜ ë„ˆë¬´ ì–´ë ¤ìš¸ ìˆ˜ ìˆìŒ
- encoderì—ë§Œ í•™ìŠµ ì§‘ì¤‘ â†’ decoderì˜ ì—­í• ì´ ì œí•œì 

---

## ğŸ§  TL;DR â€“ í•œëˆˆì— ì´í•´í•˜ëŠ” MAE

> **MAE (Masked Autoencoders)**ëŠ” ì…ë ¥ ì´ë¯¸ì§€ì˜ 75% ì´ìƒì„ ë¬´ì‘ìœ„ë¡œ ë§ˆìŠ¤í‚¹í•˜ê³ , ë‚¨ì€ ì¼ë¶€ íŒ¨ì¹˜ë§Œì„ ì¸ì½”ë”ë¡œ ì²˜ë¦¬í•˜ì—¬ ì „ì²´ ì´ë¯¸ì§€ë¥¼ ë³µì›í•˜ëŠ” ìê¸°ì§€ë„ í•™ìŠµ ë°©ì‹ì´ë‹¤.  
> **ë¹„ëŒ€ì¹­ì ì¸ encoder-decoder êµ¬ì¡°**ë¥¼ í†µí•´ ê³„ì‚° íš¨ìœ¨ì„±ê³¼ í‘œí˜„ë ¥ í•™ìŠµì„ ë™ì‹œì— ë‹¬ì„±í•˜ë©°, ImageNet-1Kë§Œì„ ì´ìš©í•œ ì‚¬ì „í•™ìŠµìœ¼ë¡œë„ ê¸°ì¡´ SOTAë¥¼ ëŠ¥ê°€í•˜ëŠ” ê²°ê³¼ë¥¼ ë³´ì—¬ì¤€ë‹¤.

| êµ¬ì„± ìš”ì†Œ    | ì„¤ëª… |
|--------------|------|
| ğŸ”§ **í•µì‹¬ ì•„ì´ë””ì–´** | NLPì˜ Masked Language Modelingì„ ì°¨ìš©í•´, ì´ë¯¸ì§€ì—ì„œë„ ë§ˆìŠ¤í‚¹ëœ ë¶€ë¶„ì„ ë³µì›í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì‹œê° í‘œí˜„ í•™ìŠµì„ ìˆ˜í–‰í•¨ |
| ğŸ§  **í•™ìŠµ ì „ëµ** | ì „ì²´ íŒ¨ì¹˜ ì¤‘ 75%ë¥¼ ë¬´ì‘ìœ„ ë§ˆìŠ¤í‚¹ â†’ visible patchë§Œ ì¸ì½”ë”ì— ì…ë ¥ â†’ ë§ˆìŠ¤í¬ í† í°ì„ í¬í•¨í•œ ì „ì²´ ì‹œí€€ìŠ¤ë¥¼ ë””ì½”ë”ê°€ ë³µì› |
| âš™ï¸ **ëª¨ë¸ êµ¬ì¡°** | ê°€ë²¼ìš´ ë””ì½”ë”ì™€ ê¹Šì€ ì¸ì½”ë”ë¥¼ ë¶„ë¦¬í•œ **ë¹„ëŒ€ì¹­ êµ¬ì¡°** ì±„íƒ â†’ ì—°ì‚°ëŸ‰ 3ë°° ì´ìƒ ê°ì†Œ, í•™ìŠµ ì‹œê°„ ë‹¨ì¶• |
| ğŸ¯ **ë³µì› ê³¼ì œ** | ë³µì› ëŒ€ìƒì€ discrete tokenì´ ì•„ë‹Œ **í”½ì…€ ë ˆë²¨ ì´ë¯¸ì§€** â†’ ì¶”ê°€ í† í¬ë‚˜ì´ì € ì—†ì´ ê°„ê²°í•˜ê³  ì§ê´€ì ì¸ í•™ìŠµì´ ê°€ëŠ¥ |
| ğŸ“ˆ **ì „ì´ ì„±ëŠ¥** | ì‚¬ì „í•™ìŠµëœ MAE + ViTëŠ” ë¶„ë¥˜, ê°ì²´ íƒì§€, ì„¸ê·¸ë©˜í…Œì´ì…˜ ë“± ë‹¤ì–‘í•œ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì‘ì—…ì—ì„œ ê¸°ì¡´ supervised ë° self-supervised ë°©ë²•ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì„ |
| ğŸ“Š **ì„±ëŠ¥ ì§€í‘œ** | ViT-H ëª¨ë¸ ê¸°ì¤€, ImageNet-1K Top-1 accuracy: **87.8%** (ì‚¬ì „í•™ìŠµì— ImageNet-1Kë§Œ ì‚¬ìš©) |
| ğŸ§© **ë¹„êµ ìš°ìœ„** | BEiT, SimMIM ë“± ê¸°ì¡´ ë§ˆìŠ¤í‚¹ ê¸°ë°˜ ë°©ë²•ë“¤ê³¼ ë¹„êµí•´ **ì¶”ê°€ í† í¬ë‚˜ì´ì € í•„ìš” ì—†ìŒ**, **ë” ë†’ì€ masking ratio ì§€ì›**, **ë” ë¹ ë¥´ê³  ë‹¨ìˆœí•œ êµ¬ì¡°** |

> âœ… MAEëŠ” ë‹¨ìˆœí•œ ì´ë¯¸ì§€ ë³µì› ê³¼ì œë¥¼ í†µí•´ íš¨ìœ¨ì ì´ê³  í™•ì¥ì„± ìˆëŠ” ë¹„ì „ í‘œí˜„ í•™ìŠµì„ ê°€ëŠ¥í•˜ê²Œ í•˜ë©°, BERT ì´í›„ ê°€ì¥ ì„±ê³µì ì¸ self-supervised í”„ë ˆì„ì›Œí¬ ì¤‘ í•˜ë‚˜ë¡œ ìë¦¬ ì¡ìŒ.

---

## ğŸ”— ì°¸ê³  ë§í¬ (References)

* [ğŸ“„ arXiv ë…¼ë¬¸](https://arxiv.org/abs/2111.06377)
* [ğŸ’» GitHub](https://github.com/facebookresearch/mae)
* [ğŸ“ˆ Papers with Code](https://paperswithcode.com/paper/masked-autoencoders-are-scalable-vision)


## ë‹¤ìŒ ë…¼ë¬¸: SegFormer
