# ğŸ“˜ Neural Machine Translation by Jointly Learning to Align and Translate

## 1. ê°œìš” (Overview)

* **ì œëª©**: Neural Machine Translation by Jointly Learning to Align and Translate
* **ì €ì**: Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio
* **ì†Œì†**: UniversitÃ© de MontrÃ©al
* **í•™íšŒ**: ICLR 2015
* **ë§í¬**: [arXiv:1409.0473](https://arxiv.org/abs/1409.0473)

> ìµœì´ˆë¡œ "Attention" ë©”ì»¤ë‹ˆì¦˜ì„ ì œì•ˆí•œ ë…¼ë¬¸ìœ¼ë¡œ, ê¸°ì¡´ RNN ê¸°ë°˜ ë²ˆì—­ê¸°ì˜ ì„±ëŠ¥ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´ ë””ì½”ë”ê°€ ì¸ì½”ë”ì˜ ì „ì²´ ì¶œë ¥ì„ **ê°€ë³€ì ìœ¼ë¡œ ì°¸ì¡°**í•  ìˆ˜ ìˆë„ë¡ í•œ ë°©ì‹ì´ë‹¤.

---

## 2. ë¬¸ì œ ì •ì˜ (Problem Formulation)

**ë¬¸ì œ ë° ê¸°ì¡´ í•œê³„**:

* ê¸°ì¡´ RNN ê¸°ë°˜ Encoder-DecoderëŠ” **ê³ ì •ëœ context ë²¡í„°**ë§Œìœ¼ë¡œ ì „ì²´ ì…ë ¥ ì‹œí€€ìŠ¤ë¥¼ ìš”ì•½
* ê¸´ ë¬¸ì¥ì¼ìˆ˜ë¡ ì •ë³´ê°€ ì†ì‹¤ë˜ì–´ ë²ˆì—­ ì„±ëŠ¥ì´ ë–¨ì–´ì§
* ëª¨ë“  ì •ë³´ë¥¼ í•˜ë‚˜ì˜ ë²¡í„°ë¡œ ì••ì¶•í•˜ë©´ ë””ì½”ë”ê°€ ëª¨ë“  ì˜ë¯¸ ì •ë³´ë¥¼ ë°˜ì˜í•˜ê¸° ì–´ë ¤ì›€

**ì œì•ˆ ë°©ì‹**:

* ë””ì½”ë”ê°€ ì¸ì½”ë”ì˜ **ëª¨ë“  hidden stateë¥¼ ë™ì ìœ¼ë¡œ ê°€ì¤‘í•©**í•˜ì—¬ context ë²¡í„° ìƒì„±
* ê° ë””ì½”ë”© ë‹¨ê³„ë§ˆë‹¤ "ì–´ë””ì— ì§‘ì¤‘í• ì§€" ê²°ì •í•˜ëŠ” **soft attention mechanism** ë„ì…

> â€» Attentionì€ alignmentë¥¼ í•™ìŠµí•˜ë©°, ë²ˆì—­ ì¤‘ ì–´ë–¤ ì…ë ¥ ë‹¨ì–´ì— ì§‘ì¤‘í• ì§€ë¥¼ ëª¨ë¸ì´ ìë™ìœ¼ë¡œ ì„ íƒí•¨

---

## 3. ëª¨ë¸ êµ¬ì¡° (Architecture)

### ì „ì²´ êµ¬ì¡°

* ì „í†µì ì¸ RNN Encoder-Decoder êµ¬ì¡°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ë˜, **context vectorë¥¼ ê³ ì •í•˜ì§€ ì•Šê³  ë™ì ìœ¼ë¡œ ìƒì„±**
* ì¸ì½”ë”ëŠ” ì–‘ë°©í–¥ RNN (Bi-RNN)ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ê° ì…ë ¥ ìœ„ì¹˜ \$i\$ì—ì„œ hidden state \$h\_i\$ë¥¼ ìƒì„±
* ë””ì½”ë”ëŠ” ì´ì „ ìƒíƒœ \$s\_{t-1}\$, ì´ì „ ì¶œë ¥ \$y\_{t-1}\$, ë™ì  context \$c\_t\$ë¥¼ ì´ìš©í•˜ì—¬ ì¶œë ¥ ìƒì„±

---

### ğŸ’  Attention Mechanism (Additive Attention)

#### ğŸ“Œ ì‘ë™ ê³¼ì • (ë‹¨ê³„ë³„ ì„¤ëª…)

1. **Score ê³„ì‚°**: ë””ì½”ë”ì˜ í˜„ì¬ ìƒíƒœ \$s\_{t-1}\$ì™€ ì¸ì½”ë”ì˜ ê° hidden state \$h\_i\$ë¥¼ ë¹„êµí•´ score \$e\_{ti}\$ë¥¼ ê³„ì‚°

$$
e_{ti} = v_a^T \tanh(W_a s_{t-1} + U_a h_i)
$$

2. **Softmax ì •ê·œí™”**: ê° scoreì— ëŒ€í•´ softmaxë¥¼ ì ìš©í•˜ì—¬ ê°€ì¤‘ì¹˜ \$\alpha\_{ti}\$ë¥¼ ê³„ì‚°

$$
\alpha_{ti} = \frac{\exp(e_{ti})}{\sum_j \exp(e_{tj})}
$$

3. **Context Vector ê³„ì‚°**: ê°€ì¤‘ì¹˜ \$\alpha\_{ti}\$ì™€ hidden state \$h\_i\$ì˜ ê°€ì¤‘í•©ìœ¼ë¡œ context vector \$c\_t\$ ê³„ì‚°

$$
c_t = \sum_i \alpha_{ti} h_i
$$

4. **ë””ì½”ë” ì¶œë ¥**: \$(c\_t, y\_{t-1})\$ë¥¼ ë””ì½”ë” RNNì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ ìƒˆë¡œìš´ hidden state \$s\_t\$ë¥¼ ìƒì„±í•˜ê³ , ìµœì¢… ì¶œë ¥ ìƒì„±

#### ğŸ“Œ ê°œë…ì ìœ¼ë¡œ ë³´ë©´:

* Attentionì€ **query** (ë””ì½”ë” ìƒíƒœ), **key/value** (ì¸ì½”ë” ìƒíƒœ)ì˜ ìŒìœ¼ë¡œ êµ¬ì„±ë¨
* ë””ì½”ë”ëŠ” ê° ì‹œì ë§ˆë‹¤ â€œì…ë ¥ì˜ ì–´ë–¤ ìœ„ì¹˜ì— ì£¼ëª©í• ì§€â€ë¥¼ í•™ìŠµí•¨
* ì´ëŠ” ê¸°ê³„ ë²ˆì—­ ê³¼ì •ì—ì„œì˜ **ë‹¨ì–´ alignment** ë¬¸ì œë¥¼ ìœ ì—°í•˜ê²Œ í•´ê²°í•¨

#### ğŸ“Œ ì¶”ê°€ ê°œë…: Alignment Modelì˜ ì—­í• 

* alignment í•¨ìˆ˜ \$a(s\_{t-1}, h\_i)\$ëŠ” ë””ì½”ë” ìƒíƒœì™€ ì¸ì½”ë” ìƒíƒœ ê°„ì˜ \*\*ìœ ì‚¬ë„(score)\*\*ë¥¼ ì¸¡ì •í•¨
* additive attentionì€ \$s\_{t-1}\$ê³¼ \$h\_i\$ë¥¼ **concat â†’ tanh â†’ ì„ í˜• ë³€í™˜**í•˜ì—¬ score ìƒì„±
* ì´ êµ¬ì¡°ëŠ” **ë¹„ì„ í˜•ì„±ê³¼ í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°**ë¥¼ í†µí•´ soft attention weightë¥¼ ìœ ë„í•¨

#### ğŸ“Œ ì‹œê°ì  í•´ì„

* Attention Weightë¥¼ ì‹œê°í™”í•˜ë©´ **ì–´ë–¤ ì…ë ¥ ë‹¨ì–´ê°€ ì–´ë–¤ ì¶œë ¥ ë‹¨ì–´ì™€ ì—°ê²°ë˜ëŠ”ì§€**ë¥¼ í™•ì¸ ê°€ëŠ¥
* ì´ëŠ” ëª¨ë¸ì˜ í•´ì„ ê°€ëŠ¥ì„± (interpretability)ì„ ë†’ì—¬ì£¼ëŠ” ì¤‘ìš”í•œ ì¥ì 
* ë²ˆì—­ë¿ ì•„ë‹ˆë¼ ë‹¤ë¥¸ ì‹œí€€ìŠ¤ í•™ìŠµ ë¬¸ì œì—ì„œë„ ì–´ë””ë¥¼ ì°¸ì¡°í–ˆëŠ”ì§€ ì•Œ ìˆ˜ ìˆìŒ

#### ğŸ“Œ í•µì‹¬ ì°¨ë³„ì 

* ê¸°ì¡´ ëª¨ë¸ì€ contextë¥¼ ì••ì¶•í•´ ì†ì‹¤ì„ ìœ ë°œí–ˆìœ¼ë‚˜, attentionì€ **ëª¨ë“  ì…ë ¥ì— softí•˜ê²Œ ì ‘ê·¼ ê°€ëŠ¥**
* ìœ„ì¹˜ì— ë”°ë¥¸ alignment ê°€ì¤‘ì¹˜ë¥¼ ì§ì ‘ í•™ìŠµí•¨ìœ¼ë¡œì¨ **ë” ì •ë°€í•œ ì •ë³´ ì„ íƒ**ì´ ê°€ëŠ¥

---

## ğŸ§  TL;DR â€“ í•œëˆˆì— ìš”ì•½

> ë””ì½”ë”ê°€ ì¸ì½”ë”ì˜ hidden state ì „ì²´ì— ëŒ€í•´ ê°€ì¤‘ì¹˜ë¥¼ í•™ìŠµí•´ ë™ì ìœ¼ë¡œ ì§‘ì¤‘í•˜ëŠ” **Attention mechanism**ì„ ì²˜ìŒ ë„ì….
> ì´ ë°©ì‹ì€ RNNì˜ ì •ë³´ ì••ì¶• í•œê³„ë¥¼ ê·¹ë³µí•˜ê³ , ì´í›„ Transformerë¡œ ì´ì–´ì§€ëŠ” Attention ì‹œëŒ€ì˜ ë¬¸ì„ ì—´ì—ˆë‹¤.

| êµ¬ì„± ìš”ì†Œ     | ì„¤ëª…                                 |
| --------- | ---------------------------------- |
| Encoder   | Bi-RNN (ì…ë ¥ ì‹œí€€ìŠ¤ ì¸ì½”ë”©)                |
| Attention | Additive attention (score = FFN)   |
| Decoder   | RNN (context + previous output ê¸°ë°˜) |
| í•µì‹¬ ê¸°ì—¬     | soft alignment í•™ìŠµ ë°©ì‹ ì œì•ˆ            |

---

## ğŸ”— ì°¸ê³  ë§í¬ (References)

* [ğŸ“„ arXiv ë…¼ë¬¸](https://arxiv.org/abs/1409.0473)
* [ğŸ’» GitHub êµ¬í˜„ (ì˜ˆì‹œ)](https://github.com/keon/seq2seq)
* [ğŸ“ˆ Papers with Code](https://paperswithcode.com/paper/neural-machine-translation-by-jointly)

## ë‹¤ìŒ ë…¼ë¬¸: Attention Is All You Need
