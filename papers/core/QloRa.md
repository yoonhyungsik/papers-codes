# ğŸ“˜ QLoRA: Efficient Finetuning of Quantized LLMs

## 1. ê°œìš” (Overview)

- **ì œëª©**: QLoRA: Efficient Finetuning of Quantized LLMs  
- **ì €ì**: Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer  
- **ì†Œì†**: University of Washington, Meta AI  
- **í•™íšŒ**: ACL 2023 (Association for Computational Linguistics)  
- **ë§í¬**:  
  - [arXiv](https://arxiv.org/abs/2305.14314)  
  - [GitHub](https://github.com/artidoro/qlora)  
  - [Papers with Code](https://paperswithcode.com/paper/qlora-efficient-finetuning-of-quantized)

> ì´ ë…¼ë¬¸ì€ **LoRA ê¸°ë°˜ íŒŒì¸íŠœë‹ ê¸°ë²•ì„ ë”ìš± ì €ë ´í•˜ê²Œ ë§Œë“¤ê¸° ìœ„í•œ í™•ì¥ ë°©ì‹**ìœ¼ë¡œ,  
> ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì„ **4-bitë¡œ ì–‘ìí™”(quantization)** í•œ í›„ì—ë„ **ì •ë°€ë„ë¥¼ ìœ ì§€í•œ ì±„ ë¯¸ì„¸ì¡°ì •(finetuning)** í•  ìˆ˜ ìˆëŠ” í˜ì‹ ì ì¸ ë°©ë²•ì„ ì œì•ˆí•œë‹¤.  
> íŠ¹íˆ, QLoRAëŠ” **ë‹¨ì¼ GPUì—ì„œë„ LLaMA-65B ê°™ì€ ì´ˆëŒ€í˜• ëª¨ë¸ì„ fine-tune ê°€ëŠ¥í•˜ê²Œ ë§Œë“  ìµœì´ˆì˜ ë°©ì‹** ì¤‘ í•˜ë‚˜ë¡œ,  
> ì—°êµ¬/ì‚°ì—… ì–‘ìª½ì—ì„œ **PEFT + ì–‘ìí™” ê¸°ë°˜ í•™ìŠµì˜ ì‹¤ìš©ì„±**ì„ ì…ì¦í–ˆë‹¤.

---

## 2. ë¬¸ì œ ì •ì˜ (Problem Formulation)

---

### â— ë¬¸ì œ ë° ê¸°ì¡´ í•œê³„

- ê¸°ì¡´ LoRAëŠ” **íš¨ìœ¨ì ì¸ íŒŒì¸íŠœë‹**ì„ ê°€ëŠ¥í•˜ê²Œ í–ˆì§€ë§Œ,  
  ì—¬ì „íˆ **full-precision (16bit ë˜ëŠ” 32bit) ëª¨ë¸ì„ ë©”ëª¨ë¦¬ì— ì˜¬ë ¤ì•¼ë§Œ** ì‘ë™í•œë‹¤.

- GPT, LLaMA ë“± **ìˆ˜ì‹­ì–µ íŒŒë¼ë¯¸í„° ëª¨ë¸ì„ ë¯¸ì„¸ì¡°ì •í•˜ë ¤ë©´**  
  **A100ê³¼ ê°™ì€ ê³ ì„±ëŠ¥ GPU ìˆ˜ì‹­ ì¥**ì´ í•„ìš”í•˜ì—¬ ì¼ë°˜ ì—°êµ¬ í™˜ê²½ì—ì„œëŠ” ì‹¤í–‰ì´ ì–´ë µë‹¤.

- ê¸°ì¡´ 8bit/4bit ì–‘ìí™”ëœ ëª¨ë¸ì€ **ì •ë°€ë„ ì†ì‹¤ë¡œ í•™ìŠµì´ ì–´ë ¤ì› ê³ **,  
  íŒŒì¸íŠœë‹ ì‹œ **ì˜¤ë¥˜ ëˆ„ì  ë° ì„±ëŠ¥ ì €í•˜**ê°€ ë¹ˆë²ˆí•˜ê²Œ ë°œìƒí•˜ì˜€ë‹¤.

---

### ğŸ’¡ ì œì•ˆ ë°©ì‹: QLoRA

- QLoRAëŠ” **ëª¨ë¸ ì „ì²´ë¥¼ 4bitë¡œ ì–‘ìí™”**í•˜ì—¬ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ê¸‰ê°ì‹œí‚¤ë©´ì„œë„,  
  ê¸°ì¡´ LoRAì™€ ìœ ì‚¬í•œ ë°©ì‹ìœ¼ë¡œ **ì •í™•ë„ ì†ì‹¤ ì—†ì´ íŒŒì¸íŠœë‹ ê°€ëŠ¥**í•˜ê²Œ í•œë‹¤.

- í•µì‹¬ êµ¬ì„±:
  - **4-bit Quantization (NF4)**: ì •ë°€ë„ì™€ ì••ì¶•ë¥  ì‚¬ì´ì—ì„œ ìµœì  ì„±ëŠ¥ ì œê³µ
  - **Double Quantization**: weight ìì²´ë¥¼ ì–‘ìí™”í•œ í›„, ì–‘ìí™” íŒŒë¼ë¯¸í„°ë„ ë‹¤ì‹œ ì–‘ìí™”
  - **LoRA ì ìš©**: ì–‘ìí™”ëœ ëª¨ë¸ì— ì €ë­í¬ í•™ìŠµ ëª¨ë“ˆë§Œ ì¶”ê°€ë¡œ í•™ìŠµ

> ê²°ê³¼ì ìœ¼ë¡œ QLoRAëŠ” **65B LLMì¡°ì°¨ ë‹¨ì¼ A100 GPUì—ì„œ í•™ìŠµ ê°€ëŠ¥**í•˜ê²Œ ë§Œë“  ê²½ëŸ‰í™” PEFT ì „ëµì´ë‹¤.

---

### ğŸ“Œ í•µì‹¬ ê°œë… ì •ì˜

- **Quantization (ì–‘ìí™”)**: ëª¨ë¸ íŒŒë¼ë¯¸í„°ì˜ í‘œí˜„ ì •ë°€ë„ë¥¼ ì¤„ì—¬ (ì˜ˆ: float16 â†’ int4),  
  ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ê³¼ ì—°ì‚°ëŸ‰ì„ ê°ì†Œì‹œí‚¤ëŠ” ê¸°ìˆ 

- **NF4 (Normalized Float 4-bit)**: QLoRAì—ì„œ ì‚¬ìš©ëœ **ì •ë³´ ì†ì‹¤ì„ ìµœì†Œí™”í•œ 4bit ë¶€ë™ì†Œìˆ˜ í‘œí˜„ ë°©ì‹**

- **Double Quantization**: weightë¥¼ ì–‘ìí™”í•œ í›„,  
  ì–‘ìí™” ì‹œ ì‚¬ìš©ëœ **scaling factor** ìì²´ë„ ë‹¤ì‹œ ì–‘ìí™”í•´ ì¶”ê°€ ì••ì¶•ì„ ë‹¬ì„±

- **LoRA**: ê¸°ì¡´ íŒŒë¼ë¯¸í„°ëŠ” ê³ ì •í•˜ê³ , ì €ë­í¬ í–‰ë ¬(Î”W = BA)ë§Œ í•™ìŠµí•˜ëŠ” ê²½ëŸ‰ íŒŒì¸íŠœë‹ ë°©ë²•

---

## 3. ëª¨ë¸ êµ¬ì¡° (Architecture)

---

### ğŸ§± ì „ì²´ êµ¬ì¡°

![QLoRA ëª¨ë¸ êµ¬ì¡°](../images/qlora..png)

QLoRAëŠ” ë‹¤ìŒ ì„¸ ê°€ì§€ í•µì‹¬ ëª¨ë“ˆì´ ê²°í•©ëœ êµ¬ì¡°ì…ë‹ˆë‹¤:

1. **4-bit ì–‘ìí™”ëœ ì–¸ì–´ëª¨ë¸** (ì •í™•ë„ ì†ì‹¤ ìµœì†Œí™”)
2. **LoRA ëª¨ë“ˆ** (ì €ë­í¬ íŒŒë¼ë¯¸í„° í•™ìŠµ)
3. **Double Quantization** (ë©”ëª¨ë¦¬ íš¨ìœ¨ í–¥ìƒ)

ëª¨ë¸ì˜ ì „ì²´ íë¦„ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:
```
[Input Text Tokens]
â†“
[Tokenizer â†’ Embedding]
â†“
[4-bit Quantized Transformer (NF4)]
â”œâ”€â”€ (LoRA module ì‚½ì…ëœ Linear Layer)
â†“
[Output Hidden States]
â†“
[Loss ê³„ì‚° + GradientëŠ” LoRAì—ë§Œ ì ìš©]
```

> ğŸ”§ ê¸°ì¡´ LoRAëŠ” full precision weight ìœ„ì— LoRAë¥¼ ì–¹ì—ˆë‹¤ë©´,  
> **QLoRAëŠ” quantized weight ìœ„ì— LoRAë¥¼ ì ìš©**í•œë‹¤ëŠ” ì ì´ êµ¬ì¡°ì  í•µì‹¬ ì°¨ì´ì…ë‹ˆë‹¤.

> ğŸ“Œ LoRA + ì–‘ìí™”ê°€ ì™„ì „íˆ ë¶„ë¦¬ë˜ì§€ ì•Šê³  **í•˜ë‚˜ì˜ í›ˆë ¨ ê°€ëŠ¥í•œ ê²½ëŸ‰ ëª¨ë“ˆë¡œ í†µí•©**ë˜ì–´ ì‘ë™í•©ë‹ˆë‹¤.

---

### ğŸ’  í•µì‹¬ ëª¨ë“ˆ ë˜ëŠ” êµ¬ì„± ìš”ì†Œ

#### ğŸ“Œ 1. 4-bit Quantized Language Model (NF4)

- **ì‘ë™ ë°©ì‹**: ëª¨ë“  weightë¥¼ **NF4 (Normalized Float 4-bit)** í˜•ì‹ìœ¼ë¡œ ì–‘ìí™”
- **NF4 íŠ¹ì§•**:
  - ê· ë“± quantizationì´ ì•„ë‹Œ **ë¶„í¬ ê¸°ë°˜ ë¹„ì„ í˜• ìŠ¤ì¼€ì¼ë§** ì‚¬ìš©
  - ê¸°ì¡´ int4ë³´ë‹¤ í›¨ì”¬ ë†’ì€ í‘œí˜„ë ¥ í™•ë³´
  - í•™ìŠµ ì‹œ ì •ë°€ë„ ì†ì‹¤ì„ ìµœì†Œí™”í•¨

- **êµ¬ì¡°ì  ê°œë…**:

  ê¸°ì¡´ float16 weight $W$ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ë³€í™˜:\
  $W \rightarrow \hat{W} = \text{Dequant}(Q(W), s)$\
  ì—¬ê¸°ì„œ $Q(W)$ëŠ” NF4ë¡œ ì–‘ìí™”ëœ ë²¡í„°, $s$ëŠ” scale factor

- **LoRAì™€ì˜ ì°¨ì´ì **:  
  LoRAëŠ” **full-precision ëª¨ë¸ ìœ„ì—ì„œë§Œ ì‘ë™ ê°€ëŠ¥**í–ˆì§€ë§Œ,  
  QLoRAëŠ” **4-bit weight ìœ„ì—ì„œ LoRA ì ìš©ì„ ê°€ëŠ¥**í•˜ê²Œ í•¨

---

#### ğŸ“Œ 2. LoRA Module

- ê¸°ì¡´ê³¼ ë™ì¼í•˜ê²Œ, íŠ¹ì • Linear ê³„ì¸µì— ëŒ€í•´\
  $W' = \hat{W} + \Delta W = \hat{W} + BA$\
  ì˜ í˜•íƒœë¡œ íŒŒë¼ë¯¸í„°ë¥¼ í•™ìŠµ

- í•™ìŠµ ëŒ€ìƒì€ $A, B$ ë‘ ì €ë­í¬ í–‰ë ¬ë¿ì´ë©°, $\hat{W}$ëŠ” 4-bit ì–‘ìí™”ëœ ì±„ë¡œ **ë™ê²°(frozen)**ë¨

- **ì°¨ì´ì  ìš”ì•½**:
  | í•­ëª©           | LoRA                        | QLoRA                                 |
  |----------------|-----------------------------|----------------------------------------|
  | ê¸°ë°˜ ëª¨ë¸      | Full-precision Transformer  | 4-bit Quantized Transformer (NF4)     |
  | ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰   | í¼                          | ëŒ€í­ ì ˆê° (ìµœëŒ€ 3Ã— ì´ìƒ)              |
  | ì—°ì‚° ë°©ì‹      | float16 or float32          | quantized weight + LoRA float         |

---

#### ğŸ“Œ 3. Double Quantization

- **ì‘ë™ ë°©ì‹**: ì–‘ìí™” weightë¥¼ ì €ì¥í•  ë•Œ ì‚¬ìš©ë˜ëŠ” **scale factor ìì²´ë„ ë‹¤ì‹œ ì–‘ìí™”**
- ì´ëŠ” weightì˜ ì •ë³´ë¥¼ ì €ì¥í•˜ëŠ” ë° í•„ìš”í•œ ë©”ëª¨ë¦¬ë¥¼ **ì¶”ê°€ë¡œ 2Ã— ê°ì†Œ**ì‹œí‚´

- **ê°œë… ìš”ì•½**:
  1. ì›ë³¸ weight $W$ â†’ 4-bit NF4 ë²¡í„° $Q(W)$ + scale factor $s$
  2. scale factor $s$ë„ ì–‘ìí™”ë˜ì–´ ì €ì¥: $Q(s)$

- **ì´ì **:
  - í•™ìŠµì—ëŠ” ì˜í–¥ ì—†ìŒ (scale factorëŠ” ì €ì¥ìš©)
  - ëª¨ë¸ checkpoint ìš©ëŸ‰ì´ í›¨ì”¬ ì‘ì•„ì§

---

### ğŸ” LoRA vs QLoRA êµ¬ì¡°ì  ë¹„êµ ìš”ì•½

| í•­ëª©                        | LoRA                          | QLoRA                                      |
|-----------------------------|-------------------------------|---------------------------------------------|
| ëª¨ë¸ ê¸°ë°˜                   | Full-precision (FP16/FP32)     | 4-bit Quantized (NF4)                      |
| ì—°ì‚°/ë©”ëª¨ë¦¬ íš¨ìœ¨             | ì œí•œì                         | ë§¤ìš° íš¨ìœ¨ì  (ë‹¨ì¼ GPUë¡œ 65B ëª¨ë¸ í•™ìŠµ ê°€ëŠ¥) |
| í•™ìŠµ íŒŒë¼ë¯¸í„°               | A, B                          | A, B (ë™ì¼)                                 |
| íŒŒì¸íŠœë‹ ê°€ëŠ¥ì„±             | ê³ ì„±ëŠ¥ GPU í•„ìš”                | ë‹¨ì¼ A100ë„ ê°€ëŠ¥                            |
| ì‘ìš© í™•ì¥ì„±                | NLP ì¤‘ì‹¬                      | NLP + ëŒ€ê·œëª¨ LLM (LLaMA-13B, 65B ë“±)        |

---

### ğŸ”„ ìš”ì•½

QLoRAëŠ” ê¸°ì¡´ LoRAì™€ ë™ì¼í•œ ì €ë­í¬ íŒŒë¼ë¯¸í„° í•™ìŠµ ì „ëµì„ ìœ ì§€í•˜ë©´ì„œë„,  
**ì „ì²´ ëª¨ë¸ì„ 4-bitë¡œ ì–‘ìí™”í•˜ì—¬ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„ íšê¸°ì ìœ¼ë¡œ í–¥ìƒ**ì‹œí‚¨ êµ¬ì¡°ë¥¼ ê°€ì§„ë‹¤.  
LoRAê°€ full-precision weightë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‘ë™í–ˆë‹¤ë©´, QLoRAëŠ” **ì–‘ìí™” weight ìœ„ì—ì„œ ì§ì ‘ ì‘ë™**í•  ìˆ˜ ìˆê²Œ ë§Œë“œëŠ” ê²ƒì´ í•µì‹¬ ì°¨ë³„ì ì´ë‹¤.

---
## âœ… ì¥ì  ë° í•œê³„

---

## ğŸŒŸ **ì¥ì **:

- âœ… **ë©”ëª¨ë¦¬ íš¨ìœ¨ ê·¹ëŒ€í™”**  
  â†’ ê¸°ì¡´ LoRAë³´ë‹¤ í›¨ì”¬ ë” ì‘ì€ ë©”ëª¨ë¦¬ë¡œ í•™ìŠµ ê°€ëŠ¥  
  â†’ ì˜ˆ: LLaMA 65B ëª¨ë¸ë„ **ë‹¨ì¼ 48GB GPU(A100)** ì—ì„œ í•™ìŠµ ê°€ëŠ¥

- âœ… **4bit Quantizationì—ë„ ì„±ëŠ¥ ìœ ì§€**  
  â†’ NF4 ì–‘ìí™” ë•ë¶„ì— 4bitì—ì„œë„ Full Precisionê³¼ ê±°ì˜ ë™ì¼í•œ ì •í™•ë„ ë‹¬ì„±  
  â†’ ì˜ˆ: GPT-3 175B ëª¨ë¸ê³¼ ìœ ì‚¬í•œ ì„±ëŠ¥ì„ Vicuna 13Bë¡œ ë³µì œ ê°€ëŠ¥

- âœ… **LoRA í˜¸í™˜ì„± ìœ ì§€**  
  â†’ ê¸°ì¡´ LoRA ë°©ì‹ ê·¸ëŒ€ë¡œ ì‚¬ìš© ê°€ëŠ¥ (LoRAì˜ ëª¨ë“  ì´ì  ìœ ì§€)  
  â†’ ê¸°ì¡´ í•™ìŠµ ì¸í”„ë¼ ë° ì½”ë“œì™€ í˜¸í™˜ë¨

- âœ… **Checkpoints ì €ì¥ ìš©ëŸ‰ ê°ì†Œ**  
  â†’ Double Quantization ë•ë¶„ì— ëª¨ë¸ ì €ì¥ ê³µê°„ë„ ì¤„ì–´ë“¦  
  â†’ ì–‘ìí™”ëœ weightì™€ ìŠ¤ì¼€ì¼ íŒŒë¼ë¯¸í„°ë„ ëª¨ë‘ ì••ì¶•ë¨

- âœ… **ì‹¤ì œ ì˜¤í”ˆì†ŒìŠ¤ ì»¤ë®¤ë‹ˆí‹°ì—ì„œ ê´‘ë²”ìœ„í•˜ê²Œ ì‚¬ìš© ì¤‘**  
  â†’ Vicuna, Guanaco ë“± ìœ ëª… ëª¨ë¸ì´ ëª¨ë‘ QLoRAë¡œ íŒŒì¸íŠœë‹ë¨

---

## âš ï¸ **í•œê³„ ë° ê°œì„  ê°€ëŠ¥ì„±**:

- âŒ **4bit ì—°ì‚°ì€ ì¼ë¶€ GPU/í”„ë ˆì„ì›Œí¬ì—ì„œ ë¯¸ì§€ì›**  
  â†’ NF4ëŠ” **BitsAndBytes**ì™€ ê°™ì€ íŠ¹ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ì— ì˜ì¡´  
  â†’ CPU ë˜ëŠ” ì¼ë¶€ GPUì—ì„œëŠ” ì†ë„ ì €í•˜ë‚˜ ì§€ì› ë¶€ì¡± ê°€ëŠ¥ì„±

- âŒ **LoRA ìœ„ì¹˜ ì„ ì •ì€ ì—¬ì „íˆ ìˆ˜ë™**  
  â†’ ì–´ë–¤ Layerì— LoRAë¥¼ ì‚½ì…í• ì§€ ê²°ì •í•˜ëŠ” ê²ƒì€ ì—¬ì „íˆ ì‹¤í—˜ì ìœ¼ë¡œ ì„¤ì •í•´ì•¼ í•¨

- âŒ **ì–‘ìí™”ëœ weightëŠ” ë¯¸ì„¸ ì¡°ì • ë¶ˆê°€ëŠ¥**  
  â†’ ì˜¤ì§ LoRA ëª¨ë“ˆë§Œ í•™ìŠµ ê°€ëŠ¥í•˜ë¯€ë¡œ ì „ì²´ weight ì¡°ì •ì€ ë¶ˆê°€

- âŒ **ë©€í‹°ëª¨ë‹¬/ë¹„ì „ ë“± í™•ì¥ì„±ì€ ê²€ì¦ ë¶€ì¡±**  
  â†’ NLPì—ì„œëŠ” ê²€ì¦ë˜ì—ˆì§€ë§Œ, ë¹„ì „/ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ì—ëŠ” ì ìš© ì—°êµ¬ê°€ ì ìŒ

---

> ğŸ“Œ **ìš”ì•½**: QLoRAëŠ” LoRAì˜ íŒŒë¼ë¯¸í„° íš¨ìœ¨ì„±ê³¼ 4bit ì–‘ìí™”ì˜ ë©”ëª¨ë¦¬ ì ˆê°ì„ ê²°í•©í•œ  
> ë§¤ìš° ì‹¤ìš©ì ì¸ PEFT ê¸°ë²•ì´ì§€ë§Œ, ì—¬ì „íˆ ì¼ë¶€ ì œì•½ì„ ì•ˆê³  ìˆìœ¼ë©° í›„ì† ì—°êµ¬ë¡œ í™•ì¥ë˜ê³  ìˆìŒ.

---

## ğŸ§  TL;DR â€“ í•œëˆˆì— ìš”ì•½

> **QLoRAëŠ” ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì˜ ì „ì²´ weightë¥¼ 4bitë¡œ ì–‘ìí™”í•˜ë©´ì„œë„,  
> LoRAë¥¼ í†µí•´ ì •ë°€í•œ íŒŒì¸íŠœë‹ì„ ê°€ëŠ¥í•˜ê²Œ ë§Œë“  ì´ˆê²½ëŸ‰ PEFT ê¸°ë²•ì´ë‹¤.**  
> ì´ë¥¼ í†µí•´ ë‹¨ì¼ GPUì—ì„œë„ LLaMA-65Bì™€ ê°™ì€ ì´ˆê±°ëŒ€ ëª¨ë¸ì˜ ë¯¸ì„¸ì¡°ì •ì´ ê°€ëŠ¥í•´ì¡Œìœ¼ë©°,  
> ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ê³¼ ì„±ëŠ¥ ëª¨ë‘ë¥¼ ê·¹ëŒ€í™”í•œ íŒŒì¸íŠœë‹ ì ‘ê·¼ë²•ì„ ì œì‹œí•œë‹¤.

---

| êµ¬ì„± ìš”ì†Œ      | ì„¤ëª… |
|----------------|------|
| **í•µì‹¬ ëª¨ë“ˆ**    | NF4 4-bit Quantized Transformer + LoRA ëª¨ë“ˆ + Double Quantization |
| **í•™ìŠµ ì „ëµ**    | Quantized weightëŠ” ë™ê²°, LoRA í–‰ë ¬ (A, B)ë§Œ í•™ìŠµ, scale factorë„ ì–‘ìí™” |
| **ì „ì´ ë°©ì‹**    | Base LLMì€ ê·¸ëŒ€ë¡œ ì¬ì‚¬ìš©, task-specific LoRA ëª¨ë“ˆë§Œ ë³„ë„ë¡œ íŒŒì¸íŠœë‹ |
| **ì„±ëŠ¥/íš¨ìœ¨ì„±** | ê¸°ì¡´ LoRA ëŒ€ë¹„ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ 3ë°° ì ˆê°, Full-precision ì„±ëŠ¥ ê±°ì˜ ìœ ì§€ |

---

## ğŸ”— ì°¸ê³  ë§í¬ (References)

* [ğŸ“„ arXiv ë…¼ë¬¸](https://arxiv.org/abs/2305.14314)
* [ğŸ’» GitHub](https://github.com/artidoro/qlora)
* [ğŸ“ˆ Papers with Code](https://paperswithcode.com/paper/qlora-efficient-finetuning-of-quantized)

---



