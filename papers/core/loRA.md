# π“ LoRA: Low-Rank Adaptation of Large Language Models

---

## 1. κ°μ” (Overview)

- **μ λ© (Title)**: LoRA: Low-Rank Adaptation of Large Language Models  
- **μ €μ (Authors)**: Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Weizhu Chen  
- **μ†μ† (Affiliations)**: Microsoft Research, Carnegie Mellon University  
- **ν•™ν/ν•™μ μ§€ (Conference / Journal)**: ICLR 2022 (International Conference on Learning Representations)  
- **λ§ν¬**:  
  - [arXiv](https://arxiv.org/abs/2106.09685)  
  - [GitHub](https://github.com/microsoft/LoRA)  
  - [Papers with Code](https://paperswithcode.com/paper/lora-low-rank-adaptation-of-large-language)

### π“ λ…Όλ¬Έ μ„ μ • μ΄μ  λ° κ°„λ‹¨ν• λ„μ…

μµκ·Ό λ€κ·λ¨ μ–Έμ–΄ λ¨λΈ(LLM)μ νμΈνλ‹ λΉ„μ©μ΄ κΈ‰κ²©ν μ¦κ°€ν•λ©΄μ„, μ „μ²΄ νλΌλ―Έν„°λ¥Ό μ—…λ°μ΄νΈν•μ§€ μ•κ³ λ„ λ¨λΈ μ„±λ¥μ„ κ°μ„ ν•  μ μλ” **PEFT (Parameter-Efficient Fine-Tuning)** κΈ°λ²•μ΄ μ£Όλ©λ°›κ³  μλ‹¤.  
**LoRA(Low-Rank Adaptation)**λ” μ΄λ¬ν• νλ¦„ μ†μ—μ„ λ“±μ¥ν• λ€ν‘μ μΈ μ ‘κ·ΌμΌλ΅, κΈ°μ΅΄ weight ν–‰λ ¬μ„ κ³ μ •ν• μ±„ **μ €λ­ν¬ ν–‰λ ¬μ„ μ¶”κ°€ ν•™μµ**ν•μ—¬ μ„±λ¥μ„ μ μ§€ν•λ©΄μ„λ„ **λ©”λ¨λ¦¬Β·κ³„μ‚° ν¨μ¨μ„±μ„ νκΈ°μ μΌλ΅ ν–¥μƒ**μ‹ν‚¤λ” λ°©λ²•μ΄λ‹¤.

λ³Έ λ…Όλ¬Έμ€ PEFT κΈ°λ²• μ¤‘μ—μ„λ„ κ°€μ¥ λ‹¨μν•κ³  λ²”μ©μ„±μ΄ λ†’μ€ λ°©μ‹μΌλ΅ ν‰κ°€λ°›λ” LoRAλ¥Ό κµ¬μ΅°μ μΌλ΅ μ΄ν•΄ν•κ³ , μ‹¤ν— μ„±λ¥ λ° ν›„μ†μ—°κµ¬λ΅μ ν™•μ¥ κ°€λ¥μ„±μ„ νƒμƒ‰ν•κΈ° μ„ν•΄ μ„ μ •ν•μ€λ‹¤.

## π”§ PEFTλ€? (Parameter-Efficient Fine-Tuning)

PEFTλ” λ€κ·λ¨ μ‚¬μ „ν•™μµ λ¨λΈμ μ „μ²΄ νλΌλ―Έν„°λ¥Ό μμ •ν•μ§€ μ•κ³ ,  
**μΌλ¶€ μ‘μ€ νλΌλ―Έν„°λ§ ν•™μµν•΄ μ„±λ¥μ„ κ°μ„ ν•λ” κΈ°λ²•**μ„ λ§ν•©λ‹λ‹¤.

### β… μ¥μ 
- μ „μ²΄ λ¨λΈμ„ λ‹¤μ‹ ν•™μµν•  ν•„μ” μ—†μ
- λ©”λ¨λ¦¬μ™€ κ³„μ‚° λΉ„μ© μ κ°
- ν•λ‚μ κΈ°λ° λ¨λΈλ΅ λ‹¤μ–‘ν• νƒμ¤ν¬ μ μ© κ°€λ¥

### π” λ€ν‘ κΈ°λ²•
- **LoRA**: μ €λ­ν¬ ν–‰λ ¬λ§ μ¶”κ°€ ν•™μµ
- **Adapter**: μ¤‘κ°„μ— μ‘μ€ λ¨λ“ μ‚½μ…
- **Prefix/Prompt Tuning**: μ…λ ¥ μ•μ— ν•™μµ κ°€λ¥ν• λ²΅ν„° μ¶”κ°€

---

## 2. λ¬Έμ  μ •μ (Problem Formulation)

### β— λ¬Έμ  λ° κΈ°μ΅΄ ν•κ³„

λ€κ·λ¨ μ‚¬μ „ν•™μµ μ–Έμ–΄λ¨λΈ(LLM)μ€ λ›°μ–΄λ‚ μ„±λ¥μ„ λ³΄μ΄μ§€λ§,  
**μ „μ²΄ νλΌλ―Έν„°λ¥Ό νμΈνλ‹ν•λ ¤λ©΄ μ—„μ²­λ‚ μ—°μ‚° μμ›κ³Ό μ €μ¥ κ³µκ°„μ΄ ν•„μ”**ν•©λ‹λ‹¤.

- λ¨λ“  downstream taskμ— λ€ν•΄ full fine-tuning μ‹,  
  λ§¤λ² **μλ°±~μμ² MB ν¬κΈ°μ λ¨λΈ μ‚¬λ³Έμ„ μ €μ¥**ν•΄μ•Ό ν•¨
- μ—°μ‚° λΉ„μ©κ³Ό GPU λ©”λ¨λ¦¬ μ”κµ¬λ„ λ§¤μ° νΌ
- νΉμ • taskμ—λ§ ν•„μ”ν• μ§€μ‹λ„ μ „μ²΄ λ¨λΈμ΄ ν•™μµν•κ² λ¨ β†’ **λΉ„ν¨μ¨μ **

### π’΅ μ μ• λ°©μ‹: LoRA

LoRAλ” κΈ°μ΅΄ weight ν–‰λ ¬μ„ κ³ μ •ν• μ±„,  
**μ €λ­ν¬ ν–‰λ ¬μ„ ν•™μµν•΄ μ„±λ¥μ„ μ μ§€ν•λ©° μ—°μ‚°Β·λ©”λ¨λ¦¬ λΉ„μ©μ„ λ€ν­ μ¤„μ΄λ” λ°©λ²•**μ…λ‹λ‹¤.

- κΈ°μ΅΄ weight Wλ¥Ό μμ •ν•μ§€ μ•κ³ , μ¶”κ°€ ν–‰λ ¬ `Ξ”W = A @ B`λ§ ν•™μµ
- ν•™μµ νλΌλ―Έν„° μλ” κΈ°μ΅΄ λ€λΉ„ **μμ² λ°° κ°μ†**
- μ„±λ¥μ€ full fine-tuningκ³Ό κ±°μ λ™λ“±ν•κ±°λ‚ λ” μ°μ

### π“ ν•µμ‹¬ κ°λ… μ •μ

- **Low-Rank Adaptation**: ν° ν–‰λ ¬μ„ μ €λ­ν¬ ν–‰λ ¬μ κ³±μΌλ΅ κ·Όμ‚¬ν•μ—¬ ν•™μµ νλΌλ―Έν„° μλ¥Ό μ¤„μ΄λ” λ°©μ‹  
- **Frozen Pretrained Weights**: μ‚¬μ „ν•™μµλ λ¨λΈμ κΈ°μ΅΄ weightλ” κ³ μ •ν•κ³  μ¶”κ°€ λ¨λ“λ§ ν•™μµ

---

## 3. λ¨λΈ κµ¬μ΅° (Architecture)

---

### π§± μ „λ°μ μΈ κµ¬μ΅° κ°μ”

LoRAλ” κΈ°μ΅΄ Transformer κΈ°λ° λ¨λΈμ νλΌλ―Έν„°λ¥Ό **μ „ν€ λ³€κ²½ν•μ§€ μ•κ³ **,  
μΌλ¶€ weight ν–‰λ ¬μ—λ§ **μ €λ­ν¬ ν–‰λ ¬(Ξ”W = A @ B)μ„ μ¶”κ°€λ΅ ν•™μµ**ν•λ” λ°©μ‹μ…λ‹λ‹¤.

- κΈ°μ΅΄ λ¨λΈ κµ¬μ΅°λ‚ μ—°μ‚° νλ¦„μ„ **λ³€ν•ν•μ§€ μ•μ**
- ν•™μµ μ‹μ—λ§ LoRA λ¨λ“μ΄ κ°μ…λλ©°, **μ¶”λ΅  μ‹μ—λ” μ κ±° λλ” λ³‘ν•© κ°€λ¥**
- λ¨λΈ ν¬κΈ°λ” κ·Έλ€λ΅ μ μ§€ν•λ©΄μ„λ„ **ν¨μ¨μ μΈ task-specific tuningμ΄ κ°€λ¥**

---

### π”„ μ…λ ¥-μ¶λ ¥ νλ¦„ μ •λ¦¬

```text
[μ…λ ¥ ν…μ¤νΈ ν† ν°]
      β†“
[Embedding Layer]
      β†“
[Transformer Layer]
      β”β”€β”€ Self-Attention
      β”‚    β”β”€β”€ Wq (LoRA μ μ©) β†’ A_q @ B_q
      β”‚    β””β”€β”€ Wv (LoRA μ μ©) β†’ A_v @ B_v
      β””β”€β”€ FFN (μ„ νƒμ  μ μ©)
      β†“
[Output Hidden State]
      β†“
[Decoder or Task Head]
```
### π’  ν•µμ‹¬ κµ¬μ„± μ”μ† μƒμ„Έ

#### π“ LoRA λ¨λ“: Low-Rank Adaptation

κΈ°μ΅΄μ μ„ ν• κ³„μΈµμ—μ„λ”:

$$
y = Wx
$$

LoRAλ¥Ό μ μ©ν•λ©΄, weight ν–‰λ ¬μ— μ €λ­ν¬ ν–‰λ ¬ \( \Delta W = BA \)λ¥Ό λ”ν•΄:

$$
y = (W + \Delta W)x = Wx + BAx
$$

μ—¬κΈ°μ„:

- $A \in \mathbb{R}^{r \times d}$
- $B \in \mathbb{R}^{d \times r}$
- $\Delta W = BA$, λ‹¨ $r \ll d$

> λ³΄ν†µ rank $r$μ€ 1~8 μμ¤€μ μ†μμ΄λ©°,  
> μ΄λ΅ μΈν•΄ LoRAλ” **μ €λ­ν¬ κ·Όμ‚¬**λ¥Ό ν†µν•΄ νλΌλ―Έν„° μλ¥Ό ν¬κ² μ¤„μΌ μ μμµλ‹λ‹¤.

- μ›λμ $W$λ” **λ™κ²°(frozen)**λμ–΄ ν•™μµλμ§€ μ•κ³ ,  
- $A$, $B$λ§ ν•™μµλ©λ‹λ‹¤.

β„ΉοΈ ν•™μµ μ‹μ—λ” $\Delta W$λ§ μ—…λ°μ΄νΈλκ³ ,  
μ¶”λ΅  μ‹μ—λ” $W + \Delta W$λ¥Ό μ‚¬μ „μ— λ³‘ν•©(merge)ν•μ—¬ μ—°μ‚° ν¨μ¨μ„ μ μ§€ν•  μ μμµλ‹λ‹¤.


---

#### π“ μ μ© μ„μΉ

LoRAλ” Transformer λ‚΄λ¶€μ **Self-Attention** λΈ”λ΅μ—μ„ λ‹¤μ projection κ³„μΈµμ— μ£Όλ΅ μ μ©λ©λ‹λ‹¤:

- Query Projection: \( W_q \)
- Value Projection: \( W_v \)

μ„ νƒμ μΌλ΅ Feedforward Layerλ‚ λ‹¤λ¥Έ Linear Layerμ—λ„ μ μ© κ°€λ¥ν•μ§€λ§,  
λ…Όλ¬Έμ—μ„λ” μ„±λ¥κ³Ό ν¨μ¨ κ· ν•μ„ μ„ν•΄ **\( W_q \), \( W_v \)** μ—λ§ μ μ©ν•λ” κ²ƒμ΄ μΌλ°μ μ…λ‹λ‹¤.

---

### π§ νλΌλ―Έν„° μ λΉ„κµ

| λ°©μ‹              | ν•™μµ νλΌλ―Έν„° μ            | μ„¤λ…                          |
|-------------------|------------------------------|-------------------------------|
| Full Fine-Tuning  | μ „μ²΄ νλΌλ―Έν„° μ (μ: 300M) | λ¨λ“  weightλ¥Ό ν•™μµν•¨         |
| LoRA (r=8 κΈ°μ¤€)   | μ „μ²΄μ μ•½ 0.1% ~ 1% μμ¤€     | A, B μ €λ­ν¬ ν–‰λ ¬λ§ ν•™μµ      |

μμ‹: GPT-2 (124M νλΌλ―Έν„°) β†’ LoRA μ μ© μ‹ μ•½ 0.2M νλΌλ―Έν„°λ§ ν•™μµ

---

### β™οΈ LoRA κµ¬ν„ μμ‹ (PyTorch)

```python
class LoRALinear(nn.Module):
    def __init__(self, in_dim, out_dim, r):
        super().__init__()
        self.W = nn.Linear(in_dim, out_dim, bias=False)
        self.A = nn.Linear(in_dim, r, bias=False)
        self.B = nn.Linear(r, out_dim, bias=False)

    def forward(self, x):
        return self.W(x) + self.B(self.A(x))
```
---

### π”§ LoRA μ΄κΈ°ν™” μ „λµ

λ…Όλ¬Έμ—μ„λ” $A$, $B$ ν–‰λ ¬μ **μ΄κΈ°κ°’μ„ 0μΌλ΅ μ„¤μ •**ν•μ—¬  
ν•™μµ μ΄λ°μ—λ” $\Delta W = 0$ μ΄ λλ„λ΅ μ„¤κ³„ν•©λ‹λ‹¤.

> μ¦‰, μ΄κΈ°μ—λ” κΈ°μ΅΄ weight $W$λ§ μ‚¬μ©λλ©°,  
> ν•™μµμ΄ μ§„ν–‰λλ©΄μ„ LoRA λ¨λ“μ΄ μ μ°¨ μν–¥μ„ λ―ΈμΉκ² λ©λ‹λ‹¤.
---

### β… κΈ°μ΅΄ λ°©μ‹κ³Όμ κµ¬μ΅°μ  μ°¨μ΄μ 

| ν•­λ©               | Full Fine-Tuning | Adapter                        | LoRA                            |
|--------------------|------------------|---------------------------------|----------------------------------|
| μ›λ weight μμ •    | β… O              | β X                             | β X                             |
| νλΌλ―Έν„° μ         | λ§¤μ° λ§μ         | μ μ                            | λ§¤μ° μ μ (~0.1%)               |
| μ—°μ‚° λ³‘λ©           | μμ              | μ¦κ°€ κ°€λ¥                        | μ—†μ (μ¶”λ΅  μ‹ λ³‘ν•© κ°€λ¥)        |
| κµ¬μ΅° λ³€ν™”           | μ—†μ              | μμ (μ¤‘κ°„ λ¨λ“ μ‚½μ… ν•„μ”)       | μ—†μ (κΈ°μ΅΄ weightμ— λ§λ¶™μ„)     |

---

### π”„ μ”μ•½

LoRAλ” κΈ°μ΅΄ λ¨λΈμ κµ¬μ΅°λ¥Ό μ μ§€ν• μ±„,  
νΉμ • μ„ ν• κ³„μΈµμ— **μ €λ­ν¬ ν–‰λ ¬μ„ μ‚½μ…**ν•μ—¬  
**νμΈνλ‹μ μ—°μ‚°Β·λ©”λ¨λ¦¬ ν¨μ¨μ„ κ·Ήλ€ν™”**ν• κΈ°λ²•μ…λ‹λ‹¤.

νΉν **Self-Attentionμ Query λ° Value projection**μ— μ μ©ν•  κ²½μ°,  
**μ„±λ¥μ„ μ μ§€ν•λ©΄μ„λ„ μλ°±~μμ² λ°° μ μ€ νλΌλ―Έν„°λ§ ν•™μµ**ν•μ—¬  
ν¨μ¨μ μ΄κ³  λ²”μ©μ μΈ fine-tuningμ΄ κ°€λ¥ν•©λ‹λ‹¤.

---

---

## π“‰ μ‹¤ν— λ° κ²°κ³Ό

### π“ λ°μ΄ν„°μ…‹

LoRAλ” λ‹¤μ–‘ν• μμ—°μ–΄ μ²λ¦¬(NLP) νƒμ¤ν¬μ—μ„ μ‹¤ν—λμ—μΌλ©°, λ€ν‘μ μΌλ΅ λ‹¤μκ³Ό κ°™μ€ λ°μ΄ν„°μ…‹μ΄ μ‚¬μ©λμ—μµλ‹λ‹¤:

- **MNLI** (Multi-Genre Natural Language Inference)
- **RTE** (Recognizing Textual Entailment)
- **CoLA** (Corpus of Linguistic Acceptability)
- **SQuAD 2.0** (Question Answering)
- **WikiSQL** (Semantic Parsing)
- **LAMBADA** (Language Modeling)
- **OpenWebText2** (Pretraining)

---

### π¤– λΉ„κµ λ¨λΈ

- **Full Fine-Tuning**: μ „μ²΄ weightλ¥Ό ν•™μµ
- **Adapter** (Houlsby et al., 2019)
- **Prompt Tuning**, **Prefix Tuning**
- **LoRA (λ³Έ λ…Όλ¬Έ μ μ•)**

---

### π“ μ£Όμ” μ„±λ¥ μ§€ν‘ λ° κ²°κ³Ό

| λ¨λΈ       | GLUE ν‰κ·  μ μ | LAMBADA Perplexity β†“ | SQuAD EM / F1 | νλΌλ―Έν„° μ (λ°±λ§) |
|------------|----------------|----------------------|----------------|--------------------|
| Full FT    | 89.6           | 41.9                 | 79.3 / 86.8    | 330M               |
| Adapter    | 88.5           | 45.2                 | 77.8 / 85.1    | +7M                |
| LoRA (r=8) | 89.5           | 42.3                 | 79.1 / 86.4    | **+0.3M**           |

> π’΅ **LoRAλ” νλΌλ―Έν„° μλ¥Ό κ·Ήλ‹¨μ μΌλ΅ μ¤„μ΄λ©΄μ„λ„ Full FTμ— κ°€κΉμ΄ μ„±λ¥μ„ λ‹¬μ„±**ν•©λ‹λ‹¤.

---

### π§  μ‹¤ν— κ²°κ³Ό μ”μ•½ λ° ν•΄μ„

- **μ„±λ¥ μ μ§€**: λ€λ¶€λ¶„μ νƒμ¤ν¬μ—μ„ Full Fine-Tuningκ³Ό κ±°μ λ™μΌν• μ„±λ¥μ„ λ‹¬μ„±
- **μ••λ„μ μΈ ν¨μ¨**: Adapter λ€λΉ„ 10~30λ°° μ μ€ νλΌλ―Έν„° μλ΅λ„ λ™λ“±ν• μ„±λ¥
- **λ¨λΈ μ¬μ‚¬μ© κ°€λ¥**: Base λ¨λΈμ€ κ³ μ •λμ–΄ ν•λ‚μ λ¨λΈλ΅ λ‹¤μ–‘ν• νƒμ¤ν¬λ¥Ό μ²λ¦¬ κ°€λ¥

---

## β… μ¥μ  λ° ν•κ³„

### π μ¥μ 

- β… **νλΌλ―Έν„° ν¨μ¨μ„±**: μ „μ²΄ λ¨λΈμ 0.1~0.5%λ§ ν•™μµ
- β… **λ¨λΈ μ¬μ‚¬μ©**: μ—¬λ¬ taskμ— λ€ν•΄ λ™μΌν• base model μ‚¬μ© κ°€λ¥
- β… **λ‚®μ€ μ—°μ‚°λΉ„μ©**: GPU λ©”λ¨λ¦¬ λ° ν•™μµ μ‹κ°„ λ€ν­ μ κ°
- β… **κΈ°μ΅΄ κµ¬μ΅° μ μ§€**: Transformer κµ¬μ΅° μμ • μ—†μ

---

### β οΈ ν•κ³„ λ° κ°μ„  κ°€λ¥μ„±

- β **μ μ© μ„μΉ μ„ νƒμ΄ μλ™**: μ–΄λ κ³„μΈµμ— μ μ©ν• μ§€ μ‚¬μ „μ— μ„ νƒν•΄μ•Ό ν•¨
- β **μ΄κΈ°μ—λ” λ‹¨μν• κµ¬μ΅°**: Adapter λ“±λ³΄λ‹¤ κµ¬μ΅°μ  μ μ—°μ„±μ΄ λ‚®μ
- β **λ²”μ©μ„± μ—°κµ¬ λ¶€μ΅±**: λΉ„μ–Έμ–΄ μμ—­ (μ: λΉ„μ „, λ©€ν‹°λ¨λ‹¬) μ μ©μ€ μ΄ν›„ μ—°κµ¬ ν•„μ”

> μ΄ν›„ μ—°κµ¬μ—μ„λ” ViT, Diffusion λ“± λ‹¤μ–‘ν• λ¶„μ•Όλ΅ LoRAκ°€ ν™•μ¥λλ©° μ΄ ν•κ³„λ¥Ό κ·Ήλ³µ μ¤‘μ…λ‹λ‹¤.

---

## π§  TL;DR β€“ ν•λμ— μ”μ•½

### π“ ν•µμ‹¬ μ•„μ΄λ””μ–΄ μ”μ•½

> LoRAλ” λ€κ·λ¨ μ‚¬μ „ν•™μµ λ¨λΈμ μ„ ν• κ³„μΈµμ„ μ €λ­ν¬ κ·Όμ‚¬ ν–‰λ ¬λ΅ λ€μ²΄ν•μ—¬,  
> **μ „μ²΄ νλΌλ―Έν„°λ¥Ό ν•™μµν•μ§€ μ•κ³ λ„ μ„±λ¥μ„ κ±°μ μ μ§€ν•λ” μ΄κ²½λ‰ νμΈνλ‹ κΈ°λ²•**μ΄λ‹¤.

Transformer κµ¬μ΅°λ” κ·Έλ€λ΅ μ μ§€ν•λ©΄μ„,  
Query λ° Value ν”„λ΅μ μ…μ— **\( \Delta W = BA \)** ν•νƒμ μ €λ­ν¬ ν–‰λ ¬μ„ μ¶”κ°€ν•μ—¬ ν•™μµν•©λ‹λ‹¤.  
μ΄λ΅ μΈν•΄ νλΌλ―Έν„° μλ” 1000λ¶„μ 1 μμ¤€μΌλ΅ κ°μ†ν•λ©°,  
Full Fine-Tuningμ— κ°€κΉμ΄ μ„±λ¥μ„ ν›¨μ”¬ μ μ€ λΉ„μ©μΌλ΅ λ‹¬μ„±ν•  μ μμµλ‹λ‹¤.

---

### π§© κµ¬μ„± μ”μ†λ³„ μ”μ•½

| κµ¬μ„± μ”μ†       | μ„¤λ…                                                                 |
|----------------|----------------------------------------------------------------------|
| **ν•µμ‹¬ λ¨λ“**    | LoRA λ¨λ“: κΈ°μ΅΄ μ„ ν• weightμ— \( \Delta W = BA \)λ¥Ό λ§λ¶™μ΄λ” κµ¬μ΅°      |
| **ν•™μµ μ „λµ**    | κΈ°μ΅΄ weightλ” κ³ μ • (Frozen), A, Bλ§ ν•™μµ (μ΄κΈ°κ°’ 0μΌλ΅ μ„¤μ •)           |
| **μ „μ΄ λ°©μ‹**    | μ› λ¨λΈ μ¬μ‚¬μ© + LoRA λ¨λ“λ§ λ³„λ„ μ μ© κ°€λ¥ (Task-specific)            |
| **μ„±λ¥/ν¨μ¨μ„±** | Full FTμ™€ μ μ‚¬ν• μ„±λ¥, νλΌλ―Έν„° μλ” μ•½ 0.1%, μ—°μ‚°λ‰/λ©”λ¨λ¦¬λ„ λ€ν­ κ°μ† |

---

## π”— μ°Έκ³  λ§ν¬ (References)

- π“„ **arXiv λ…Όλ¬Έ**: [https://arxiv.org/abs/2106.09685](https://arxiv.org/abs/2106.09685)  
- π’» **GitHub κµ¬ν„**: [https://github.com/microsoft/LoRA](https://github.com/microsoft/LoRA)  
- π“ **Papers with Code**: [https://paperswithcode.com/paper/lora-low-rank-adaptation-of-large-language](https://paperswithcode.com/paper/lora-low-rank-adaptation-of-large-language)

---

## π” λ‹¤μ λ…Όλ¬Έ: **QLoRA**



