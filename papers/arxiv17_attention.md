# ğŸ“˜ Attention Is All You Need 

## 1. ê°œìš” (Overview)

* **ì œëª©**: Attention Is All You Need
* **ì €ì**: Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, Polosukhin
* **ì†Œì†**: Google Brain
* **í•™íšŒ**: NeurIPS (NIPS) 2017
* **ë§í¬**: [arXiv](https://arxiv.org/abs/1706.03762), [GitHub](https://github.com/tensorflow/tensor2tensor)

> ì™„ì „í•œ Self-Attention ê¸°ë°˜ êµ¬ì¡°ì¸ Transformerë¥¼ ì²˜ìŒ ì œì•ˆí•œ ë…¼ë¬¸. ìˆœì°¨ì²˜ë¦¬ êµ¬ì¡°(RNN/CNN)ë¥¼ ì œê±°í•˜ê³ , ë³‘ë ¬ ì²˜ë¦¬ ë° ì„±ëŠ¥ í–¥ìƒì„ ëª¨ë‘ ë‹¬ì„±í•¨.

---

## 2. ë¬¸ì œ ì •ì˜ (Problem Formulation)

* **ë¬¸ì œ**: ê¸°ê³„ ë²ˆì—­ (Sequence-to-Sequence)
* **ê¸°ì¡´ í•œê³„**: RNN/CNN ê¸°ë°˜ ëª¨ë¸ì€ ë³‘ë ¬í™”ê°€ ì–´ë µê³ , ì¥ê±°ë¦¬ ì˜ì¡´ì„±(long-range dependency) í•™ìŠµì´ ë¹„íš¨ìœ¨ì ì„
* **ì œì•ˆ ë°©ì‹**: ëª¨ë“  ìœ„ì¹˜ ê°„ ê´€ê³„ë¥¼ Self-Attentionìœ¼ë¡œ ê³„ì‚°í•´, ì¥ê¸° ì˜ì¡´ì„± ë¬¸ì œ í•´ê²° + ë³‘ë ¬ ì²˜ë¦¬ ê°€ëŠ¥

---

## 3. ëª¨ë¸ êµ¬ì¡° (Architecture)

### ì „ì²´ êµ¬ì¡°

* Encoder-Decoder êµ¬ì¡°
* ê° ë¸”ë¡ì€ Multi-Head Attention + Feed-Forward Network
* Residual Connection + Layer Normalization í¬í•¨

### ì£¼ìš” ì»´í¬ë„ŒíŠ¸ ì„¤ëª…

#### ğŸ’  Scaled Dot-Product Attention

$\text{Attention}(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V$

#### ğŸ’  Multi-Head Attention

* ì—¬ëŸ¬ ê°œì˜ Attention Headë¥¼ ë³‘ë ¬ë¡œ ì‚¬ìš©í•´ ë‹¤ì–‘í•œ í‘œí˜„ì„ í•™ìŠµ

#### ğŸ’  Positional Encoding

* ìˆœì„œë¥¼ ì•Œ ìˆ˜ ì—†ê¸° ë•Œë¬¸ì—, ì…ë ¥ì— ìœ„ì¹˜ ì •ë³´ë¥¼ ë”í•¨
* ìˆ˜ì‹:
  $PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)$
  $PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)$

---

## 4. í•™ìŠµ ë°©ì‹ (Training & Optimization)

* **Loss**: Cross-Entropy Loss (Label Smoothing ì ìš©)
* **Optimizer**: Adam ($\beta_1=0.9, \beta_2=0.98$)
* **Learning Rate Schedule**:
  $lr = d_{model}^{-0.5} \cdot \min(step^{-0.5}, step \cdot warmup^{-1.5})$
* **Dropout**: 0.1 ì ìš©
* **Parameter Initialization**: Xavier init

---

## 5. ì‹¤í—˜ ì„¤ì • (Experiment Settings)

* **ë°ì´í„°ì…‹**:

  * WMT 2014 English-German (4.5M ë¬¸ì¥ìŒ)
  * WMT 2014 English-French (36M ë¬¸ì¥ìŒ)
* **Tokenization**: Byte-Pair Encoding (BPE)
* **ë°°ì¹˜ í¬ê¸°**: 25,000 token per batch

---

## 6. ê²°ê³¼ ë¶„ì„ (Results & Evaluation)

| ëª¨ë¸          | BLEU (En-De) | í•™ìŠµ ì‹œê°„ | íŒŒë¼ë¯¸í„° ìˆ˜ |
| ----------- | ------------ | ----- | ------ |
| Transformer | **28.4**     | ë¹ ë¦„    | 65M    |
| GNMT        | 24.6         | ëŠë¦¼    | 213M   |

* **Ablation**:

  * Head ìˆ˜, Depth, Positional Encoding ìœ ë¬´ ì‹¤í—˜
* **ì„±ëŠ¥ ì§€í‘œ**: BLEU (Bilingual Evaluation Understudy Score)

---

## 7. í•œê³„ ë° í–¥í›„ ì—°êµ¬ (Limitations & Future Work)

* Attentionì€ O(n^2) ë©”ëª¨ë¦¬ ë³µì¡ë„ â†’ ê¸´ ì‹œí€€ìŠ¤ ë¹„íš¨ìœ¨ì 
* ì´í›„ ì—°êµ¬ì—ì„œëŠ” Efficient Attention, Sparse Attention ë“±ìœ¼ë¡œ í™•ì¥

---

## 8. ê´€ë ¨ ì—°êµ¬ì™€ ë¹„êµ (Related Works)

| ëª¨ë¸           | êµ¬ì¡°             | ë³‘ë ¬í™” ê°€ëŠ¥ | ì¥ê±°ë¦¬ ì˜ì¡´ì„± |
| ------------ | -------------- | ------ | ------- |
| LSTM Seq2Seq | RNN ê¸°ë°˜         | âŒ      | ì œí•œì      |
| GNMT         | LSTM+Attention | ğŸ”¶ ë¶€ë¶„ì  | ë³´ì™„ì      |
| Transformer  | Attention-only | âœ…      | íš¨ê³¼ì      |

---

## 9. ë…¼ë¬¸ ì½ìœ¼ë©° ë©”ëª¨í•œ í¬ì¸íŠ¸ (Notes)

* Attention ì—°ì‚°ì´ ì‹œê°ì ìœ¼ë¡œ ëª…í™•í•´ ë¶„ì„ ìš©ì´
* Residual + LayerNormì´ í•™ìŠµ ì•ˆì •í™”ì— í° ê¸°ì—¬
* Positional Encodingì€ ì‚¬ì¸-ì½”ì‚¬ì¸ í•¨ìˆ˜ë¡œ ë§¤ìš° ìš°ì•„í•˜ê²Œ í•´ê²°

---

## 10. í•œ ì¤„ ìš”ì•½ (TL;DR)

> "RNN ì—†ì´ë„ ìˆœì°¨ í•™ìŠµì´ ê°€ëŠ¥í•˜ë©°, Self-Attentionë§Œìœ¼ë¡œ ë³‘ë ¬ì„± + ì„±ëŠ¥ì„ ëª¨ë‘ ë‹¬ì„±í•œ íšê¸°ì  ëª¨ë¸"

---

## ğŸ”— ì°¸ê³  ë§í¬ (References)

* [ğŸ“„ arXiv ë…¼ë¬¸](https://arxiv.org/abs/1706.03762)
* [ğŸ’» ê³µì‹ GitHub (Tensor2Tensor)](https://github.com/tensorflow/tensor2tensor)
* [ğŸ“ˆ Papers with Code](https://paperswithcode.com/paper/attention-is-all-you-need)
