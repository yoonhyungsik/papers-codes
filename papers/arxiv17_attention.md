# ğŸ“˜ Attention Is All You Need 

## 1. ê°œìš” (Overview)

* **ì œëª©**: Attention Is All You Need
* **ì €ì**: Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, Polosukhin
* **ì†Œì†**: Google Brain
* **í•™íšŒ**: NeurIPS (NIPS) 2017
* **ë§í¬**: [arXiv](https://arxiv.org/abs/1706.03762), [GitHub](https://github.com/tensorflow/tensor2tensor)

> ë…¼ë¬¸ ì •ë¦¬ ì²« ì‹œì‘ìœ¼ë¡œ íŠ¸ëœìŠ¤í¬ë¨¸ êµ¬ì¡°ì˜ ê°€ì¥ ê¸°ë³¸ì´ ëœë‹¤ê³  ìƒê°í•˜ëŠ” Attention Is All You Needë¥¼ ì„ ì •í•¨. í™•ì‹¤íˆ íŠ¸ëœìŠ¤í¬ë¨¸ ë¶€í„° ê°œë…ì´ ì•½í•˜ë‹¤ê³  ìƒê°í•˜ì—¬ ì—¬ê¸°ì„œë¶€í„° ì‹œì‘í•´ì•¼ê² ë‹¤ ìƒê°í•¨.
> ì™„ì „í•œ Self-Attention ê¸°ë°˜ êµ¬ì¡°ì¸ Transformerë¥¼ ì²˜ìŒ ì œì•ˆí•œ ë…¼ë¬¸. ìˆœì°¨ì²˜ë¦¬ êµ¬ì¡°(RNN/CNN)ë¥¼ ì œê±°í•˜ê³ , ë³‘ë ¬ ì²˜ë¦¬ ë° ì„±ëŠ¥ í–¥ìƒì„ ëª¨ë‘ ë‹¬ì„±í•¨.

---

## 2. ë¬¸ì œ ì •ì˜ (Problem Formulation)

* **ë¬¸ì œ ë° ê¸°ì¡´í•œê³„**: Reccurent modelì—ì„œëŠ” ì…ì¶œë ¥ ì‹œí€€ìŠ¤ì˜ ê¸°í˜¸ ìœ„ì¹˜ì— ë”°ë¼ ê³„ì‚°ì„ ìˆ˜í–‰í•¨. ê³„ì‚° ì‹œê°„ì— ë”°ë¼ ìœ„ì¹˜ë¥¼ ì •ë ¬í•˜ë©´ ì´ì „ hidden state(h)ì™€ ìœ„ì¹˜(t)ì— ëŒ€í•œ hidden state sequence(h_t)ê°€ ìƒì„±ë¨. ì´ëŸ¬í•œ ìˆœì°¨ì  êµ¬ì¡°ëŠ” ê° ì‹œí€€ìŠ¤ ë‚´ë¶€ì˜ ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ì–´ë µê²Œ ë§Œë“¤ë©°,
ì‹œí€€ìŠ¤ ê¸¸ì´ê°€ ê¸¸ì–´ì§ˆìˆ˜ë¡ ì—¬ëŸ¬ ì‹œí€€ìŠ¤ë¥¼ ë™ì‹œì— ì²˜ë¦¬í•˜ëŠ” ë° ë©”ëª¨ë¦¬ í•œê³„ë¡œ ì¸í•´ ë°°ì¹˜ êµ¬ì„±ì´ ì œí•œë˜ëŠ” ë¬¸ì œê°€ ë”ìš± ì‹¬ê°í•´ì§. ì´ì „ ì—°êµ¬ì—ì„œëŠ” ì¸ìˆ˜ë¶„í•´ì™€ ì¡°ê±´ë¶€ ê³„ì‚°ì„ í†µí•´ íš¨ìœ¨ì„ ê°œì„ í–ˆì§€ë§Œ sequential computationì˜ ì œì•½ì€ ì—¬ì „íˆ ë‚¨ì•„ìˆìŒ.
* 
* 
* **ì œì•ˆ ë°©ì‹**: ìˆœí™˜ ì—†ì´ self-attentionë§Œìœ¼ë¡œ global dependencyë¥¼ ì²˜ë¦¬
* 
ë³‘ë ¬í™” ê°€ëŠ¥, RNN/CNN ëŒ€ë¹„ ë‹¨ìˆœí•˜ê³  íš¨ê³¼ì ì¸ êµ¬ì¡°
* 
â€»Global Dependency
ì…ë ¥ ë˜ëŠ” ì¶œë ¥ ì‹œí€€ìŠ¤ ë‚´ì˜ ë©€ë¦¬ ë–¨ì–´ì§„ ìš”ì†Œë“¤ ê°„ì˜ ì˜ì¡´ ê´€ê³„
ì¦‰, ë¬¸ì¥ì´ë‚˜ ì‹œí€€ìŠ¤ì˜ ì²˜ìŒê³¼ ëì²˜ëŸ¼ ì„œë¡œ ë©€ë¦¬ ë–¨ì–´ì§„ ìœ„ì¹˜ ì‚¬ì´ì— ì˜ë¯¸ì ì¸ ì—°ê²°ì´ë‚˜ ì˜í–¥ì„ ì£¼ê³ ë°›ëŠ” ê´€ê³„ë¥¼ ì˜ë¯¸.
* 


---

## 3. ëª¨ë¸ êµ¬ì¡° (Architecture)

### ì „ì²´ êµ¬ì¡°

* Encoder-Decoder êµ¬ì¡°
* 
* ì¸ì½”ë”ì—ëŠ” ë‘ê°œì˜ í•˜ìœ„ ë ˆì´ì–´ê°€ ì¡´ì¬. ê°ê° multi-head self attentionê³¼ ë‹¨ìˆœ feed forward networkë¡œ êµ¬ì„±
ë‘ê°œì˜ í•˜ìœ„ ë ˆì´ì–´ ê°ê°ì— residual connectionì„ ì‚¬ìš©í•œ í›„ layer normì§„í–‰. ì¦‰, ê° ì„œë¸Œ ë ˆì´ì–´ì˜ ì¶œë ¥ì€ $ \text{LayerNorm}(x + \text{Sublayer}(x)) $
ì´ëŸ¬í•œ residual connection ìš©ì´í•˜ê²Œ í•˜ê¸° ìœ„í•´ ëª¨ë¸ì˜ ëª¨ë“  sub-layerì™€ embedding layerëŠ” 512ì°¨ì›ì˜ ì¶œë ¥ ìƒì„±.

* ë””ì½”ë”ì˜ ê²½ìš° ì¸ì½”ë”ì™€ ë§ˆì°¬ê°€ì§€ë¡œ multi-head self attentionê³¼ ë‹¨ìˆœ feed forward networkë¥¼ ê°€ì§. ì—¬ê¸°ì— ì¸ì½”ë” ìŠ¤íƒì˜ ì¶œë ¥ì— ëŒ€í•´  multi-head self attention ìˆ˜í–‰í•˜ëŠ” ì„¸ë²ˆì§¸ sub-layer ì¶”ê°€.
ë””ì½”ë”ì˜ self-attention ì„œë¸Œë ˆì´ì–´ëŠ” íŠ¹ì • ìœ„ì¹˜ê°€ ìê¸°ë³´ë‹¤ ë’¤ì— ìˆëŠ” ìœ„ì¹˜ë¥¼ ì°¸ì¡°í•˜ì§€ ëª»í•˜ë„ë¡(masking) ë³€ê²½ë¨. ì´ ë§ˆìŠ¤í‚¹ì€, ì¶œë ¥ ì„ë² ë”©ì´ í•œ ì¹¸ ë’¤ë¡œ(offset) ë°€ë ¤ ìˆëŠ” êµ¬ì¡°ì™€ ê²°í•©ë˜ì–´ ië²ˆì§¸ ìœ„ì¹˜ì˜ ì˜ˆì¸¡ì´ ì˜¤ì§ ië³´ë‹¤ ì‘ì€ ìœ„ì¹˜ë“¤ì˜ ì¶œë ¥ì—ë§Œ ì˜ì¡´í•˜ë„ë¡ ë§Œë“ ë‹¤. -> ë¯¸ë˜ ë‹¨ì–´ë¥¼ ë¯¸ë¦¬ ë³´ë©´ ì•ˆ ë˜ê¸° ë•Œë¬¸ (â†’ ì˜¤í† ë¦¬ê·¸ë ˆì‹œë¸Œ ë°©ì‹ ìœ ì§€)

## â€»Residual Connection(ì”ì°¨ ì—°ê²°)
ìˆ˜ì‹ì€:
$$
\text{Output} = \mathcal{F}(x) + x
$$
ì—¬ê¸°ì„œ $\mathcal{F}(x)$ëŠ” ë ˆì´ì–´ë¥¼ ê±°ì¹œ ì¶œë ¥ì´ê³ , $x$ëŠ” ì…ë ¥ê°’.
âœ”ï¸ í•™ìŠµ ì•ˆì •í™”
ê¹Šì€ ë„¤íŠ¸ì›Œí¬ì¼ìˆ˜ë¡ ê¸°ìš¸ê¸° ì†Œì‹¤/í­ì£¼ ë¬¸ì œê°€ ìƒê¹€
Residual ì—°ê²°ì€ ê¸°ìš¸ê¸°ê°€ ì—­ì „íŒŒë  ê²½ë¡œë¥¼ ì§ì ‘ ë§Œë“¤ì–´ì¤Œ
âœ”ï¸ ì •ë³´ ë³´ì¡´
ì´ì „ì˜ ì›ë³¸ ì…ë ¥ ì •ë³´ë¥¼ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ë©´ì„œ ìƒˆë¡œìš´ íŠ¹ì§•ë§Œ ë”í•˜ê²Œ ë¨

*Attentionì€ query,key-valueìŒì„ ì¶œë ¥ì— ë§¤í•‘í•˜ëŠ”ê²ƒìœ¼ë¡œ ì„¤ëª…ê°€ëŠ¥í•¨.(ì—¬ê¸°ì„œ ëª¨ë“  ê°’ë“¤ì€ ë²¡í„°) ì¶œë ¥ì€ ê°€ì¤‘ì¹˜ í•©ìœ¼ë¡œ ê³„ì‚°ë¨.

* ê° ë¸”ë¡ì€ Multi-Head Attention + Feed-Forward Network
* Residual Connection + Layer Normalization í¬í•¨

### ì£¼ìš” ì»´í¬ë„ŒíŠ¸ ì„¤ëª…

#### ğŸ’  Scaled Dot-Product Attention

$\text{Attention}(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V$

#### ğŸ’  Multi-Head Attention

* ì—¬ëŸ¬ ê°œì˜ Attention Headë¥¼ ë³‘ë ¬ë¡œ ì‚¬ìš©í•´ ë‹¤ì–‘í•œ í‘œí˜„ì„ í•™ìŠµ

#### ğŸ’  Positional Encoding

* ìˆœì„œë¥¼ ì•Œ ìˆ˜ ì—†ê¸° ë•Œë¬¸ì—, ì…ë ¥ì— ìœ„ì¹˜ ì •ë³´ë¥¼ ë”í•¨
* ìˆ˜ì‹:
  $PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)$
  $PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)$

---

## 4. í•™ìŠµ ë°©ì‹ (Training & Optimization)

* **Loss**: Cross-Entropy Loss (Label Smoothing ì ìš©)
* **Optimizer**: Adam ($\beta_1=0.9, \beta_2=0.98$)
* **Learning Rate Schedule**:
  $lr = d_{model}^{-0.5} \cdot \min(step^{-0.5}, step \cdot warmup^{-1.5})$
* **Dropout**: 0.1 ì ìš©
* **Parameter Initialization**: Xavier init

---

## 5. ì‹¤í—˜ ì„¤ì • (Experiment Settings)

* **ë°ì´í„°ì…‹**:

  * WMT 2014 English-German (4.5M ë¬¸ì¥ìŒ)
  * WMT 2014 English-French (36M ë¬¸ì¥ìŒ)
* **Tokenization**: Byte-Pair Encoding (BPE)
* **ë°°ì¹˜ í¬ê¸°**: 25,000 token per batch

---

## 6. ê²°ê³¼ ë¶„ì„ (Results & Evaluation)

| ëª¨ë¸          | BLEU (En-De) | í•™ìŠµ ì‹œê°„ | íŒŒë¼ë¯¸í„° ìˆ˜ |
| ----------- | ------------ | ----- | ------ |
| Transformer | **28.4**     | ë¹ ë¦„    | 65M    |
| GNMT        | 24.6         | ëŠë¦¼    | 213M   |

* **Ablation**:

  * Head ìˆ˜, Depth, Positional Encoding ìœ ë¬´ ì‹¤í—˜
* **ì„±ëŠ¥ ì§€í‘œ**: BLEU (Bilingual Evaluation Understudy Score)

---

## 7. í•œê³„ ë° í–¥í›„ ì—°êµ¬ (Limitations & Future Work)

* Attentionì€ O(n^2) ë©”ëª¨ë¦¬ ë³µì¡ë„ â†’ ê¸´ ì‹œí€€ìŠ¤ ë¹„íš¨ìœ¨ì 
* ì´í›„ ì—°êµ¬ì—ì„œëŠ” Efficient Attention, Sparse Attention ë“±ìœ¼ë¡œ í™•ì¥

---

## 8. ê´€ë ¨ ì—°êµ¬ì™€ ë¹„êµ (Related Works)

| ëª¨ë¸           | êµ¬ì¡°             | ë³‘ë ¬í™” ê°€ëŠ¥ | ì¥ê±°ë¦¬ ì˜ì¡´ì„± |
| ------------ | -------------- | ------ | ------- |
| LSTM Seq2Seq | RNN ê¸°ë°˜         | âŒ      | ì œí•œì      |
| GNMT         | LSTM+Attention | ğŸ”¶ ë¶€ë¶„ì  | ë³´ì™„ì      |
| Transformer  | Attention-only | âœ…      | íš¨ê³¼ì      |

---

## 9. ë…¼ë¬¸ ì½ìœ¼ë©° ë©”ëª¨í•œ í¬ì¸íŠ¸ (Notes)

* Attention ì—°ì‚°ì´ ì‹œê°ì ìœ¼ë¡œ ëª…í™•í•´ ë¶„ì„ ìš©ì´
* Residual + LayerNormì´ í•™ìŠµ ì•ˆì •í™”ì— í° ê¸°ì—¬
* Positional Encodingì€ ì‚¬ì¸-ì½”ì‚¬ì¸ í•¨ìˆ˜ë¡œ ë§¤ìš° ìš°ì•„í•˜ê²Œ í•´ê²°

---

## 10. í•œ ì¤„ ìš”ì•½ (TL;DR)

> "RNN ì—†ì´ë„ ìˆœì°¨ í•™ìŠµì´ ê°€ëŠ¥í•˜ë©°, Self-Attentionë§Œìœ¼ë¡œ ë³‘ë ¬ì„± + ì„±ëŠ¥ì„ ëª¨ë‘ ë‹¬ì„±í•œ íšê¸°ì  ëª¨ë¸"

---

## ğŸ”— ì°¸ê³  ë§í¬ (References)

* [ğŸ“„ arXiv ë…¼ë¬¸](https://arxiv.org/abs/1706.03762)
* [ğŸ’» ê³µì‹ GitHub (Tensor2Tensor)](https://github.com/tensorflow/tensor2tensor)
* [ğŸ“ˆ Papers with Code](https://paperswithcode.com/paper/attention-is-all-you-need)
