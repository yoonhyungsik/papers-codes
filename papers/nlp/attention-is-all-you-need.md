# ğŸ“˜ Attention Is All You Need

## 1. ê°œìš” (Overview)

* **ì œëª©**: Attention Is All You Need
* **ì €ì**: Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, Polosukhin
* **ì†Œì†**: Google Brain
* **í•™íšŒ**: NeurIPS (NIPS) 2017
* **ë§í¬**: [arXiv](https://arxiv.org/abs/1706.03762), [GitHub](https://github.com/tensorflow/tensor2tensor)

> ë…¼ë¬¸ ì •ë¦¬ ì²« ì‹œì‘ìœ¼ë¡œ íŠ¸ëœìŠ¤í¬ë¨¸ êµ¬ì¡°ì˜ ê°€ì¥ ê¸°ë³¸ì´ ëœë‹¤ê³  ìƒê°í•˜ëŠ” Attention Is All You Needë¥¼ ì„ ì •í•¨. í™•ì‹¤íˆ íŠ¸ëœìŠ¤í¬ë¨¸ë¶€í„° ê°œë…ì´ ì•½í•˜ë‹¤ê³  ìƒê°í•˜ì—¬ ì—¬ê¸°ì„œë¶€í„° ì‹œì‘í•´ì•¼ê² ë‹¤ ìƒê°í•¨.
> ì™„ì „í•œ Self-Attention ê¸°ë°˜ êµ¬ì¡°ì¸ Transformerë¥¼ ì²˜ìŒ ì œì•ˆí•œ ë…¼ë¬¸. ìˆœì°¨ì²˜ë¦¬ êµ¬ì¡°(RNN/CNN)ë¥¼ ì œê±°í•˜ê³ , ë³‘ë ¬ ì²˜ë¦¬ ë° ì„±ëŠ¥ í–¥ìƒì„ ëª¨ë‘ ë‹¬ì„±í•¨.

---

## 2. ë¬¸ì œ ì •ì˜ (Problem Formulation)

**ë¬¸ì œ ë° ê¸°ì¡´ í•œê³„**:

* Recurrent modelì—ì„œëŠ” ì…ì¶œë ¥ ì‹œí€€ìŠ¤ì˜ ê¸°í˜¸ ìœ„ì¹˜ì— ë”°ë¼ ê³„ì‚°ì„ ìˆ˜í–‰í•¨. ê³„ì‚° ì‹œê°„ì— ë”°ë¼ ìœ„ì¹˜ë¥¼ ì •ë ¬í•˜ë©´ ì´ì „ hidden state(h)ì™€ ìœ„ì¹˜(t)ì— ëŒ€í•œ hidden state sequence(h\_t)ê°€ ìƒì„±ë¨.
* ì´ëŸ¬í•œ ìˆœì°¨ì  êµ¬ì¡°ëŠ” ê° ì‹œí€€ìŠ¤ ë‚´ë¶€ì˜ ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ì–´ë µê²Œ ë§Œë“¤ë©°, ì‹œí€€ìŠ¤ ê¸¸ì´ê°€ ê¸¸ì–´ì§ˆìˆ˜ë¡ ì—¬ëŸ¬ ì‹œí€€ìŠ¤ë¥¼ ë™ì‹œì— ì²˜ë¦¬í•˜ëŠ” ë° ë©”ëª¨ë¦¬ í•œê³„ë¡œ ì¸í•´ ë°°ì¹˜ êµ¬ì„±ì´ ì œí•œë˜ëŠ” ë¬¸ì œê°€ ë”ìš± ì‹¬ê°í•´ì§.
* ì´ì „ ì—°êµ¬ì—ì„œëŠ” ì¸ìˆ˜ë¶„í•´ì™€ ì¡°ê±´ë¶€ ê³„ì‚°ì„ í†µí•´ íš¨ìœ¨ì„ ê°œì„ í–ˆì§€ë§Œ sequential computationì˜ ì œì•½ì€ ì—¬ì „íˆ ë‚¨ì•„ìˆìŒ.

**ì œì•ˆ ë°©ì‹**:

* ìˆœí™˜ êµ¬ì¡° ì—†ì´ self-attentionë§Œìœ¼ë¡œ global dependencyë¥¼ ì²˜ë¦¬í•¨
* ë³‘ë ¬í™” ê°€ëŠ¥í•˜ê³ , RNN/CNN ëŒ€ë¹„ ë‹¨ìˆœí•˜ë©´ì„œë„ íš¨ê³¼ì ì¸ êµ¬ì¡°ë¥¼ ê°€ì§

> â€» **Global Dependency**: ì…ë ¥ ë˜ëŠ” ì¶œë ¥ ì‹œí€€ìŠ¤ ë‚´ì˜ ë©€ë¦¬ ë–¨ì–´ì§„ ìš”ì†Œë“¤ ê°„ì˜ ì˜ë¯¸ì  ì—°ê²°. ë¬¸ì¥ ì•ë’¤ì— ìˆëŠ” ë‹¨ì–´ë“¤ ì‚¬ì´ì˜ ê´€ê³„ë¥¼ ëª¨ë¸ë§í•˜ëŠ” ê²ƒ.

---

## 3. ëª¨ë¸ êµ¬ì¡° (Architecture)

### ì „ì²´ êµ¬ì¡°

![íŠ¸ëœìŠ¤í¬ë¨¸ êµ¬ì¡°](/papers/images/AIAYNmodel.png)

* TransformerëŠ” **Encoder-Decoder êµ¬ì¡°**ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŒ

* **ì¸ì½”ë”**: ê° ë ˆì´ì–´ëŠ” ë‘ ê°œì˜ ì„œë¸Œë ˆì´ì–´ë¡œ êµ¬ì„±ë¨

  1. Multi-Head Self-Attention
  2. Position-wise Feed-Forward Network
     ê° ì„œë¸Œë ˆì´ì–´ì— ëŒ€í•´ **Residual Connection + LayerNorm**ì„ ì ìš©í•¨:

  $$
  \text{Output} = \text{LayerNorm}(x + \text{Sublayer}(x))
  $$

  ëª¨ë“  ì„œë¸Œë ˆì´ì–´ ë° ì„ë² ë”©ì€ 512ì°¨ì›ì˜ ì¶œë ¥ì„ ìƒì„±í•¨

* **ë””ì½”ë”**: ì¸ì½”ë”ì™€ ë™ì¼í•œ êµ¬ì¡°ì— ë”í•´, ì„¸ ë²ˆì§¸ ì„œë¸Œë ˆì´ì–´ ì¶”ê°€:
  3\. Multi-Head Attention over Encoder Output

  ë””ì½”ë”ì˜ Self-Attentionì—ëŠ” \*\*ë§ˆìŠ¤í‚¹(Masking)\*\*ì´ ì ìš©ë˜ì–´ **ë¯¸ë˜ ìœ„ì¹˜ì˜ ì •ë³´ ì°¸ì¡°ë¥¼ ë°©ì§€**í•¨
  â†’ ië²ˆì§¸ ìœ„ì¹˜ì˜ ì˜ˆì¸¡ì€ ì˜¤ì§ ië³´ë‹¤ ì•ì„  ìœ„ì¹˜ì˜ ì •ë³´ì—ë§Œ ì˜ì¡´í•¨ (ì˜¤í† ë¦¬ê·¸ë ˆì‹œë¸Œ)

> âœ”ï¸ **Residual Connection(ì”ì°¨ ì—°ê²°)**:
>
> * í•™ìŠµ ì•ˆì •í™”ë¥¼ ë•ê³ , ê¸°ìš¸ê¸° ì†Œì‹¤ì„ ë°©ì§€
> * ì›ë³¸ ì…ë ¥ ì •ë³´ë¥¼ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ë©´ì„œ ìƒˆë¡œìš´ í‘œí˜„ë§Œ ì¶”ê°€í•¨

---

## âœ¨ Attention ë©”ì»¤ë‹ˆì¦˜ ê°œìš”

* Attentionì€ query, key, valueì˜ ì„¸ ê°€ì§€ ë²¡í„° ìŒì„ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ **ê°€ì¤‘í•©ëœ value**ë¥¼ ì¶œë ¥í•˜ëŠ” êµ¬ì¡°

![ì–´í…ì…˜ ê³µì‹](/papers/images/attentioncalc.png)

---

### ğŸ’  Scaled Dot-Product Attention

* Queryì™€ ëª¨ë“  Key ê°„ì˜ **ë‚´ì  (dot product)** ì„ ê³„ì‚°í•˜ê³ , ê° ê²°ê³¼ë¥¼ \$\sqrt{d\_k}\$ë¡œ ë‚˜ëˆˆ ë’¤
  **Softmax í•¨ìˆ˜ë¥¼ ì ìš©í•˜ì—¬ Valueì— ëŒ€í•œ ê°€ì¤‘ì¹˜**ë¥¼ ì–»ëŠ”ë‹¤.

* ì—¬ëŸ¬ ê°œì˜ Queryë¥¼ \*\*í–‰ë ¬ \$Q$\*\*ë¡œ ë¬¶ì–´ ë³‘ë ¬ì ìœ¼ë¡œ ê³„ì‚°í•˜ë©°, Keyì™€ Valueë„ ê°ê° \$K\$, \$V\$ë¡œ ë¬¶ìŒ

* ì „ì²´ Attention ê³„ì‚° ìˆœì„œ:

$$
\text{Query-Key ë‚´ì } \;\to\; \text{ì •ê·œí™”} \;\to\; \text{Softmax} \;\to\; \text{Value ê°€ì¤‘í•©}
$$

* ìˆ˜ì‹ í‘œí˜„:

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

---

## âš–ï¸ Additive Attention vs Dot-Product Attention

* ëŒ€í‘œì ì¸ ì–´í…ì…˜ ë°©ì‹ì€ **Additive Attention**ê³¼ **Dot-Product (Multiplicative) Attention**
* Dot-Product ë°©ì‹ì€ ë³¸ ë…¼ë¬¸ì—ì„œ ì œì•ˆí•œ ë°©ì‹ê³¼ ê±°ì˜ ê°™ìœ¼ë‚˜, ê¸°ì¡´ ë°©ì‹ì—ëŠ” ìŠ¤ì¼€ì¼ë§ì´ ì—†ìŒ

> Dot-Product Attention: ë‚´ì  í›„ softmax
> Additive Attention: FFN ê¸°ë°˜ í˜¸í™˜ì„± í•¨ìˆ˜ ì‚¬ìš©

* Dot-ProductëŠ” **ê³ ì† í–‰ë ¬ê³± ê¸°ë°˜ìœ¼ë¡œ ë” ë¹ ë¥´ê³  ë©”ëª¨ë¦¬ íš¨ìœ¨ì´ ë†’ìŒ**

---

## ğŸ“‰ Scalingì´ í•„ìš”í•œ ì´ìœ 

* \$d\_k\$ê°€ ì‘ì„ ë•ŒëŠ” í° ì°¨ì´ê°€ ì—†ì§€ë§Œ, **\$d\_k\$ê°€ í´ ê²½ìš° Dot-Productì˜ í¬ê¸°ê°€ ë„ˆë¬´ ì»¤ì ¸** softmaxê°€ **gradient ì†Œì‹¤ ì˜ì—­ìœ¼ë¡œ ë“¤ì–´ê°**

* ì´ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ **\$\frac{1}{\sqrt{d\_k}}\$ë¡œ ìŠ¤ì¼€ì¼ë§í•˜ì—¬ softmax ì…ë ¥ì„ ì•ˆì •í™”**ì‹œí‚´

### ğŸ’  Multi-Head Attention

- Query, Key, Valueë¥¼ ê°ê° $h$ê°œì˜ ì„ í˜• ë³€í™˜ì„ í†µí•´ $d_k$, $d_k$, $d_v$ ì°¨ì›ìœ¼ë¡œ íˆ¬ì˜
- ê° headì—ì„œ ë³‘ë ¬ë¡œ Scaled Dot-Product Attentionì„ ìˆ˜í–‰:

$$
\text{head}_i = \text{Attention}(Q W^Q_i, K W^K_i, V W^V_i)
$$

- ëª¨ë“  headì˜ ì¶œë ¥ì„ Concatenate í•œ ë’¤, ë˜ í•œ ë²ˆ ì„ í˜• ë³€í™˜:

$$
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_h) W^O
$$

- ì´ êµ¬ì¡°ëŠ” ë‹¤ì–‘í•œ ìœ„ì¹˜ì—ì„œ ë‹¤ì–‘í•œ í‘œí˜„ ê³µê°„ì— ëŒ€í•´ ë™ì‹œì— ì£¼ì˜ë¥¼ ì§‘ì¤‘í•˜ê²Œ í•´ ì¤Œ

#### ğŸ“Œ ë…¼ë¬¸ ì„¤ì •
- $h = 8$
- $d_k = d_v = \frac{d_{\text{model}}}{h} = 64$
- $W^Q_i, W^K_i, W^V_i \in \mathbb{R}^{d_{\text{model}} \times d_k}$
- $W^O \in \mathbb{R}^{hd_v \times d_{\text{model}}}$

> ì—¬ëŸ¬ headë¥¼ ì‚¬ìš©í•˜ë©´ averagingì— ì˜í•œ ì •ë³´ ì†ì‹¤ì„ ë°©ì§€í•˜ê³ , ë” ë‹¤ì–‘í•œ ê´€ê³„ë¥¼ ë™ì‹œì— í¬ì°© ê°€ëŠ¥í•¨.

### ğŸ’  3.2.3 Attentionì˜ í™œìš© ë°©ì‹ 

Transformerì—ì„œëŠ” Multi-Head Attentionì´ **ì´ 3ê°€ì§€ ë°©ì‹**ìœ¼ë¡œ ì‚¬ìš©ëœë‹¤:

---

#### 1. ğŸ§­ Encoder-Decoder Attention
- QueryëŠ” ë””ì½”ë”ì—ì„œ, Keyì™€ ValueëŠ” ì¸ì½”ë”ì˜ ì¶œë ¥ì—ì„œ ê°€ì ¸ì˜´
- ë””ì½”ë”ì˜ ê° ìœ„ì¹˜ê°€ **ì…ë ¥ ì‹œí€€ìŠ¤ ì „ì²´ë¥¼ ì°¸ì¡°í•  ìˆ˜ ìˆê²Œ** í•¨
- ê¸°ì¡´ì˜ seq2seq ëª¨ë¸ì—ì„œ ì‚¬ìš©ë˜ë˜ attention ë°©ì‹ê³¼ ìœ ì‚¬í•¨

---

#### 2. ğŸ” Encoder Self-Attention
- Query, Key, Value ëª¨ë‘ **ì¸ì½”ë”ì˜ ë™ì¼í•œ ìœ„ì¹˜ ì¶œë ¥ì„ ì‚¬ìš©**
- ì¸ì½”ë”ì˜ ê° ìœ„ì¹˜ëŠ” **ì…ë ¥ ì‹œí€€ìŠ¤ ì „ì²´ ìœ„ì¹˜ë¥¼ ì°¸ì¡°**í•  ìˆ˜ ìˆìŒ

---

#### 3. â© Decoder Self-Attention (Masked)
- Query, Key, Value ëª¨ë‘ **ë””ì½”ë”ì˜ ì´ì „ ë ˆì´ì–´ ì¶œë ¥**
- ë‹¨, **ië²ˆì§¸ ìœ„ì¹˜ëŠ” ië³´ë‹¤ í¬ê±°ë‚˜ ê°™ì€ ë¯¸ë˜ ìœ„ì¹˜ë¥¼ ì°¸ì¡°í•˜ì§€ ëª»í•˜ë„ë¡ ë§ˆìŠ¤í‚¹ ì²˜ë¦¬**
  - ë§ˆìŠ¤í‚¹ì€ softmax ì…ë ¥ì—ì„œ **ë¶ˆê°€ëŠ¥í•œ ì—°ê²°ì„ $-\\infty$ë¡œ ì„¤ì •í•˜ì—¬ ë¬´íš¨í™”**
- ì´ëŠ” **ì˜¤í† ë¦¬ê·¸ë ˆì‹œë¸Œ(autoregressive)** ì„±ì§ˆì„ ìœ ì§€í•˜ê¸° ìœ„í•¨

> ìš”ì•½:
> - Encoder-Decoder Attention â†’ ë””ì½”ë”ê°€ ì¸ì½”ë” ì°¸ì¡°  
> - Encoder Self-Attention â†’ ì…ë ¥ ë‚´ë¶€ ì°¸ì¡°  
> - Decoder Self-Attention â†’ ì¶œë ¥ ë‚´ë¶€ ì°¸ì¡° + ë§ˆìŠ¤í‚¹

### ğŸ’  3.3 Position-wise Feed-Forward Networks

- Encoderì™€ Decoderì˜ ê° ë ˆì´ì–´ì—ëŠ” Attention ì„œë¸Œë ˆì´ì–´ ì™¸ì—ë„ **Fully Connected Feed-Forward Network (FFN)**ì´ ì¡´ì¬í•¨.
- ì´ FFNì€ **ê° ìœ„ì¹˜(position)ì— ëŒ€í•´ ë…ë¦½ì ì´ê³  ë™ì¼í•˜ê²Œ ì ìš©**ë¨.
- êµ¬ì¡°ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤:

$$
\text{FFN}(x) = \max(0, xW_1 + b_1) W_2 + b_2
$$

- ReLU í™œì„±í™” í•¨ìˆ˜ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ **ë‘ ê°œì˜ ì„ í˜• ë³€í™˜**ìœ¼ë¡œ êµ¬ì„±ë¨.
- ë™ì¼í•œ ìœ„ì¹˜ì— ëŒ€í•´ì„œëŠ” ë™ì¼í•œ FFNì´ ì ìš©ë˜ì§€ë§Œ, **ë ˆì´ì–´ë§ˆë‹¤ ë‹¤ë¥¸ íŒŒë¼ë¯¸í„°**ë¥¼ ì‚¬ìš©í•¨.
- **ì»¤ë„ í¬ê¸° 1ì˜ convolutionìœ¼ë¡œ í•´ì„í•  ìˆ˜ë„ ìˆìŒ.**
- ë…¼ë¬¸ì—ì„œëŠ” ì…ë ¥ ë° ì¶œë ¥ ì°¨ì› $d_{\text{model}} = 512$, ë‚´ë¶€ ì°¨ì› $d_{\text{ff}} = 2048$ ì‚¬ìš©.

---

### ğŸ’  3.4 Embeddings and Softmax

- ì…ë ¥ í† í°ê³¼ ì¶œë ¥ í† í°ì€ **í•™ìŠµëœ ì„ë² ë”©(Embedding)**ì„ í†µí•´ $d_{\text{model}}$ ì°¨ì› ë²¡í„°ë¡œ ë³€í™˜ë¨.
- ë””ì½”ë”ì˜ ì¶œë ¥ì€ **ì„ í˜• ë³€í™˜ + Softmax**ë¥¼ ê±°ì³ ë‹¤ìŒ í† í°ì— ëŒ€í•œ í™•ë¥  ë¶„í¬ë¥¼ ìƒì„±í•¨.

> ğŸ¯ íŠ¹ì§•:
> - **ì…ë ¥ ì„ë² ë”©**, **ì¶œë ¥ ì„ë² ë”©**, **pre-Softmax ì„ í˜• ë³€í™˜**ì— **ë™ì¼í•œ ê°€ì¤‘ì¹˜ í–‰ë ¬(weight tying)** ì‚¬ìš©  
> - ì„ë² ë”© í–‰ë ¬ì€ $\\sqrt{d_{\\text{model}}}$ë¥¼ ê³±í•˜ì—¬ ìŠ¤ì¼€ì¼ ì¡°ì •ë¨

---

### ğŸ’  3.5 Positional Encoding

- Transformerì—ëŠ” **RNNì´ë‚˜ CNNì´ ì—†ê¸° ë•Œë¬¸ì—**, í† í° ê°„ **ìˆœì„œ ì •ë³´(position)**ë¥¼ ì¸ì½”ë”©í•´ì•¼ í•¨.
- ì´ë¥¼ ìœ„í•´ **Positional Encoding**ì„ ì…ë ¥ ì„ë² ë”©ì— ë”í•´ì¤Œ.
- Positional Encodingì€ **ì„ë² ë”©ê³¼ ë™ì¼í•œ ì°¨ì›($d_{\text{model}}$)**ì„ ê°€ì§€ë¯€ë¡œ ë”í•˜ê¸°ê°€ ê°€ëŠ¥í•¨.

> ë…¼ë¬¸ì—ì„œëŠ” **ê³ ì •ëœ(fixed)** ë°©ì‹ì˜ Positional Encoding ì‚¬ìš©  
> (í•™ìŠµí•˜ëŠ” ë°©ì‹ë„ ì‹¤í—˜í–ˆì§€ë§Œ í° ì°¨ì´ëŠ” ì—†ìŒ)

#### ğŸ“ ìˆ˜ì‹:

$$
PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right)
$$

$$
PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right)
$$

- $pos$ëŠ” ìœ„ì¹˜, $i$ëŠ” ì„ë² ë”©ì˜ ì°¨ì› ì¸ë±ìŠ¤
- ê° ì°¨ì›ì€ ì„œë¡œ ë‹¤ë¥¸ ì£¼íŒŒìˆ˜ì˜ ì‚¬ì¸/ì½”ì‚¬ì¸ í•¨ìˆ˜ë¥¼ ì‚¬ìš©
- ì£¼íŒŒìˆ˜ëŠ” $2\\pi$ì—ì„œ $10000 \\cdot 2\\pi$ê¹Œì§€ **ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ì¦ê°€**

> ì´ ë°©ì‹ì€ **ìƒëŒ€ ìœ„ì¹˜ì— ë”°ë¼ attentionì´ ì˜ ì‘ë™í•˜ë„ë¡ ìœ ë„**  
> (ì˜ˆ: $PE_{pos+k}$ëŠ” $PE_{pos}$ì˜ ì„ í˜• í•¨ìˆ˜ë¡œ í‘œí˜„ ê°€ëŠ¥)

---

## ğŸ§  4. Why Self-Attention?

ì´ ì„¹ì…˜ì—ì„œëŠ” Transformerê°€ ê¸°ì¡´ì˜ RNN ë˜ëŠ” CNN ëŒ€ì‹  **Self-Attention**ì„ ì‚¬ìš©í•˜ëŠ” ì´ìœ ë¥¼ ì„¸ ê°€ì§€ ì¸¡ë©´ì—ì„œ ë¶„ì„í•œë‹¤:

---

### ğŸ“Œ 1. ì—°ì‚° ë³µì¡ë„ (Computational Complexity per Layer)

- **Self-Attention**: $\mathcal{O}(n^2 \cdot d)$
- **Recurrent Layer**: $\mathcal{O}(n \cdot d^2)$
- **Convolutional Layer**: $\mathcal{O}(k \cdot n \cdot d^2)$

> ì—¬ê¸°ì„œ $n$ì€ ì‹œí€€ìŠ¤ ê¸¸ì´, $d$ëŠ” í‘œí˜„ ì°¨ì›, $k$ëŠ” ì»¤ë„ í¬ê¸°

- ëŒ€ë¶€ë¶„ì˜ ìì—°ì–´ ì²˜ë¦¬ì—ì„œëŠ” $n < d$ì´ê¸° ë•Œë¬¸ì— self-attentionì´ ë” íš¨ìœ¨ì 
- **Separable convolution**ì„ ì‚¬ìš©í•´ë„, self-attention + FFN ì¡°í•©ê³¼ ì—°ì‚°ëŸ‰ì´ ìœ ì‚¬í•¨

---

### ğŸ“Œ 2. ë³‘ë ¬í™” ê°€ëŠ¥ì„± (Parallelization)

- **Self-Attention**: ëª¨ë“  ìœ„ì¹˜ì— ëŒ€í•´ **ë™ì‹œì— ì—°ì‚° ê°€ëŠ¥ (O(1) ìˆœì°¨ ì—°ì‚°)**  
- **RNN**: ë°˜ë“œì‹œ **ì´ì „ ì‹œì ì˜ ê²°ê³¼ì— ì˜ì¡´ (O(n) ìˆœì°¨ ì—°ì‚°)**  
- **CNN**: ë³‘ë ¬í™” ê°€ëŠ¥í•˜ì§€ë§Œ, **ì „ì²´ ìœ„ì¹˜ ê°„ ì—°ê²°ì„ ìœ„í•´ ì—¬ëŸ¬ ë ˆì´ì–´ í•„ìš”**

> âœ… ë³‘ë ¬í™” ê´€ì ì—ì„œ Self-Attentionì€ RNNë³´ë‹¤ í›¨ì”¬ ìœ ë¦¬í•¨

---

### ğŸ“Œ 3. ì¥ê±°ë¦¬ ì˜ì¡´ì„± í•™ìŠµ (Path Length for Long-Range Dependencies)

> ì‹œí€€ìŠ¤ì˜ ìœ„ì¹˜ ê°„ ì •ë³´ê°€ ì „ë‹¬ë˜ê¸°ê¹Œì§€ì˜ **ìµœì¥ ê²½ë¡œ ê¸¸ì´**ëŠ” í•™ìŠµ ë‚œì´ë„ì— ì§ì ‘ì ì¸ ì˜í–¥ì„ ì¤Œ

| êµ¬ì¡° | ìµœëŒ€ ê²½ë¡œ ê¸¸ì´ (Max Path Length) |
|------|-----------------------------------|
| Self-Attention | $\mathcal{O}(1)$ |
| RNN | $\mathcal{O}(n)$ |
| CNN | $\mathcal{O}(\log_k n)$ (dilated) ë˜ëŠ” $\mathcal{O}(n/k)$ (contiguous) |
| ì œí•œëœ Attention ($r$ ì´ì›ƒ) | $\mathcal{O}(n/r)$ |

- Self-Attentionì€ ëª¨ë“  ìœ„ì¹˜ ê°„ **ì§ì ‘ ì—°ê²°ì´ ê°€ëŠ¥**í•˜ë¯€ë¡œ **ì¥ê±°ë¦¬ ì˜ì¡´ì„± í•™ìŠµì´ ìœ ë¦¬**
- RNNì´ë‚˜ CNNì€ **ì—¬ëŸ¬ ë ˆì´ì–´ ë˜ëŠ” ì‹œê°„ ë‹¨ê³„ë¥¼ ê±°ì³ì•¼ë§Œ ì •ë³´ê°€ ì „ë‹¬**ë¨

---

### âœ… ë¶€ê°€ì  ì¥ì : í•´ì„ ê°€ëŠ¥ì„± (Interpretability)

- ê° Attention Headê°€ **ì„œë¡œ ë‹¤ë¥¸ ì–¸ì–´ì  ê¸°ëŠ¥ì„ í•™ìŠµ**í•˜ëŠ” ê²½í–¥ì´ ìˆìŒ
- ë¬¸ë²•ì /ì˜ë¯¸ì  êµ¬ì¡°ì— ë”°ë¥¸ Attention ë¶„í¬ê°€ ë‚˜íƒ€ë‚¨ â†’ **ë¶„ì„/ì‹œê°í™” ìš©ì´**

---

## ğŸ” ìš”ì•½

| í•­ëª© | Self-Attention | RNN | CNN |
|------|----------------|-----|-----|
| ì—°ì‚° ë³µì¡ë„ | $\mathcal{O}(n^2 \cdot d)$ | $\mathcal{O}(n \cdot d^2)$ | $\mathcal{O}(k \cdot n \cdot d^2)$ |
| ë³‘ë ¬í™” ê°€ëŠ¥ì„± | âœ… ë§¤ìš° ë†’ìŒ (O(1)) | âŒ ë‚®ìŒ (O(n)) | ğŸ”¶ ì¤‘ê°„ (ë³‘ë ¬ ê°€ëŠ¥) |
| ì¥ê±°ë¦¬ ì˜ì¡´ì„± | âœ… ì§ì ‘ ì—°ê²° (O(1)) | âŒ ì‹œê°„ ìˆœì°¨ ì˜ì¡´ | ğŸ”¶ ì—¬ëŸ¬ ë ˆì´ì–´ í•„ìš” |

> âœ”ï¸ ê²°ë¡ : Self-Attentionì€ **ë³‘ë ¬ì„±, í‘œí˜„ë ¥, ì—°ì‚° íš¨ìœ¨, í•´ì„ ê°€ëŠ¥ì„±** ì¸¡ë©´ì—ì„œ RNNê³¼ CNNì„ ëª¨ë‘ ëŠ¥ê°€í•¨


---

## 7. í•œê³„ ë° í–¥í›„ ì—°êµ¬ (Limitations & Future Work)

* Attentionì€ O(n^2) ë©”ëª¨ë¦¬ ë³µì¡ë„ â†’ ê¸´ ì‹œí€€ìŠ¤ ë¹„íš¨ìœ¨ì 
* ì´í›„ ì—°êµ¬ì—ì„œëŠ” Efficient Attention, Sparse Attention ë“±ìœ¼ë¡œ í™•ì¥


---

## ğŸ§  TL;DR â€“ í•œëˆˆì— ë³´ëŠ” í•µì‹¬ ìš”ì•½

> **"Attention Is All You Need"**ëŠ” RNN/CNN ì—†ì´ **Self-Attentionë§Œìœ¼ë¡œ ì‹œí€€ìŠ¤ë¥¼ ì²˜ë¦¬**í•˜ëŠ” ìƒˆë¡œìš´ ëª¨ë¸ **Transformer**ë¥¼ ì œì•ˆí•˜ì˜€ë‹¤.  
> ì´ êµ¬ì¡°ëŠ” ë³‘ë ¬ ì²˜ë¦¬ì™€ ì¥ê±°ë¦¬ ì˜ì¡´ì„± í•™ìŠµì— ìœ ë¦¬í•˜ë©°, ê¸°ì¡´ ë°©ì‹ ëŒ€ë¹„ ì—°ì‚° íš¨ìœ¨ê³¼ ì„±ëŠ¥ ëª¨ë‘ ë›°ì–´ë‚˜ë‹¤.

---

### ğŸ“Œ í•µì‹¬ êµ¬ì„± ìš”ì†Œ ìš”ì•½

| êµ¬ì„± ìš”ì†Œ | ì„¤ëª… |
|------------|----------------------------------------------------|
| Self-Attention | ì…ë ¥ ì‹œí€€ìŠ¤ ë‚´ ëª¨ë“  ìœ„ì¹˜ ìŒì˜ ê´€ê³„ë¥¼ í•œ ë²ˆì— ê³„ì‚° |
| Multi-Head Attention | ì—¬ëŸ¬ í‘œí˜„ ê³µê°„ì—ì„œ ë³‘ë ¬ì ìœ¼ë¡œ attention ìˆ˜í–‰ |
| Feed-Forward Network | ê° ìœ„ì¹˜ì— ë…ë¦½ì ìœ¼ë¡œ ì ìš©ë˜ëŠ” 2-layer MLP |
| Positional Encoding | ìˆœì„œ ì •ë³´ë¥¼ ì‚¬ì¸/ì½”ì‚¬ì¸ í•¨ìˆ˜ë¡œ ì¸ì½”ë”©í•´ ì…ë ¥ì— ì¶”ê°€ |
| Residual + LayerNorm | í•™ìŠµ ì•ˆì •í™” ë° ì •ë³´ ë³´ì¡´ ì—­í•  ìˆ˜í–‰ |

---

### âš–ï¸ ê¸°ì¡´ êµ¬ì¡°ì™€ ë¹„êµ (RNN, CNN ëŒ€ë¹„)

| í•­ëª© | Self-Attention | RNN | CNN |
|------|----------------|-----|-----|
| ë³‘ë ¬ ì²˜ë¦¬ | âœ… ë§¤ìš° ìš°ìˆ˜ | âŒ ë¶ˆê°€ | ğŸ”¶ ê°€ëŠ¥ |
| ì¥ê±°ë¦¬ ì˜ì¡´ì„± | âœ… ì§ì ‘ ì—°ê²° (O(1)) | âŒ O(n) | ğŸ”¶ ì—¬ëŸ¬ ë ˆì´ì–´ í•„ìš” |
| í•´ì„ ê°€ëŠ¥ì„± | âœ… ì‹œê°í™” ìš©ì´ | âŒ ë‚®ìŒ | ğŸ”¶ ì œí•œì  |
| ì—°ì‚° íš¨ìœ¨ | âœ… íš¨ìœ¨ì  (ì§§ì€ ì‹œí€€ìŠ¤) | âŒ ë¹„íš¨ìœ¨ì  | ğŸ”¶ ì»¤ë„ í¬ê¸°ì— ì˜ì¡´ |

---


### âœ… ê²°ë¡ 

> TransformerëŠ” **ìˆœì°¨ì„± ì—†ëŠ” ë³‘ë ¬ êµ¬ì¡°**, **ê°•ë ¥í•œ í‘œí˜„ë ¥**, **íš¨ìœ¨ì ì¸ ê³„ì‚°**, **í•´ì„ ê°€ëŠ¥ì„±**ì´ë¼ëŠ” ë„¤ ë§ˆë¦¬ í† ë¼ë¥¼ ëª¨ë‘ ì¡ì€ ì‹œí€€ìŠ¤ ëª¨ë¸ì´ë‹¤.  
> ì´í›„ BERT, GPT, T5 ë“± ëŒ€ë¶€ë¶„ì˜ NLP ëª¨ë¸ì´ ì´ êµ¬ì¡°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë°œì „í•˜ì˜€ë‹¤.

## ğŸ”— ì°¸ê³  ë§í¬ (References)

* [ğŸ“„ arXiv ë…¼ë¬¸](https://arxiv.org/abs/1706.03762)
* [ğŸ’» ê³µì‹ GitHub (Tensor2Tensor)](https://github.com/tensorflow/tensor2tensor)
* [ğŸ“ˆ Papers with Code](https://paperswithcode.com/paper/attention-is-all-you-need)

##ë‹¤ìŒ ë…¼ë¬¸: BERT
